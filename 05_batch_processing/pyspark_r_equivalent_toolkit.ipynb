{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NYC Taxi Data Analysis - Incremental Year-on-Year Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset and Goals\n",
        "- Goal: Predict the duration of NYC taxi rides using features like pickup time and trip coordinates\n",
        "- **NEW**: Incremental processing strategy for 20-30GB dataset growing at 15GB/year\n",
        "- Optimized for GCP Spark Notebook with sufficient memory and computation bandwidth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook Workflow\n",
        "- **Incremental Data Loading**: Load data year by year for efficient processing\n",
        "- Explore and visualize raw data with temporal analysis\n",
        "- Engineer new features with historical context\n",
        "- Examine outliers across different time periods\n",
        "- Incorporate external datasets\n",
        "    - Weather data\n",
        "- Visualize and analyze how these new features impact trip duration over time\n",
        "- Briefly explore a classification approach to predicting duration ranges\n",
        "- Build XGBoost models with temporal validation\n",
        "- **NEW**: Implement incremental model training and evaluation\n",
        "\n",
        "All of this will be done in PySpark, optimized for large-scale incremental processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
        "# from pyspark.sql.connect import functions as F\n",
        "from pyspark.sql.functions import col, year, month, dayofyear, when, lit, unix_timestamp, count, avg\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.figure import Figure\n",
        "import numpy as np\n",
        "import math\n",
        "from typing import List, Optional, Union, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core PySpark Setup & BigQuery Integration - Optimized for Incremental Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/07/05 16:11:08 WARN Utils: Your hostname, Sagars-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.29.162 instead (on interface en0)\n",
            "25/07/05 16:11:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Ivy Default Cache set to: /Users/saggysimmba/.ivy2/cache\n",
            "The jars for the packages stored in: /Users/saggysimmba/.ivy2/jars\n",
            "com.google.cloud.spark#spark-bigquery-with-dependencies_2.12 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-30f7e725-ddc7-4c38-bd9c-5b9ae1a72b2f;1.0\n",
            "\tconfs: [default]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":: loading settings :: url = jar:file:/Applications/saggydev/projects_learning/data_engineering_course/.venv/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tfound com.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.32.0 in central\n",
            "downloading https://repo1.maven.org/maven2/com/google/cloud/spark/spark-bigquery-with-dependencies_2.12/0.32.0/spark-bigquery-with-dependencies_2.12-0.32.0.jar ...\n",
            "\t[SUCCESSFUL ] com.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.32.0!spark-bigquery-with-dependencies_2.12.jar (5121ms)\n",
            ":: resolution report :: resolve 24463ms :: artifacts dl 5123ms\n",
            "\t:: modules in use:\n",
            "\tcom.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.32.0 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   1   |   1   |   1   |   0   ||   1   |   1   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-30f7e725-ddc7-4c38-bd9c-5b9ae1a72b2f\n",
            "\tconfs: [default]\n",
            "\t1 artifacts copied, 0 already retrieved (37746kB/35ms)\n",
            "25/07/05 16:11:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Spark Version: 3.5.4\n",
            "‚úÖ Spark UI: http://192.168.29.162:4040\n",
            "üìä Working with: dtc-de-course-457315.nyc_taxi_data\n",
            "üîÑ Incremental Processing: 2015-2016\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/07/05 16:11:52 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# Create Dataproc Spark Connect session with custom configs\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Enhanced Spark configuration for large-scale incremental processing\n",
        "# üîß SPARK CONFIGURATION EXPLAINED:\n",
        "# \n",
        "# ADAPTIVE QUERY EXECUTION (AQE) - Runtime optimization based on actual data\n",
        "# ‚îú‚îÄ \"spark.sql.adaptive.enabled\" = \"true\"\n",
        "# ‚îÇ  ‚îî‚îÄ Enables AQE for dynamic query optimization during execution\n",
        "# ‚îÇ\n",
        "# ‚îú‚îÄ \"spark.sql.adaptive.coalescePartitions.enabled\" = \"true\" \n",
        "# ‚îÇ  ‚îî‚îÄ Automatically reduces number of partitions after shuffle to optimize performance\n",
        "# ‚îÇ\n",
        "# ‚îú‚îÄ \"spark.sql.adaptive.localShuffleReader.enabled\" = \"true\"\n",
        "# ‚îÇ  ‚îî‚îÄ Reduces network I/O by reading shuffle data locally when possible\n",
        "# ‚îÇ\n",
        "# ‚îî‚îÄ \"spark.sql.adaptive.advisoryPartitionSizeInBytes\" = \"256MB\"\n",
        "#    ‚îî‚îÄ Target size for each partition (256MB optimal for most workloads)\n",
        "#\n",
        "# SKEW HANDLING-Deals with uneven data distribution\n",
        "# ‚îú‚îÄ \"spark.sql.adaptive.skewJoin.enabled\" = \"true\"\n",
        "# ‚îÇ  ‚îî‚îÄ Automatically detects and handles data skew in joins\n",
        "# ‚îÇ\n",
        "# ‚îú‚îÄ \"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\" = \"256MB\"\n",
        "# ‚îÇ  ‚îî‚îÄ Partition size threshold to identify skewed partitions\n",
        "# ‚îÇ\n",
        "# ‚îî‚îÄ \"spark.sql.adaptive.skewJoin.skewedPartitionFactor\" = \"5\"\n",
        "#    ‚îî‚îÄ Factor to determine skew (partition 5x larger than median = skewed)\n",
        "#\n",
        "# PERFORMANCE OPTIMIZATIONS\n",
        "# ‚îú‚îÄ \"spark.sql.shuffle.partitions\" = \"400\"\n",
        "# ‚îÇ  ‚îî‚îÄ Number of partitions for shuffle operations (good for 20-30GB datasets)\n",
        "# ‚îÇ\n",
        "# ‚îú‚îÄ \"spark.serializer\" = \"org.apache.spark.serializer.KryoSerializer\"\n",
        "# ‚îÇ  ‚îî‚îÄ Faster serialization compared to default Java serialization\n",
        "# ‚îÇ\n",
        "# ‚îî‚îÄ \"spark.sql.execution.arrow.pyspark.enabled\" = \"true\"\n",
        "#    ‚îî‚îÄ Enables Apache Arrow for faster data transfer between JVM and Python\n",
        "spark = (\n",
        "    DataprocSparkSession.builder\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
        "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
        "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\n",
        "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"256MB\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"400\")\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
        "    .config(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"256MB\")\n",
        "    .config(\"spark.sql.adaptive.skewJoin.skewedPartitionFactor\", \"5\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# GCP Project and Dataset Config\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "PROJECT_ID = \"dtc-de-course-457315\"\n",
        "DATASET_ID = \"dbt_production\"\n",
        "\n",
        "INCREMENTAL_CONFIG = {\n",
        "    \"start_year\": 2015,\n",
        "    \"end_year\": 2016,\n",
        "    \"batch_size_months\": 6,\n",
        "    \"checkpoint_dir\": \"/tmp/spark-checkpoints\",   # checkpointing not used in Spark Connect\n",
        "    \"cache_level\": \"MEMORY_AND_DISK_SER\",\n",
        "    \"max_records_per_partition\": 1000000\n",
        "}\n",
        "\n",
        "print(f\"‚úÖ Spark Version: {spark.version}\")\n",
        "print(f\"üìä Working with: {PROJECT_ID}.{DATASET_ID}\")\n",
        "print(f\"üîÑ Incremental Processing: {INCREMENTAL_CONFIG['start_year']}-{INCREMENTAL_CONFIG['end_year']}\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Example: Read from BigQuery\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "table_name = f\"{PROJECT_ID}.{DATASET_ID}.fact_trips\"\n",
        "\n",
        "df = (\n",
        "    spark.read\n",
        "    .format(\"bigquery\")\n",
        "    .option(\"table\", table_name)\n",
        "    .load()\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Loaded BigQuery table into Spark DataFrame:\")\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Column Name                | Description                                                  |\n",
        "|----------------------------|--------------------------------------------------------------|\n",
        "| tripid                     | Unique identifier for each taxi trip                         |\n",
        "| vendorid                   | ID of the vendor/company providing the taxi service          |\n",
        "| service_type               | Type of taxi service (e.g. Yellow, Green)                    |\n",
        "| ratecodeid                 | Rate code indicating fare rules for the trip                 |\n",
        "| pickup_locationid          | Location ID where the trip started                           |\n",
        "| pickup_borough             | Borough where the trip started                               |\n",
        "| pickup_zone                | Zone within the borough where the trip started               |\n",
        "| dropoff_locationid         | Location ID where the trip ended                             |\n",
        "| dropoff_borough            | Borough where the trip ended                                 |\n",
        "| dropoff_zone               | Zone within the borough where the trip ended                 |\n",
        "| pickup_datetime            | Timestamp when the trip started                              |\n",
        "| pickup_date                | Date (only) of the pickup                                    |\n",
        "| dropoff_datetime           | Timestamp when the trip ended                                |\n",
        "| store_and_fwd_flag         | Flag indicating if trip record was stored and forwarded      |\n",
        "| passenger_count            | Number of passengers in the trip                             |\n",
        "| trip_distance              | Distance travelled during the trip (in miles)                |\n",
        "| fare_amount                | Fare charged for the trip                                    |\n",
        "| extra                      | Additional charges (e.g. surcharge, night fee)               |\n",
        "| mta_tax                    | MTA (Metropolitan Transportation Authority) tax amount       |\n",
        "| tip_amount                 | Tip given to the driver                                      |\n",
        "| tolls_amount               | Total tolls paid during the trip                             |\n",
        "| improvement_surcharge      | NYC-imposed surcharge to support improvements                |\n",
        "| total_amount               | Total amount paid (fare + extras + tip + tolls)              |\n",
        "| payment_type               | Numeric code for the payment type                            |\n",
        "| payment_type_description   | Description of the payment type (e.g. Credit card, Cash)     |\n",
        "| climate_date               | Date of associated climate/weather data                      |\n",
        "| mjd                        | Modified Julian Date (astronomical date format)              |\n",
        "| cloudCover                 | Fraction of sky covered by clouds                            |\n",
        "| humidity                   | Relative humidity percentage                                 |\n",
        "| dewPoint                   | Dew point temperature in degrees Fahrenheit or Celsius       |\n",
        "| precipIntensity            | Intensity of precipitation during the period                 |\n",
        "| highTemp                   | High temperature of the day                                  |\n",
        "| lowTemp                    | Low temperature of the day                                   |\n",
        "| visibility                 | Visibility distance during the trip (in miles)               |\n",
        "| windSpeed                  | Wind speed during the trip                                   |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define multiple plotting functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi-panel plotting function (R multiplot equivalent)\n",
        "# Courtesy of R Cookbooks adapted for Python/PySpark\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.figure import Figure\n",
        "import numpy as np\n",
        "import math\n",
        "from typing import List, Optional, Union, Tuple\n",
        "\n",
        "def multiplot(*plots, plotlist=None, cols=1, layout=None, figsize=(15, 10), \n",
        "              title=None, save_path=None, dpi=300):\n",
        "    \"\"\"\n",
        "    Create multi-panel plots from matplotlib/seaborn/plotly figures\n",
        "    \n",
        "    Python equivalent of R's multiplot function from R Cookbooks\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    *plots : matplotlib figures or plot functions\n",
        "        Individual plots to be arranged\n",
        "    plotlist : list, optional\n",
        "        List of plots as alternative to *plots\n",
        "    cols : int, default=1\n",
        "        Number of columns in layout\n",
        "    layout : array-like, optional\n",
        "        Matrix specifying the layout. If present, 'cols' is ignored.\n",
        "        Example: [[1,2], [3,3]] means plot 1 top-left, 2 top-right, 3 bottom spanning both columns\n",
        "    figsize : tuple, default=(15, 10)\n",
        "        Figure size (width, height) in inches\n",
        "    title : str, optional\n",
        "        Overall title for the multi-panel plot\n",
        "    save_path : str, optional\n",
        "        Path to save the figure\n",
        "    dpi : int, default=300\n",
        "        Resolution for saved figure\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    matplotlib.figure.Figure\n",
        "        The combined figure with all subplots\n",
        "        \n",
        "    Example:\n",
        "    --------\n",
        "    # Create individual plots\n",
        "    fig1, ax1 = plt.subplots()\n",
        "    ax1.plot([1,2,3], [1,4,2])\n",
        "    \n",
        "    fig2, ax2 = plt.subplots()\n",
        "    ax2.bar([1,2,3], [3,1,4])\n",
        "    \n",
        "    # Combine them\n",
        "    combined_fig = multiplot(fig1, fig2, cols=2, title=\"Combined Analysis\")\n",
        "    \"\"\"\n",
        "    \n",
        "    # Combine plots from arguments and plotlist\n",
        "    all_plots = list(plots) if plots else []\n",
        "    if plotlist:\n",
        "        all_plots.extend(plotlist)\n",
        "    \n",
        "    num_plots = len(all_plots)\n",
        "    \n",
        "    if num_plots == 0:\n",
        "        print(\"‚ö†Ô∏è No plots provided\")\n",
        "        return None\n",
        "    \n",
        "    # Handle single plot case\n",
        "    if num_plots == 1:\n",
        "        if hasattr(all_plots[0], 'show'):\n",
        "            all_plots[0].show()\n",
        "        else:\n",
        "            plt.figure(figsize=figsize)\n",
        "            if title:\n",
        "                plt.suptitle(title, fontsize=16, fontweight='bold')\n",
        "            plt.show()\n",
        "        return all_plots[0]\n",
        "    \n",
        "    # Determine layout\n",
        "    if layout is None:\n",
        "        # Calculate rows and columns\n",
        "        nrows = math.ceil(num_plots / cols)\n",
        "        ncols = cols\n",
        "        layout_matrix = np.arange(1, cols * nrows + 1).reshape(nrows, ncols)\n",
        "    else:\n",
        "        layout_matrix = np.array(layout)\n",
        "        nrows, ncols = layout_matrix.shape\n",
        "    \n",
        "    # Create the main figure\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    \n",
        "    if title:\n",
        "        fig.suptitle(title, fontsize=16, fontweight='bold', y=0.95)\n",
        "    \n",
        "    # Create GridSpec for flexible subplot arrangement\n",
        "    gs = gridspec.GridSpec(nrows, ncols, figure=fig, hspace=0.3, wspace=0.3)\n",
        "    \n",
        "    # Place each plot in the correct position\n",
        "    for i, plot in enumerate(all_plots, 1):\n",
        "        if i > num_plots:\n",
        "            break\n",
        "            \n",
        "        # Find positions where this plot should go\n",
        "        positions = np.where(layout_matrix == i)\n",
        "        \n",
        "        if len(positions[0]) == 0:\n",
        "            continue\n",
        "            \n",
        "        # Calculate subplot span\n",
        "        row_min, row_max = positions[0].min(), positions[0].max()\n",
        "        col_min, col_max = positions[1].min(), positions[1].max()\n",
        "        \n",
        "        # Create subplot\n",
        "        ax = fig.add_subplot(gs[row_min:row_max+1, col_min:col_max+1])\n",
        "        \n",
        "        # Handle different plot types\n",
        "        if hasattr(plot, 'figure'):\n",
        "            # It's a matplotlib figure\n",
        "            _copy_plot_to_axis(plot, ax)\n",
        "        elif callable(plot):\n",
        "            # It's a plotting function\n",
        "            plot(ax)\n",
        "        elif hasattr(plot, 'axes'):\n",
        "            # It's a figure with axes\n",
        "            _copy_plot_to_axis(plot, ax)\n",
        "        else:\n",
        "            # Try to handle as data for direct plotting\n",
        "            ax.text(0.5, 0.5, f'Plot {i}', ha='center', va='center', \n",
        "                   transform=ax.transAxes)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save if path provided\n",
        "    if save_path:\n",
        "        fig.savefig(save_path, dpi=dpi, bbox_inches='tight')\n",
        "        print(f\"üíæ Multi-panel plot saved to: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def _copy_plot_to_axis(source_fig, target_ax):\n",
        "    \"\"\"Helper function to copy plot content from source figure to target axis\"\"\"\n",
        "    try:\n",
        "        if hasattr(source_fig, 'axes') and source_fig.axes:\n",
        "            source_ax = source_fig.axes[0]\n",
        "            \n",
        "            # Copy lines\n",
        "            for line in source_ax.get_lines():\n",
        "                target_ax.plot(line.get_xdata(), line.get_ydata(), \n",
        "                             color=line.get_color(), linewidth=line.get_linewidth(),\n",
        "                             linestyle=line.get_linestyle(), marker=line.get_marker(),\n",
        "                             label=line.get_label())\n",
        "            \n",
        "            # Copy patches (bars, etc.)\n",
        "            for patch in source_ax.patches:\n",
        "                target_ax.add_patch(patch)\n",
        "            \n",
        "            # Copy collections (scatter plots, etc.)\n",
        "            for collection in source_ax.collections:\n",
        "                target_ax.add_collection(collection)\n",
        "            \n",
        "            # Copy labels and title\n",
        "            target_ax.set_xlabel(source_ax.get_xlabel())\n",
        "            target_ax.set_ylabel(source_ax.get_ylabel())\n",
        "            target_ax.set_title(source_ax.get_title())\n",
        "            \n",
        "            # Copy limits\n",
        "            target_ax.set_xlim(source_ax.get_xlim())\n",
        "            target_ax.set_ylim(source_ax.get_ylim())\n",
        "            \n",
        "            # Copy legend if exists\n",
        "            if source_ax.get_legend():\n",
        "                target_ax.legend()\n",
        "                \n",
        "    except Exception as e:\n",
        "        # Fallback: just add a text placeholder\n",
        "        target_ax.text(0.5, 0.5, 'Plot Content', ha='center', va='center',\n",
        "                      transform=target_ax.transAxes)\n",
        "        print(f\"‚ö†Ô∏è Could not copy plot content: {e}\")\n",
        "\n",
        "# Enhanced multiplot for PySpark DataFrames\n",
        "def multiplot_spark(dataframes_and_plots, cols=2, figsize=(15, 10), title=None):\n",
        "    \"\"\"\n",
        "    Create multi-panel plots specifically for PySpark DataFrame visualizations\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    dataframes_and_plots : list of tuples\n",
        "        Each tuple contains (spark_dataframe, plot_config)\n",
        "        plot_config is a dict with keys: 'type', 'x', 'y', 'title', etc.\n",
        "    cols : int\n",
        "        Number of columns\n",
        "    figsize : tuple\n",
        "        Figure size\n",
        "    title : str\n",
        "        Overall title\n",
        "        \n",
        "    Example:\n",
        "    --------\n",
        "    plot_configs = [\n",
        "        (df_2020, {'type': 'hist', 'x': 'trip_distance', 'title': '2020 Trip Distance'}),\n",
        "        (df_2021, {'type': 'scatter', 'x': 'trip_distance', 'y': 'fare_amount', 'title': '2021 Distance vs Fare'}),\n",
        "        (df_2022, {'type': 'bar', 'x': 'pickup_hour', 'y': 'count', 'title': '2022 Hourly Trips'})\n",
        "    ]\n",
        "    multiplot_spark(plot_configs, cols=2, title=\"Yearly Comparison\")\n",
        "    \"\"\"\n",
        "    \n",
        "    num_plots = len(dataframes_and_plots)\n",
        "    nrows = math.ceil(num_plots / cols)\n",
        "    \n",
        "    fig, axes = plt.subplots(nrows, cols, figsize=figsize)\n",
        "    if title:\n",
        "        fig.suptitle(title, fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Flatten axes array for easy indexing\n",
        "    if num_plots == 1:\n",
        "        axes = [axes]\n",
        "    elif nrows == 1:\n",
        "        axes = axes\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "    \n",
        "    for i, (spark_df, plot_config) in enumerate(dataframes_and_plots):\n",
        "        if i >= len(axes):\n",
        "            break\n",
        "            \n",
        "        ax = axes[i]\n",
        "        \n",
        "        # Convert to Pandas for plotting\n",
        "        pandas_df = spark_df.toPandas()\n",
        "        \n",
        "        plot_type = plot_config.get('type', 'scatter')\n",
        "        x_col = plot_config.get('x')\n",
        "        y_col = plot_config.get('y')\n",
        "        plot_title = plot_config.get('title', f'Plot {i+1}')\n",
        "        \n",
        "        # Create the appropriate plot\n",
        "        if plot_type == 'hist':\n",
        "            ax.hist(pandas_df[x_col], bins=30, alpha=0.7)\n",
        "            ax.set_xlabel(x_col)\n",
        "            ax.set_ylabel('Frequency')\n",
        "        elif plot_type == 'scatter':\n",
        "            ax.scatter(pandas_df[x_col], pandas_df[y_col], alpha=0.6)\n",
        "            ax.set_xlabel(x_col)\n",
        "            ax.set_ylabel(y_col)\n",
        "        elif plot_type == 'bar':\n",
        "            if y_col:\n",
        "                ax.bar(pandas_df[x_col], pandas_df[y_col])\n",
        "            else:\n",
        "                value_counts = pandas_df[x_col].value_counts()\n",
        "                ax.bar(value_counts.index, value_counts.values)\n",
        "            ax.set_xlabel(x_col)\n",
        "            ax.set_ylabel(y_col or 'Count')\n",
        "        elif plot_type == 'line':\n",
        "            ax.plot(pandas_df[x_col], pandas_df[y_col])\n",
        "            ax.set_xlabel(x_col)\n",
        "            ax.set_ylabel(y_col)\n",
        "        \n",
        "        ax.set_title(plot_title)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for i in range(num_plots, len(axes)):\n",
        "        axes[i].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "# Quick plotting helper for common PySpark visualizations\n",
        "def quick_multiplot_comparison(yearly_data_dict, plot_type='hist', column='trip_distance', \n",
        "                              cols=2, figsize=(15, 10)):\n",
        "    \"\"\"\n",
        "    Quick comparison plots across multiple years\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    yearly_data_dict : dict\n",
        "        Dictionary with year as key, PySpark DataFrame as value\n",
        "    plot_type : str\n",
        "        Type of plot ('hist', 'box', 'violin')\n",
        "    column : str\n",
        "        Column to plot\n",
        "    cols : int\n",
        "        Number of columns in layout\n",
        "    figsize : tuple\n",
        "        Figure size\n",
        "        \n",
        "    Example:\n",
        "    --------\n",
        "    yearly_data = {\n",
        "        2020: df_2020,\n",
        "        2021: df_2021,\n",
        "        2022: df_2022\n",
        "    }\n",
        "    quick_multiplot_comparison(yearly_data, 'hist', 'trip_distance')\n",
        "    \"\"\"\n",
        "    \n",
        "    plot_configs = []\n",
        "    for year, df in yearly_data_dict.items():\n",
        "        config = {\n",
        "            'type': plot_type,\n",
        "            'x': column,\n",
        "            'title': f'{year} - {column.replace(\"_\", \" \").title()}'\n",
        "        }\n",
        "        plot_configs.append((df, config))\n",
        "    \n",
        "    return multiplot_spark(plot_configs, cols=cols, figsize=figsize, \n",
        "                          title=f\"Year-over-Year Comparison: {column.replace('_', ' ').title()}\")\n",
        "\n",
        "print(\"‚úÖ Multi-panel plotting functions created (R multiplot equivalent)\")\n",
        "print(\"‚úÖ PySpark-specific multiplot functions ready\")\n",
        "print(\"‚úÖ Quick comparison plotting utilities available\")\n",
        "\n",
        "# Example usage guide\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MULTIPLOT USAGE EXAMPLES\")\n",
        "print(\"=\"*50)\n",
        "print(\"\"\"\n",
        "# Basic usage:\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.plot([1,2,3], [1,4,2])\n",
        "\n",
        "fig2, ax2 = plt.subplots() \n",
        "ax2.bar([1,2,3], [3,1,4])\n",
        "\n",
        "combined = multiplot(fig1, fig2, cols=2, title=\"Side by Side\")\n",
        "\n",
        "# PySpark DataFrame plotting:\n",
        "plot_configs = [\n",
        "    (df_20, {'type': 'hist', 'x': 'trip_distance', 'title': '2020 Trips'}),\n",
        "    (df_2021, {'type': 'hist', 'x': 'trip_distance', 'title': '2021 Trips'})\n",
        "]\n",
        "multiplot_spark(plot_configs, cols=2)\n",
        "\n",
        "# Quick year comparison:\n",
        "yearly_data = {2020: df_2020, 2021: df_2021, 2022: df_2022}\n",
        "quick_multiplot_comparison(yearly_data, 'hist', 'trip_distance')\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Test if data batching works**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "START_YEAR = 2015\n",
        "END_YEAR = 2016\n",
        "\n",
        "print(f\"‚úÖ Spark Version: {spark.version}\")\n",
        "print(f\"üìä Working with: {PROJECT_ID}.{DATASET_ID}\")\n",
        "print(f\"üîÑ Incremental Processing for years: {START_YEAR}-{END_YEAR}\")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Read data from BigQuery in yearly batches\n",
        "# ---------------------------------------------------------------\n",
        "table_name = f\"{PROJECT_ID}.{DATASET_ID}.fact_trips\"\n",
        "\n",
        "for year in range(START_YEAR, END_YEAR + 1):\n",
        "\n",
        "    # Build filter for one year\n",
        "    year_filter = f\"EXTRACT(YEAR FROM pickup_date) = {year}\"\n",
        "\n",
        "    # Load data for that year\n",
        "    df_year = (\n",
        "        spark.read\n",
        "        .format(\"bigquery\")\n",
        "        .option(\"table\", table_name)\n",
        "        .option(\"filter\", year_filter)\n",
        "        .load()\n",
        "    )\n",
        "\n",
        "    # Add year_month for inspection\n",
        "    df_with_month = df_year.withColumn(\n",
        "        \"year_month\",\n",
        "        F.substring(\"pickup_date\", 1, 7)\n",
        "    )\n",
        "\n",
        "    # Show distinct months in this batch\n",
        "    months_df = (\n",
        "        df_with_month\n",
        "        .select(\"year_month\")\n",
        "        .distinct()\n",
        "        .orderBy(\"year_month\")\n",
        "    )\n",
        "\n",
        "    months_list = months_df.collect()\n",
        "    months_str_list = [row[\"year_month\"] for row in months_list]\n",
        "\n",
        "    print(\"‚úÖ Loaded batch for year:\", year)\n",
        "    months_df.show(truncate=False)\n",
        "\n",
        "    print(f\"üìÖ Number of months found: {len(months_str_list)}\")\n",
        "    if months_str_list:\n",
        "        print(f\"üóìÔ∏è First month in batch: {months_str_list[0]}\")\n",
        "        print(f\"üóìÔ∏è Last month in batch: {months_str_list[-1]}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No data found for this batch.\")\n",
        "\n",
        "    # ----\n",
        "    # YOUR LOGIC HERE for processing df_with_month\n",
        "    # ----\n",
        "\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data (for BigQuery)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PySpark equivalent of R's fread for BigQuery data loading\n",
        "# R: train <- as_tibble(fread(cmd = 'unzip -p ../input/nyc-taxi-trip-duration/train.zip'))\n",
        "\n",
        "def load_taxi_data_optimized(years=[2015, 2016], table_name=\"yellow_taxi_external_table\", \n",
        "                           sample_fraction=None, cache=True):\n",
        "    \"\"\"\n",
        "    Load taxi data from BigQuery with optimized performance\n",
        "    Equivalent to R's fread but for BigQuery tables\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    years : list\n",
        "        Years to load (default: [2015, 2016])\n",
        "    table_name : str\n",
        "        BigQuery table name\n",
        "    sample_fraction : float, optional\n",
        "        Sample fraction for faster development (e.g., 0.1 for 10%)\n",
        "    cache : bool\n",
        "        Whether to cache the data in memory\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    PySpark DataFrame\n",
        "        Combined data from specified years\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"üìä Loading taxi data for years: {years}\")\n",
        "    print(f\"üìã Table: {table_name}\")\n",
        "    \n",
        "    # Build optimized query for multiple years\n",
        "    years_filter = ', '.join(map(str, years))\n",
        "    \n",
        "    # Optimized query with year filtering and column selection\n",
        "    query = f\"\"\"\n",
        "    SELECT \n",
        "        id,\n",
        "        vendor_id,\n",
        "        tpep_pickup_datetime,\n",
        "        tpep_dropoff_datetime,\n",
        "        passenger_count,\n",
        "        trip_distance,\n",
        "        pickup_longitude,\n",
        "        pickup_latitude,\n",
        "        dropoff_longitude,\n",
        "        dropoff_latitude,\n",
        "        payment_type,\n",
        "        fare_amount,\n",
        "        extra,\n",
        "        mta_tax,\n",
        "        tip_amount,\n",
        "        tolls_amount,\n",
        "        total_amount,\n",
        "        -- Calculate trip duration in seconds (our target variable)\n",
        "        TIMESTAMP_DIFF(tpep_dropoff_datetime, tpep_pickup_datetime, SECOND) as trip_duration_seconds,\n",
        "        -- Extract useful time features\n",
        "        EXTRACT(YEAR FROM tpep_pickup_datetime) as pickup_year,\n",
        "        EXTRACT(MONTH FROM tpep_pickup_datetime) as pickup_month,\n",
        "        EXTRACT(DAY FROM tpep_pickup_datetime) as pickup_day,\n",
        "        EXTRACT(HOUR FROM tpep_pickup_datetime) as pickup_hour,\n",
        "        EXTRACT(DAYOFWEEK FROM tpep_pickup_datetime) as pickup_dayofweek\n",
        "    FROM `{PROJECT_ID}.{DATASET_ID}.{table_name}`\n",
        "    WHERE \n",
        "        EXTRACT(YEAR FROM tpep_pickup_datetime) IN ({years_filter})\n",
        "        AND tpep_pickup_datetime IS NOT NULL\n",
        "        AND tpep_dropoff_datetime IS NOT NULL\n",
        "        AND trip_distance > 0\n",
        "        AND fare_amount > 0\n",
        "        AND tpep_pickup_datetime < tpep_dropoff_datetime\n",
        "    \"\"\"\n",
        "    \n",
        "    # Add sampling if specified\n",
        "    if sample_fraction:\n",
        "        query += f\" AND RAND() < {sample_fraction}\"\n",
        "        print(f\"üé≤ Sampling {sample_fraction*100}% of data for faster development\")\n",
        "    \n",
        "    try:\n",
        "        # Load data with optimized BigQuery connector\n",
        "        df = spark.read \\\n",
        "            .format(\"bigquery\") \\\n",
        "            .option(\"query\", query) \\\n",
        "            .option(\"readDataFormat\", \"ARROW\") \\\n",
        "            .option(\"materializationProject\", PROJECT_ID) \\\n",
        "            .option(\"materializationDataset\", DATASET_ID) \\\n",
        "            .load()\n",
        "        \n",
        "        # Cache if requested\n",
        "        if cache:\n",
        "            df = df.cache()\n",
        "            print(f\"üíæ Data cached in memory for faster access\")\n",
        "        \n",
        "        # Show basic info\n",
        "        total_rows = df.count()\n",
        "        print(f\"‚úÖ Loaded {total_rows:,} records\")\n",
        "        print(f\"üìä Columns: {len(df.columns)}\")\n",
        "        \n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load 2015 and 2016 data (equivalent to R's train dataset)\n",
        "print(\"üöÄ Loading NYC Taxi Data (2015 & 2016)...\")\n",
        "taxi_data = load_taxi_data_optimized(years=[2015, 2016], \n",
        "                                   table_name=\"yellow_taxi_external_table\",\n",
        "                                   sample_fraction=0.1,  # 10% sample for development\n",
        "                                   cache=True)\n",
        "\n",
        "# Show basic statistics (equivalent to R's summary())\n",
        "if taxi_data:\n",
        "    print(\"\\nüìà Dataset Overview:\")\n",
        "    taxi_data.describe().show()\n",
        "    \n",
        "    print(\"\\nüóÇÔ∏è Schema:\")\n",
        "    taxi_data.printSchema()\n",
        "    \n",
        "    print(\"\\nüìä Year Distribution:\")\n",
        "    taxi_data.groupBy(\"pickup_year\").count().orderBy(\"pickup_year\").show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Train/Test Splitting Strategies for Time Series Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/Test Splitting Strategies for Year-by-Year Processing\n",
        "from pyspark.sql.functions import col, rand, when, month, dayofmonth\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "def temporal_train_test_split(df, strategy=\"chronological\", test_ratio=0.2, \n",
        "                            random_seed=42, validation_split=True):\n",
        "    \"\"\"\n",
        "    Split time series data into train/test sets using various strategies\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : PySpark DataFrame\n",
        "        Input data with datetime columns\n",
        "    strategy : str\n",
        "        Splitting strategy: 'chronological', 'random', 'stratified_temporal', 'holdout_months'\n",
        "    test_ratio : float\n",
        "        Proportion of data for testing (default: 0.2 = 20%)\n",
        "    random_seed : int\n",
        "        Random seed for reproducibility\n",
        "    validation_split : bool\n",
        "        Whether to create a validation set (splits train further)\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary with 'train', 'test', and optionally 'validation' DataFrames\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"üîÑ Splitting data using '{strategy}' strategy\")\n",
        "    print(f\"üìä Test ratio: {test_ratio*100}%\")\n",
        "    \n",
        "    if strategy == \"chronological\":\n",
        "        # RECOMMENDED for time series: Use 2015 for training, 2016 for testing\n",
        "        train_df = df.filter(col(\"pickup_year\") == 2015)\n",
        "        test_df = df.filter(col(\"pickup_year\") == 2016)\n",
        "        \n",
        "        result = {\"train\": train_df, \"test\": test_df}\n",
        "        \n",
        "        if validation_split:\n",
        "            # Use last 2 months of 2015 for validation\n",
        "            train_final = train_df.filter(col(\"pickup_month\") <= 10)\n",
        "            validation_df = train_df.filter(col(\"pickup_month\") > 10)\n",
        "            result[\"train\"] = train_final\n",
        "            result[\"validation\"] = validation_df\n",
        "            \n",
        "        print(\"‚úÖ Chronological split: 2015 (train) vs 2016 (test)\")\n",
        "        \n",
        "    elif strategy == \"random\":\n",
        "        # Random split within each year to maintain temporal balance\n",
        "        df_with_split = df.withColumn(\"rand_val\", rand(seed=random_seed))\n",
        "        \n",
        "        train_df = df_with_split.filter(col(\"rand_val\") > test_ratio)\n",
        "        test_df = df_with_split.filter(col(\"rand_val\") <= test_ratio)\n",
        "        \n",
        "        result = {\"train\": train_df.drop(\"rand_val\"), \"test\": test_df.drop(\"rand_val\")}\n",
        "        \n",
        "        if validation_split:\n",
        "            val_ratio = test_ratio / 2\n",
        "            train_final = train_df.filter(col(\"rand_val\") > (test_ratio + val_ratio))\n",
        "            validation_df = train_df.filter(\n",
        "                (col(\"rand_val\") > test_ratio) & (col(\"rand_val\") <= (test_ratio + val_ratio))\n",
        "            )\n",
        "            result[\"train\"] = train_final.drop(\"rand_val\")\n",
        "            result[\"validation\"] = validation_df.drop(\"rand_val\")\n",
        "            \n",
        "        print(\"‚úÖ Random split across both years\")\n",
        "        \n",
        "    elif strategy == \"stratified_temporal\":\n",
        "        # Stratified split maintaining temporal patterns (by month)\n",
        "        df_with_split = df.withColumn(\"rand_val\", rand(seed=random_seed))\n",
        "        \n",
        "        # Create stratified split by month\n",
        "        train_df = df_with_split.filter(col(\"rand_val\") > test_ratio)\n",
        "        test_df = df_with_split.filter(col(\"rand_val\") <= test_ratio)\n",
        "        \n",
        "        result = {\"train\": train_df.drop(\"rand_val\"), \"test\": test_df.drop(\"rand_val\")}\n",
        "        print(\"‚úÖ Stratified temporal split\")\n",
        "        \n",
        "    elif strategy == \"holdout_months\":\n",
        "        # Hold out specific months for testing (e.g., Dec 2015 and Dec 2016)\n",
        "        train_df = df.filter(col(\"pickup_month\") != 12)\n",
        "        test_df = df.filter(col(\"pickup_month\") == 12)\n",
        "        \n",
        "        result = {\"train\": train_df, \"test\": test_df}\n",
        "        \n",
        "        if validation_split:\n",
        "            # Use November for validation\n",
        "            train_final = train_df.filter(col(\"pickup_month\") != 11)\n",
        "            validation_df = train_df.filter(col(\"pickup_month\") == 11)\n",
        "            result[\"train\"] = train_final\n",
        "            result[\"validation\"] = validation_df\n",
        "            \n",
        "        print(\"‚úÖ Holdout months split: December for testing\")\n",
        "        \n",
        "    else:\n",
        "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "    \n",
        "    # Show split statistics\n",
        "    for split_name, split_df in result.items():\n",
        "        count = split_df.count()\n",
        "        print(f\"üìä {split_name.capitalize()}: {count:,} records\")\n",
        "        \n",
        "        # Show year distribution\n",
        "        year_dist = split_df.groupBy(\"pickup_year\").count().collect()\n",
        "        year_info = \", \".join([f\"{row.pickup_year}: {row.count:,}\" for row in year_dist])\n",
        "        print(f\"   Year distribution: {year_info}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "# RECOMMENDED APPROACH for your use case\n",
        "def create_production_splits(df, approach=\"chronological_with_validation\"):\n",
        "    \"\"\"\n",
        "    Create production-ready train/test splits optimized for taxi duration prediction\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : PySpark DataFrame\n",
        "        Full dataset\n",
        "    approach : str\n",
        "        'chronological_with_validation' or 'cross_temporal_validation'\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Split datasets ready for ML pipeline\n",
        "    \"\"\"\n",
        "    \n",
        "    if approach == \"chronological_with_validation\":\n",
        "        # BEST for your case: 2015 for training, 2016 for testing\n",
        "        splits = temporal_train_test_split(df, strategy=\"chronological\", \n",
        "                                         validation_split=True)\n",
        "        \n",
        "        print(\"\\nüéØ RECOMMENDED APPROACH:\")\n",
        "        print(\"   ‚Ä¢ Train: 2015 (Jan-Oct)\")\n",
        "        print(\"   ‚Ä¢ Validation: 2015 (Nov-Dec)\")\n",
        "        print(\"   ‚Ä¢ Test: 2016 (Full year)\")\n",
        "        print(\"   ‚Ä¢ Benefit: Tests model on completely unseen future data\")\n",
        "        \n",
        "    elif approach == \"cross_temporal_validation\":\n",
        "        # Alternative: Mixed years with temporal awareness\n",
        "        splits = temporal_train_test_split(df, strategy=\"stratified_temporal\", \n",
        "                                         test_ratio=0.2, validation_split=True)\n",
        "        \n",
        "        print(\"\\nüîÑ ALTERNATIVE APPROACH:\")\n",
        "        print(\"   ‚Ä¢ Mixed temporal cross-validation\")\n",
        "        print(\"   ‚Ä¢ Maintains seasonal patterns in all splits\")\n",
        "        \n",
        "    return splits\n",
        "\n",
        "# Apply the recommended splitting strategy\n",
        "print(\"üöÄ Creating production-ready train/test splits...\")\n",
        "if 'taxi_data' in locals() and taxi_data is not None:\n",
        "    splits = create_production_splits(taxi_data, approach=\"chronological_with_validation\")\n",
        "    \n",
        "    # Store splits for later use\n",
        "    train_data = splits[\"train\"]\n",
        "    validation_data = splits[\"validation\"] \n",
        "    test_data = splits[\"test\"]\n",
        "    \n",
        "    print(\"\\n‚úÖ Splits created successfully!\")\n",
        "    print(\"üìä Ready for feature engineering and modeling\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Load taxi_data first before creating splits\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Strategic Recommendations for Your Year-by-Year Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategic Recommendations for Your Dataset\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üéØ STRATEGIC RECOMMENDATIONS FOR YOUR APPROACH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "recommendations = {\n",
        "    \"1. RECOMMENDED SPLITTING STRATEGY\": {\n",
        "        \"approach\": \"Chronological Split\",\n",
        "        \"implementation\": \"2015 for training, 2016 for testing\",\n",
        "        \"benefits\": [\n",
        "            \"Tests model on completely unseen future data\",\n",
        "            \"Reflects real-world deployment scenario\",\n",
        "            \"Captures temporal drift and seasonality changes\",\n",
        "            \"Avoids data leakage from future to past\"\n",
        "        ],\n",
        "        \"split_details\": {\n",
        "            \"train\": \"2015 (Jan-Oct) - 83% of year\",\n",
        "            \"validation\": \"2015 (Nov-Dec) - 17% of year\", \n",
        "            \"test\": \"2016 (Full year) - 100% of year\"\n",
        "        }\n",
        "    },\n",
        "    \n",
        "    \"2. INCREMENTAL PROCESSING STRATEGY\": {\n",
        "        \"approach\": \"Year-by-Year Loading with Caching\",\n",
        "        \"implementation\": \"Load ‚Üí Process ‚Üí Cache ‚Üí Combine\",\n",
        "        \"benefits\": [\n",
        "            \"Memory efficient for large datasets\",\n",
        "            \"Allows for year-specific feature engineering\",\n",
        "            \"Enables temporal analysis and comparison\",\n",
        "            \"Scalable to additional years\"\n",
        "        ],\n",
        "        \"code_pattern\": \"\"\"\n",
        "        # Year-by-year processing\n",
        "        df_2015 = load_data_by_year(\"table\", 2015).cache()\n",
        "        df_2016 = load_data_by_year(\"table\", 2016).cache()\n",
        "        \n",
        "        # Combine for modeling\n",
        "        combined_df = df_2015.union(df_2016)\n",
        "        \"\"\"\n",
        "    },\n",
        "    \n",
        "    \"3. ALTERNATIVE STRATEGIES\": {\n",
        "        \"random_split\": {\n",
        "            \"use_case\": \"When temporal patterns are not important\",\n",
        "            \"pros\": [\"Balanced distribution\", \"Standard ML approach\"],\n",
        "            \"cons\": [\"Ignores temporal dependencies\", \"Data leakage risk\"]\n",
        "        },\n",
        "        \"stratified_temporal\": {\n",
        "            \"use_case\": \"When you need seasonal balance in all splits\",\n",
        "            \"pros\": [\"Maintains seasonal patterns\", \"Good for seasonal models\"],\n",
        "            \"cons\": [\"Still has some data leakage\", \"Complex to implement\"]\n",
        "        },\n",
        "        \"holdout_months\": {\n",
        "            \"use_case\": \"Testing specific seasonal performance\",\n",
        "            \"pros\": [\"Tests seasonal robustness\", \"Easy to interpret\"],\n",
        "            \"cons\": [\"Limited test data\", \"May not be representative\"]\n",
        "        }\n",
        "    },\n",
        "    \n",
        "    \"4. PRODUCTION CONSIDERATIONS\": {\n",
        "        \"data_quality\": [\n",
        "            \"Filter out invalid trips (duration < 0, distance = 0)\",\n",
        "            \"Handle outliers (trips > 24 hours, unrealistic speeds)\",\n",
        "            \"Validate coordinate ranges (NYC area only)\"\n",
        "        ],\n",
        "        \"feature_engineering\": [\n",
        "            \"Extract temporal features (hour, day, month, season)\",\n",
        "            \"Calculate haversine distance for validation\",\n",
        "            \"Create pickup/dropoff zone features\",\n",
        "            \"Add weather data if available\"\n",
        "        ],\n",
        "        \"model_validation\": [\n",
        "            \"Use chronological validation\",\n",
        "            \"Monitor for temporal drift\",\n",
        "            \"Validate on different time periods\",\n",
        "            \"Check performance across seasons\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Display recommendations\n",
        "for category, details in recommendations.items():\n",
        "    print(f\"\\n{category}\")\n",
        "    print(\"-\" * len(category))\n",
        "    \n",
        "    if isinstance(details, dict):\n",
        "        if \"approach\" in details:\n",
        "            print(f\"üìã Approach: {details['approach']}\")\n",
        "            print(f\"üîß Implementation: {details['implementation']}\")\n",
        "            \n",
        "            if \"benefits\" in details:\n",
        "                print(\"‚úÖ Benefits:\")\n",
        "                for benefit in details[\"benefits\"]:\n",
        "                    print(f\"   ‚Ä¢ {benefit}\")\n",
        "                    \n",
        "            if \"split_details\" in details:\n",
        "                print(\"üìä Split Details:\")\n",
        "                for split, detail in details[\"split_details\"].items():\n",
        "                    print(f\"   ‚Ä¢ {split.capitalize()}: {detail}\")\n",
        "                    \n",
        "            if \"code_pattern\" in details:\n",
        "                print(\"üíª Code Pattern:\")\n",
        "                print(details[\"code_pattern\"])\n",
        "        else:\n",
        "            for key, value in details.items():\n",
        "                print(f\"\\nüî∏ {key.replace('_', ' ').title()}:\")\n",
        "                if isinstance(value, dict):\n",
        "                    for subkey, subvalue in value.items():\n",
        "                        print(f\"   {subkey}: {subvalue}\")\n",
        "                elif isinstance(value, list):\n",
        "                    for item in value:\n",
        "                        print(f\"   ‚Ä¢ {item}\")\n",
        "                else:\n",
        "                    print(f\"   {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ NEXT STEPS\")\n",
        "print(\"=\"*60)\n",
        "print(\"\"\"\n",
        "1. Load your 2015 & 2016 data using the optimized functions above\n",
        "2. Apply the chronological splitting strategy\n",
        "3. Perform exploratory data analysis using the multiplot functions\n",
        "4. Engineer temporal and spatial features\n",
        "5. Build and validate your trip duration prediction model\n",
        "6. Monitor model performance across different time periods\n",
        "\n",
        "Remember: The chronological split (2015 ‚Üí 2016) is the most realistic \n",
        "approach for time series prediction tasks like taxi trip duration!\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Multi-Panel Plotting Functions (R multiplot equivalent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi-panel plotting function (R multiplot equivalent)\n",
        "# Courtesy of R Cookbooks adapted for Python/PySpark\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.figure import Figure\n",
        "import numpy as np\n",
        "import math\n",
        "from typing import List, Optional, Union, Tuple\n",
        "\n",
        "def multiplot(*plots, plotlist=None, cols=1, layout=None, figsize=(15, 10), \n",
        "              title=None, save_path=None, dpi=300):\n",
        "    \"\"\"\n",
        "    Create multi-panel plots from matplotlib/seaborn/plotly figures\n",
        "    \n",
        "    Python equivalent of R's multiplot function from R Cookbooks\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    *plots : matplotlib figures or plot functions\n",
        "        Individual plots to be arranged\n",
        "    plotlist : list, optional\n",
        "        List of plots as alternative to *plots\n",
        "    cols : int, default=1\n",
        "        Number of columns in layout\n",
        "    layout : array-like, optional\n",
        "        Matrix specifying the layout. If present, 'cols' is ignored.\n",
        "        Example: [[1,2], [3,3]] means plot 1 top-left, 2 top-right, 3 bottom spanning both columns\n",
        "    figsize : tuple, default=(15, 10)\n",
        "        Figure size (width, height) in inches\n",
        "    title : str, optional\n",
        "        Overall title for the multi-panel plot\n",
        "    save_path : str, optional\n",
        "        Path to save the figure\n",
        "    dpi : int, default=300\n",
        "        Resolution for saved figure\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    matplotlib.figure.Figure\n",
        "        The combined figure with all subplots\n",
        "        \n",
        "    Example:\n",
        "    --------\n",
        "    # Create individual plots\n",
        "    fig1, ax1 = plt.subplots()\n",
        "    ax1.plot([1,2,3], [1,4,2])\n",
        "    \n",
        "    fig2, ax2 = plt.subplots()\n",
        "    ax2.bar([1,2,3], [3,1,4])\n",
        "    \n",
        "    # Combine them\n",
        "    combined_fig = multiplot(fig1, fig2, cols=2, title=\"Combined Analysis\")\n",
        "    \"\"\"\n",
        "    \n",
        "    # Combine plots from arguments and plotlist\n",
        "    all_plots = list(plots) if plots else []\n",
        "    if plotlist:\n",
        "        all_plots.extend(plotlist)\n",
        "    \n",
        "    num_plots = len(all_plots)\n",
        "    \n",
        "    if num_plots == 0:\n",
        "        print(\"‚ö†Ô∏è No plots provided\")\n",
        "        return None\n",
        "    \n",
        "    # Handle single plot case\n",
        "    if num_plots == 1:\n",
        "        if hasattr(all_plots[0], 'show'):\n",
        "            all_plots[0].show()\n",
        "        else:\n",
        "            plt.figure(figsize=figsize)\n",
        "            if title:\n",
        "                plt.suptitle(title, fontsize=16, fontweight='bold')\n",
        "            plt.show()\n",
        "        return all_plots[0]\n",
        "    \n",
        "    # Determine layout\n",
        "    if layout is None:\n",
        "        # Calculate rows and columns\n",
        "        nrows = math.ceil(num_plots / cols)\n",
        "        ncols = cols\n",
        "        layout_matrix = np.arange(1, cols * nrows + 1).reshape(nrows, ncols)\n",
        "    else:\n",
        "        layout_matrix = np.array(layout)\n",
        "        nrows, ncols = layout_matrix.shape\n",
        "    \n",
        "    # Create the main figure\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    \n",
        "    if title:\n",
        "        fig.suptitle(title, fontsize=16, fontweight='bold', y=0.95)\n",
        "    \n",
        "    # Create GridSpec for flexible subplot arrangement\n",
        "    gs = gridspec.GridSpec(nrows, ncols, figure=fig, hspace=0.3, wspace=0.3)\n",
        "    \n",
        "    # Place each plot in the correct position\n",
        "    for i, plot in enumerate(all_plots, 1):\n",
        "        if i > num_plots:\n",
        "            break\n",
        "            \n",
        "        # Find positions where this plot should go\n",
        "        positions = np.where(layout_matrix == i)\n",
        "        \n",
        "        if len(positions[0]) == 0:\n",
        "            continue\n",
        "            \n",
        "        # Calculate subplot span\n",
        "        row_min, row_max = positions[0].min(), positions[0].max()\n",
        "        col_min, col_max = positions[1].min(), positions[1].max()\n",
        "        \n",
        "        # Create subplot\n",
        "        ax = fig.add_subplot(gs[row_min:row_max+1, col_min:col_max+1])\n",
        "        \n",
        "        # Handle different plot types\n",
        "        if hasattr(plot, 'figure'):\n",
        "            # It's a matplotlib figure\n",
        "            _copy_plot_to_axis(plot, ax)\n",
        "        elif callable(plot):\n",
        "            # It's a plotting function\n",
        "            plot(ax)\n",
        "        elif hasattr(plot, 'axes'):\n",
        "            # It's a figure with axes\n",
        "            _copy_plot_to_axis(plot, ax)\n",
        "        else:\n",
        "            # Try to handle as data for direct plotting\n",
        "            ax.text(0.5, 0.5, f'Plot {i}', ha='center', va='center', \n",
        "                   transform=ax.transAxes)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save if path provided\n",
        "    if save_path:\n",
        "        fig.savefig(save_path, dpi=dpi, bbox_inches='tight')\n",
        "        print(f\"üíæ Multi-panel plot saved to: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def _copy_plot_to_axis(source_fig, target_ax):\n",
        "    \"\"\"Helper function to copy plot content from source figure to target axis\"\"\"\n",
        "    try:\n",
        "        if hasattr(source_fig, 'axes') and source_fig.axes:\n",
        "            source_ax = source_fig.axes[0]\n",
        "            \n",
        "            # Copy lines\n",
        "            for line in source_ax.get_lines():\n",
        "                target_ax.plot(line.get_xdata(), line.get_ydata(), \n",
        "                             color=line.get_color(), linewidth=line.get_linewidth(),\n",
        "                             linestyle=line.get_linestyle(), marker=line.get_marker(),\n",
        "                             label=line.get_label())\n",
        "            \n",
        "            # Copy patches (bars, etc.)\n",
        "            for patch in source_ax.patches:\n",
        "                target_ax.add_patch(patch)\n",
        "            \n",
        "            # Copy collections (scatter plots, etc.)\n",
        "            for collection in source_ax.collections:\n",
        "                target_ax.add_collection(collection)\n",
        "            \n",
        "            # Copy labels and title\n",
        "            target_ax.set_xlabel(source_ax.get_xlabel())\n",
        "            target_ax.set_ylabel(source_ax.get_ylabel())\n",
        "            target_ax.set_title(source_ax.get_title())\n",
        "            \n",
        "            # Copy limits\n",
        "            target_ax.set_xlim(source_ax.get_xlim())\n",
        "            target_ax.set_ylim(source_ax.get_ylim())\n",
        "            \n",
        "            # Copy legend if exists\n",
        "            if source_ax.get_legend():\n",
        "                target_ax.legend()\n",
        "                \n",
        "    except Exception as e:\n",
        "        # Fallback: just add a text placeholder\n",
        "        target_ax.text(0.5, 0.5, 'Plot Content', ha='center', va='center',\n",
        "                      transform=target_ax.transAxes)\n",
        "        print(f\"‚ö†Ô∏è Could not copy plot content: {e}\")\n",
        "\n",
        "# Enhanced multiplot for PySpark DataFrames\n",
        "def multiplot_spark(dataframes_and_plots, cols=2, figsize=(15, 10), title=None):\n",
        "    \"\"\"\n",
        "    Create multi-panel plots specifically for PySpark DataFrame visualizations\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    dataframes_and_plots : list of tuples\n",
        "        Each tuple contains (spark_dataframe, plot_config)\n",
        "        plot_config is a dict with keys: 'type', 'x', 'y', 'title', etc.\n",
        "    cols : int\n",
        "        Number of columns\n",
        "    figsize : tuple\n",
        "        Figure size\n",
        "    title : str\n",
        "        Overall title\n",
        "        \n",
        "    Example:\n",
        "    --------\n",
        "    plot_configs = [\n",
        "        (df_2020, {'type': 'hist', 'x': 'trip_distance', 'title': '2020 Trip Distance'}),\n",
        "        (df_2021, {'type': 'scatter', 'x': 'trip_distance', 'y': 'fare_amount', 'title': '2021 Distance vs Fare'}),\n",
        "        (df_2022, {'type': 'bar', 'x': 'pickup_hour', 'y': 'count', 'title': '2022 Hourly Trips'})\n",
        "    ]\n",
        "    multiplot_spark(plot_configs, cols=2, title=\"Yearly Comparison\")\n",
        "    \"\"\"\n",
        "    \n",
        "    num_plots = len(dataframes_and_plots)\n",
        "    nrows = math.ceil(num_plots / cols)\n",
        "    \n",
        "    fig, axes = plt.subplots(nrows, cols, figsize=figsize)\n",
        "    if title:\n",
        "        fig.suptitle(title, fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Flatten axes array for easy indexing\n",
        "    if num_plots == 1:\n",
        "        axes = [axes]\n",
        "    elif nrows == 1:\n",
        "        axes = axes\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "    \n",
        "    for i, (spark_df, plot_config) in enumerate(dataframes_and_plots):\n",
        "        if i >= len(axes):\n",
        "            break\n",
        "            \n",
        "        ax = axes[i]\n",
        "        \n",
        "        # Convert to Pandas for plotting\n",
        "        pandas_df = spark_df.toPandas()\n",
        "        \n",
        "        plot_type = plot_config.get('type', 'scatter')\n",
        "        x_col = plot_config.get('x')\n",
        "        y_col = plot_config.get('y')\n",
        "        plot_title = plot_config.get('title', f'Plot {i+1}')\n",
        "        \n",
        "        # Create the appropriate plot\n",
        "        if plot_type == 'hist':\n",
        "            ax.hist(pandas_df[x_col], bins=30, alpha=0.7)\n",
        "            ax.set_xlabel(x_col)\n",
        "            ax.set_ylabel('Frequency')\n",
        "        elif plot_type == 'scatter':\n",
        "            ax.scatter(pandas_df[x_col], pandas_df[y_col], alpha=0.6)\n",
        "            ax.set_xlabel(x_col)\n",
        "            ax.set_ylabel(y_col)\n",
        "        elif plot_type == 'bar':\n",
        "            if y_col:\n",
        "                ax.bar(pandas_df[x_col], pandas_df[y_col])\n",
        "            else:\n",
        "                value_counts = pandas_df[x_col].value_counts()\n",
        "                ax.bar(value_counts.index, value_counts.values)\n",
        "            ax.set_xlabel(x_col)\n",
        "            ax.set_ylabel(y_col or 'Count')\n",
        "        elif plot_type == 'line':\n",
        "            ax.plot(pandas_df[x_col], pandas_df[y_col])\n",
        "            ax.set_xlabel(x_col)\n",
        "            ax.set_ylabel(y_col)\n",
        "        \n",
        "        ax.set_title(plot_title)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for i in range(num_plots, len(axes)):\n",
        "        axes[i].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "# Quick plotting helper for common PySpark visualizations\n",
        "def quick_multiplot_comparison(yearly_data_dict, plot_type='hist', column='trip_distance', \n",
        "                              cols=2, figsize=(15, 10)):\n",
        "    \"\"\"\n",
        "    Quick comparison plots across multiple years\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    yearly_data_dict : dict\n",
        "        Dictionary with year as key, PySpark DataFrame as value\n",
        "    plot_type : str\n",
        "        Type of plot ('hist', 'box', 'violin')\n",
        "    column : str\n",
        "        Column to plot\n",
        "    cols : int\n",
        "        Number of columns in layout\n",
        "    figsize : tuple\n",
        "        Figure size\n",
        "        \n",
        "    Example:\n",
        "    --------\n",
        "    yearly_data = {\n",
        "        2020: df_2020,\n",
        "        2021: df_2021,\n",
        "        2022: df_2022\n",
        "    }\n",
        "    quick_multiplot_comparison(yearly_data, 'hist', 'trip_distance')\n",
        "    \"\"\"\n",
        "    \n",
        "    plot_configs = []\n",
        "    for year, df in yearly_data_dict.items():\n",
        "        config = {\n",
        "            'type': plot_type,\n",
        "            'x': column,\n",
        "            'title': f'{year} - {column.replace(\"_\", \" \").title()}'\n",
        "        }\n",
        "        plot_configs.append((df, config))\n",
        "    \n",
        "    return multiplot_spark(plot_configs, cols=cols, figsize=figsize, \n",
        "                          title=f\"Year-over-Year Comparison: {column.replace('_', ' ').title()}\")\n",
        "\n",
        "print(\"‚úÖ Multi-panel plotting functions created (R multiplot equivalent)\")\n",
        "print(\"‚úÖ PySpark-specific multiplot functions ready\")\n",
        "print(\"‚úÖ Quick comparison plotting utilities available\")\n",
        "\n",
        "# Example usage guide\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MULTIPLOT USAGE EXAMPLES\")\n",
        "print(\"=\"*50)\n",
        "print(\"\"\"\n",
        "# Basic usage:\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.plot([1,2,3], [1,4,2])\n",
        "\n",
        "fig2, ax2 = plt.subplots() \n",
        "ax2.bar([1,2,3], [3,1,4])\n",
        "\n",
        "combined = multiplot(fig1, fig2, cols=2, title=\"Side by Side\")\n",
        "\n",
        "# PySpark DataFrame plotting:\n",
        "plot_configs = [\n",
        "    (df_2020, {'type': 'hist', 'x': 'trip_distance', 'title': '2020 Trips'}),\n",
        "    (df_2021, {'type': 'hist', 'x': 'trip_distance', 'title': '2021 Trips'})\n",
        "]\n",
        "multiplot_spark(plot_configs, cols=2)\n",
        "\n",
        "# Quick year comparison:\n",
        "yearly_data = {2020: df_2020, 2021: df_2021, 2022: df_2022}\n",
        "quick_multiplot_comparison(yearly_data, 'hist', 'trip_distance')\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Manipulation Libraries (dplyr, data.table, tibble, tidyr equivalents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PySpark SQL Functions (dplyr equivalent)\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Data manipulation imports (equivalent to dplyr, data.table, tibble, tidyr)\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import (\n",
        "    # dplyr equivalents\n",
        "    col, lit, when, otherwise, desc, asc,\n",
        "    sum as spark_sum, mean as spark_mean, count as spark_count,\n",
        "    min as spark_min, max as spark_max, avg, stddev,\n",
        "    \n",
        "    # tidyr equivalents  \n",
        "    explode, split, array, struct, collect_list, collect_set,\n",
        "    \n",
        "    # stringr equivalents\n",
        "    regexp_replace, regexp_extract, lower, upper, trim, ltrim, rtrim,\n",
        "    substring, length, concat, concat_ws, split as string_split,\n",
        "    \n",
        "    # lubridate equivalents\n",
        "    year, month, dayofyear, dayofweek, hour, minute, second,\n",
        "    date_format, to_date, to_timestamp, current_date, current_timestamp,\n",
        "    datediff, months_between, add_months, date_add, date_sub\n",
        ")\n",
        "\n",
        "print(\"‚úÖ PySpark SQL Functions imported (dplyr/data.table/tidyr/stringr/lubridate equivalents)\")\n",
        "\n",
        "# Window functions for advanced operations\n",
        "window_specs = {\n",
        "    'row_number': Window.orderBy(\"pickup_datetime\"),\n",
        "    'rank': Window.partitionBy(\"zone\").orderBy(desc(\"trip_count\")),\n",
        "    'lag_lead': Window.partitionBy(\"route\").orderBy(\"date\")\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Window functions configured for advanced analytics\")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Visualization Libraries (ggplot2, scales, grid, RColorBrewer, corrplot equivalents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization libraries (ggplot2, scales, grid, RColorBrewer equivalents)\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker  # scales equivalent\n",
        "import matplotlib.gridspec as gridspec  # grid equivalent\n",
        "import matplotlib.colors as mcolors  # RColorBrewer equivalent\n",
        "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
        "\n",
        "import seaborn as sns  # ggplot2 + RColorBrewer equivalent\n",
        "import plotly.express as px  # ggplot2 equivalent\n",
        "import plotly.graph_objects as go\n",
        "import plotly.figure_factory as ff\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Configure plotting styles\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# RColorBrewer equivalent color palettes\n",
        "color_palettes = {\n",
        "    'Set1': sns.color_palette(\"Set1\", 10),\n",
        "    'Set2': sns.color_palette(\"Set2\", 8), \n",
        "    'Dark2': sns.color_palette(\"Dark2\", 8),\n",
        "    'Paired': sns.color_palette(\"Paired\", 12),\n",
        "    'Blues': sns.color_palette(\"Blues\", 10),\n",
        "    'Reds': sns.color_palette(\"Reds\", 10),\n",
        "    'Greens': sns.color_palette(\"Greens\", 10),\n",
        "    'Viridis': sns.color_palette(\"viridis\", 10),\n",
        "    'Plasma': sns.color_palette(\"plasma\", 10)\n",
        "}\n",
        "\n",
        "# Figure configuration\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.grid'] = True\n",
        "\n",
        "print(\"‚úÖ Visualization libraries imported (ggplot2/scales/grid/RColorBrewer equivalents)\")\n",
        "print(f\"‚úÖ Available color palettes: {list(color_palettes.keys())}\")\n",
        "\n",
        "# Utility function for quick plotting (ggplot2 qplot equivalent)\n",
        "def quick_plot(df, x, y=None, kind='scatter', title=\"\", color_col=None):\n",
        "    \"\"\"Quick plotting function similar to R's qplot\"\"\"\n",
        "    if isinstance(df, DataFrame):  # Convert PySpark to Pandas\n",
        "        df = df.toPandas()\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    if kind == 'scatter':\n",
        "        if color_col:\n",
        "            plt.scatter(df[x], df[y], c=df[color_col], alpha=0.6)\n",
        "        else:\n",
        "            plt.scatter(df[x], df[y], alpha=0.6)\n",
        "    elif kind == 'line':\n",
        "        plt.plot(df[x], df[y])\n",
        "    elif kind == 'hist':\n",
        "        plt.hist(df[x], bins=30, alpha=0.7)\n",
        "    elif kind == 'box':\n",
        "        df.boxplot(column=x)\n",
        "    \n",
        "    plt.title(title)\n",
        "    plt.xlabel(x)\n",
        "    if y: plt.ylabel(y)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úÖ Quick plot function created (qplot equivalent)\")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Correlation & Flow Visualization (corrplot, alluvial equivalents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation plotting (corrplot equivalent) \n",
        "import numpy as np\n",
        "from pyspark.ml.stat import Correlation\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "def spark_corrplot(spark_df, numeric_cols, method='pearson', figsize=(10, 8)):\n",
        "    \"\"\"\n",
        "    Create correlation plot from PySpark DataFrame (corrplot equivalent)\n",
        "    \"\"\"\n",
        "    # Assemble features for correlation\n",
        "    assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
        "    df_assembled = assembler.transform(spark_df)\n",
        "    \n",
        "    # Calculate correlation matrix\n",
        "    correlation_matrix = Correlation.corr(df_assembled, \"features\", method).head()[0]\n",
        "    correlation_array = np.array(correlation_matrix.toArray())\n",
        "    \n",
        "    # Create correlation heatmap\n",
        "    plt.figure(figsize=figsize)\n",
        "    mask = np.triu(np.ones_like(correlation_array, dtype=bool))  # Upper triangle mask\n",
        "    \n",
        "    sns.heatmap(correlation_array, \n",
        "                mask=mask,\n",
        "                annot=True, \n",
        "                cmap='RdBu_r', \n",
        "                center=0,\n",
        "                xticklabels=numeric_cols,\n",
        "                yticklabels=numeric_cols,\n",
        "                square=True,\n",
        "                fmt='.2f')\n",
        "    \n",
        "    plt.title(f'{method.title()} Correlation Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return correlation_array\n",
        "\n",
        "# Alluvial/Sankey diagrams (alluvial equivalent)\n",
        "def create_sankey_diagram(source_col, target_col, value_col, df, title=\"Flow Diagram\"):\n",
        "    \"\"\"\n",
        "    Create Sankey diagram from PySpark DataFrame (alluvial equivalent)\n",
        "    \"\"\"\n",
        "    if isinstance(df, DataFrame):  # Convert PySpark to Pandas\n",
        "        df_pandas = df.toPandas()\n",
        "    else:\n",
        "        df_pandas = df\n",
        "    \n",
        "    # Get unique nodes\n",
        "    sources = df_pandas[source_col].unique()\n",
        "    targets = df_pandas[target_col].unique()\n",
        "    all_nodes = list(set(list(sources) + list(targets)))\n",
        "    \n",
        "    # Create node mapping\n",
        "    node_map = {node: idx for idx, node in enumerate(all_nodes)}\n",
        "    \n",
        "    # Prepare data for Sankey\n",
        "    source_indices = [node_map[src] for src in df_pandas[source_col]]\n",
        "    target_indices = [node_map[tgt] for tgt in df_pandas[target_col]]\n",
        "    values = df_pandas[value_col].tolist()\n",
        "    \n",
        "    # Create Sankey diagram\n",
        "    fig = go.Figure(data=[go.Sankey(\n",
        "        node = dict(\n",
        "            pad = 15,\n",
        "            thickness = 20,\n",
        "            line = dict(color = \"black\", width = 0.5),\n",
        "            label = all_nodes,\n",
        "            color = px.colors.qualitative.Set3[:len(all_nodes)]\n",
        "        ),\n",
        "        link = dict(\n",
        "            source = source_indices,\n",
        "            target = target_indices, \n",
        "            value = values\n",
        "        )\n",
        "    )])\n",
        "    \n",
        "    fig.update_layout(title_text=title, font_size=10)\n",
        "    fig.show()\n",
        "    \n",
        "    return fig\n",
        "\n",
        "print(\"‚úÖ Correlation plotting function created (corrplot equivalent)\")\n",
        "print(\"‚úÖ Sankey diagram function created (alluvial equivalent)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Geospatial & Mapping Libraries (geosphere, leaflet, maps equivalents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Geospatial libraries (geosphere, leaflet, maps equivalents)\n",
        "import folium  # leaflet equivalent\n",
        "from folium import plugins  # leaflet.extras equivalent\n",
        "import geopandas as gpd  # maps equivalent\n",
        "from geopy.distance import geodesic  # geosphere equivalent\n",
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "# PySpark geospatial functions\n",
        "from pyspark.sql.functions import radians, cos, sin, asin, sqrt, atan2\n",
        "\n",
        "def haversine_distance_spark(lat1, lon1, lat2, lon2):\n",
        "    \"\"\"\n",
        "    Calculate Haversine distance in PySpark (geosphere equivalent)\n",
        "    Returns distance in miles\n",
        "    \"\"\"\n",
        "    # Convert to radians\n",
        "    lat1_rad, lon1_rad = radians(lat1), radians(lon1)\n",
        "    lat2_rad, lon2_rad = radians(lat2), radians(lon2)\n",
        "    \n",
        "    # Haversine formula\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    \n",
        "    a = sin(dlat/2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon/2)**2\n",
        "    c = 2 * asin(sqrt(a))\n",
        "    \n",
        "    # Earth radius in miles\n",
        "    earth_radius_miles = 3959\n",
        "    distance = earth_radius_miles * c\n",
        "    \n",
        "    return distance\n",
        "\n",
        "def create_interactive_map(df, lat_col, lon_col, popup_col=None, title=\"Interactive Map\"):\n",
        "    \"\"\"\n",
        "    Create interactive map from PySpark DataFrame (leaflet equivalent)\n",
        "    \"\"\"\n",
        "    if isinstance(df, DataFrame):  # Convert PySpark to Pandas\n",
        "        df_pandas = df.toPandas()\n",
        "    else:\n",
        "        df_pandas = df\n",
        "    \n",
        "    # Get center coordinates\n",
        "    center_lat = df_pandas[lat_col].mean()\n",
        "    center_lon = df_pandas[lon_col].mean()\n",
        "    \n",
        "    # Create base map\n",
        "    m = folium.Map(\n",
        "        location=[center_lat, center_lon], \n",
        "        zoom_start=10,\n",
        "        tiles='OpenStreetMap'\n",
        "    )\n",
        "    \n",
        "    # Add markers\n",
        "    for idx, row in df_pandas.iterrows():\n",
        "        popup_text = str(row[popup_col]) if popup_col else f\"Point {idx}\"\n",
        "        folium.Marker(\n",
        "            location=[row[lat_col], row[lon_col]],\n",
        "            popup=popup_text,\n",
        "            icon=folium.Icon(color='blue', icon='info-sign')\n",
        "        ).add_to(m)\n",
        "    \n",
        "    # Add heatmap if many points\n",
        "    if len(df_pandas) > 100:\n",
        "        heat_data = [[row[lat_col], row[lon_col]] for idx, row in df_pandas.iterrows()]\n",
        "        plugins.HeatMap(heat_data).add_to(m)\n",
        "    \n",
        "    return m\n",
        "\n",
        "def create_choropleth_map(geo_data, data_col, title=\"Choropleth Map\"):\n",
        "    \"\"\"\n",
        "    Create choropleth map (leaflet + geospatial equivalent)\n",
        "    \"\"\"\n",
        "    # NYC bounds\n",
        "    nyc_center = [40.7128, -74.0060]\n",
        "    \n",
        "    m = folium.Map(location=nyc_center, zoom_start=10)\n",
        "    \n",
        "    if isinstance(geo_data, dict):  # GeoJSON\n",
        "        folium.Choropleth(\n",
        "            geo_data=geo_data,\n",
        "            data=data_col,\n",
        "            columns=['zone_id', 'value'],\n",
        "            key_on='feature.properties.zone_id',\n",
        "            fill_color='YlOrRd',\n",
        "            fill_opacity=0.7,\n",
        "            line_opacity=0.2,\n",
        "            legend_name=title\n",
        "        ).add_to(m)\n",
        "    \n",
        "    return m\n",
        "\n",
        "# NYC Taxi Zone utilities\n",
        "def load_nyc_taxi_zones():\n",
        "    \"\"\"Load NYC taxi zone shapefile for mapping\"\"\"\n",
        "    # This would typically load from GeoJSON or shapefile\n",
        "    # For now, we'll create a placeholder\n",
        "    print(\"üìç NYC Taxi Zones loader ready\")\n",
        "    print(\"   Use: geopandas.read_file('path/to/taxi_zones.geojson')\")\n",
        "    return None\n",
        "\n",
        "print(\"‚úÖ Geospatial libraries imported (geosphere/leaflet/maps equivalents)\")\n",
        "print(\"‚úÖ Haversine distance function created for PySpark\")\n",
        "print(\"‚úÖ Interactive mapping functions created (leaflet equivalent)\")\n",
        "print(\"‚úÖ Choropleth mapping ready\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Machine Learning Libraries (xgboost, caret equivalents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Machine Learning libraries (xgboost, caret equivalents)\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import (\n",
        "    VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder,\n",
        "    Bucketizer, QuantileDiscretizer, PCA, MinMaxScaler\n",
        ")\n",
        "from pyspark.ml.classification import (\n",
        "    RandomForestClassifier, GBTClassifier, LogisticRegression,\n",
        "    DecisionTreeClassifier, LinearSVC, MultilayerPerceptronClassifier\n",
        ")\n",
        "from pyspark.ml.regression import (\n",
        "    RandomForestRegressor, GBTRegressor, LinearRegression,\n",
        "    DecisionTreeRegressor, GeneralizedLinearRegression\n",
        ")\n",
        "from pyspark.ml.evaluation import (\n",
        "    RegressionEvaluator, BinaryClassificationEvaluator,\n",
        "    MulticlassClassificationEvaluator\n",
        ")\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit\n",
        "from pyspark.ml.stat import Correlation\n",
        "\n",
        "# Traditional ML libraries for comparison\n",
        "import xgboost as xgb  # Direct xgboost equivalent\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor as SKRandomForest\n",
        "from sklearn.metrics import mean_squared_error, r2_score, classification_report\n",
        "\n",
        "# ML utility functions (caret equivalent)\n",
        "def create_ml_pipeline(feature_cols, target_col, model_type='rf', task='regression'):\n",
        "    \"\"\"\n",
        "    Create ML pipeline (caret trainControl equivalent)\n",
        "    \"\"\"\n",
        "    # Feature engineering stages\n",
        "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
        "    \n",
        "    # Model selection\n",
        "    if task == 'regression':\n",
        "        if model_type == 'rf':\n",
        "            model = RandomForestRegressor(featuresCol=\"scaled_features\", labelCol=target_col)\n",
        "        elif model_type == 'gbt':\n",
        "            model = GBTRegressor(featuresCol=\"scaled_features\", labelCol=target_col)\n",
        "        elif model_type == 'linear':\n",
        "            model = LinearRegression(featuresCol=\"scaled_features\", labelCol=target_col)\n",
        "    else:  # classification\n",
        "        if model_type == 'rf':\n",
        "            model = RandomForestClassifier(featuresCol=\"scaled_features\", labelCol=target_col)\n",
        "        elif model_type == 'gbt':\n",
        "            model = GBTClassifier(featuresCol=\"scaled_features\", labelCol=target_col)\n",
        "        elif model_type == 'logistic':\n",
        "            model = LogisticRegression(featuresCol=\"scaled_features\", labelCol=target_col)\n",
        "    \n",
        "    # Create pipeline\n",
        "    pipeline = Pipeline(stages=[assembler, scaler, model])\n",
        "    \n",
        "    return pipeline\n",
        "\n",
        "def tune_hyperparameters(pipeline, train_data, param_grid, evaluator, folds=3):\n",
        "    \"\"\"\n",
        "    Hyperparameter tuning (caret tune equivalent)\n",
        "    \"\"\"\n",
        "    cv = CrossValidator(\n",
        "        estimator=pipeline,\n",
        "        estimatorParamMaps=param_grid,\n",
        "        evaluator=evaluator,\n",
        "        numFolds=folds,\n",
        "        seed=42\n",
        "    )\n",
        "    \n",
        "    cv_model = cv.fit(train_data)\n",
        "    return cv_model\n",
        "\n",
        "def evaluate_model(model, test_data, task='regression'):\n",
        "    \"\"\"\n",
        "    Model evaluation (caret confusionMatrix equivalent)\n",
        "    \"\"\"\n",
        "    predictions = model.transform(test_data)\n",
        "    \n",
        "    if task == 'regression':\n",
        "        evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
        "        rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
        "        r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
        "        mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
        "        \n",
        "        print(f\"üìä Regression Metrics:\")\n",
        "        print(f\"   RMSE: {rmse:.4f}\")\n",
        "        print(f\"   R¬≤: {r2:.4f}\")  \n",
        "        print(f\"   MAE: {mae:.4f}\")\n",
        "        \n",
        "        return {\"rmse\": rmse, \"r2\": r2, \"mae\": mae}\n",
        "    else:\n",
        "        evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\")\n",
        "        auc = evaluator.evaluate(predictions)\n",
        "        \n",
        "        mc_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
        "        accuracy = mc_evaluator.evaluate(predictions, {mc_evaluator.metricName: \"accuracy\"})\n",
        "        precision = mc_evaluator.evaluate(predictions, {mc_evaluator.metricName: \"weightedPrecision\"})\n",
        "        recall = mc_evaluator.evaluate(predictions, {mc_evaluator.metricName: \"weightedRecall\"})\n",
        "        \n",
        "        print(f\"üìä Classification Metrics:\")\n",
        "        print(f\"   AUC: {auc:.4f}\")\n",
        "        print(f\"   Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"   Precision: {precision:.4f}\")\n",
        "        print(f\"   Recall: {recall:.4f}\")\n",
        "        \n",
        "        return {\"auc\": auc, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall}\n",
        "\n",
        "# Feature importance function\n",
        "def get_feature_importance(model, feature_cols):\n",
        "    \"\"\"\n",
        "    Extract feature importance (caret varImp equivalent)\n",
        "    \"\"\"\n",
        "    if hasattr(model, 'stages'):\n",
        "        # Extract from pipeline\n",
        "        final_model = model.stages[-1]\n",
        "    else:\n",
        "        final_model = model\n",
        "    \n",
        "    if hasattr(final_model, 'featureImportances'):\n",
        "        importances = final_model.featureImportances.toArray()\n",
        "        feature_importance_df = spark.createDataFrame(\n",
        "            zip(feature_cols, importances), \n",
        "            ['feature', 'importance']\n",
        "        ).orderBy(desc('importance'))\n",
        "        \n",
        "        return feature_importance_df\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Model doesn't support feature importance\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ PySpark ML libraries imported (xgboost/caret equivalents)\")\n",
        "print(\"‚úÖ ML pipeline utilities created\")\n",
        "print(\"‚úÖ Hyperparameter tuning functions ready\")\n",
        "print(\"‚úÖ Model evaluation functions created\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Practical Example - NYC Taxi Analysis Using All Tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load sample NYC taxi data from BigQuery\n",
        "try:\n",
        "    # Load taxi trip data\n",
        "    yellow_taxi_df = spark.read \\\n",
        "        .format(\"bigquery\") \\\n",
        "        .option(\"table\", f\"{PROJECT_ID}.{DATASET_ID}.yellow_taxi_external_table\") \\\n",
        "        .option(\"maxPartitions\", \"50\") \\\n",
        "        .load()\n",
        "    \n",
        "    # Sample for demonstration (remove .sample() for full analysis)\n",
        "    sample_df = yellow_taxi_df.sample(0.01, seed=42)  # 1% sample\n",
        "    \n",
        "    print(f\"üìä Loaded sample data: {sample_df.count():,} records\")\n",
        "    \n",
        "    # Data manipulation using dplyr-equivalent functions\n",
        "    processed_df = sample_df \\\n",
        "        .filter(\n",
        "            (col(\"trip_distance\") > 0) & \n",
        "            (col(\"trip_distance\") < 50) &\n",
        "            (col(\"fare_amount\") > 0) &\n",
        "            (col(\"PULocationID\").isNotNull())\n",
        "        ) \\\n",
        "        .withColumn(\"trip_duration_min\", \n",
        "                   (unix_timestamp(\"tpep_dropoff_datetime\") - \n",
        "                    unix_timestamp(\"tpep_pickup_datetime\")) / 60) \\\n",
        "        .withColumn(\"avg_speed\", col(\"trip_distance\") / (col(\"trip_duration_min\") / 60)) \\\n",
        "        .withColumn(\"pickup_hour\", hour(\"tpep_pickup_datetime\")) \\\n",
        "        .withColumn(\"pickup_day\", dayofweek(\"tpep_pickup_datetime\")) \\\n",
        "        .filter((col(\"trip_duration_min\") > 1) & (col(\"avg_speed\") <= 80))\n",
        "    \n",
        "    print(\"‚úÖ Data processed using dplyr-equivalent operations\")\n",
        "    \n",
        "    # Show basic statistics\n",
        "    processed_df.select(\"trip_distance\", \"fare_amount\", \"avg_speed\", \"trip_duration_min\") \\\n",
        "        .describe().show()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Note: To run this example, ensure your BigQuery tables are set up\")\n",
        "    print(f\"Error: {e}\")\n",
        "    \n",
        "    # Create sample data for demonstration\n",
        "    from pyspark.sql.types import StructType, StructField, FloatType, IntegerType\n",
        "    \n",
        "    sample_data = [\n",
        "        (10.5, 25.0, 35.2, 18.0, 14, 2),\n",
        "        (5.2, 12.5, 42.1, 7.4, 9, 1),\n",
        "        (15.8, 42.0, 28.5, 33.2, 17, 5),\n",
        "        (3.1, 8.5, 38.7, 4.8, 7, 3)\n",
        "    ]\n",
        "    \n",
        "    schema = StructType([\n",
        "        StructField(\"trip_distance\", FloatType()),\n",
        "        StructField(\"fare_amount\", FloatType()),\n",
        "        StructField(\"avg_speed\", FloatType()),\n",
        "        StructField(\"trip_duration_min\", FloatType()),\n",
        "        StructField(\"pickup_hour\", IntegerType()),\n",
        "        StructField(\"pickup_day\", IntegerType())\n",
        "    ])\n",
        "    \n",
        "    processed_df = spark.createDataFrame(sample_data, schema)\n",
        "    print(\"üìä Created sample dataset for demonstration\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
