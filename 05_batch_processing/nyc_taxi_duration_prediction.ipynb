{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 326,
          "status": "ok",
          "timestamp": 1754355560280,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "nVcdAebS-JGw"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, year, month, dayofyear, when, lit, unix_timestamp, count, avg\n",
        "from pyspark.sql.types import IntegerType\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.figure import Figure\n",
        "import numpy as np\n",
        "import math\n",
        "from typing import List, Optional, Union, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "executionInfo": {
          "elapsed": 184256,
          "status": "ok",
          "timestamp": 1754355744855,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "p2NN8wwF7YRa",
        "outputId": "8ae84fd9-d1c6-44d5-f010-4ebe59858ca0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Applications/saggydev/projects_learning/data_engineering_course/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîê Using credentials: /Applications/saggydev/projects_learning/data_engineering_course/secrets/dtc-de-course-466501-e23cbf158abc.json\n",
            ":: loading settings :: url = jar:file:/Applications/saggydev/projects_learning/data_engineering_course/.venv/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ivy Default Cache set to: /Users/saggysimmba/.ivy2/cache\n",
            "The jars for the packages stored in: /Users/saggysimmba/.ivy2/jars\n",
            "com.google.cloud.spark#spark-bigquery-with-dependencies_2.12 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9e82a2d8-150b-4775-8c14-478c5827d7ab;1.0\n",
            "\tconfs: [default]\n",
            "\tfound com.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.32.0 in central\n",
            ":: resolution report :: resolve 79ms :: artifacts dl 1ms\n",
            "\t:: modules in use:\n",
            "\tcom.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.32.0 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-9e82a2d8-150b-4775-8c14-478c5827d7ab\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 1 already retrieved (0kB/3ms)\n",
            "25/08/04 19:59:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Spark Version: 3.5.4\n",
            "üìä Working with: dtc-de-course-466501.dbt_production\n",
            "üîÑ Incremental Processing: 2015-2016\n",
            "üè† Local Processing Mode: Limits enabled for efficient local development\n",
            "\n",
            "================================================================================\n",
            "üöÄ Testing BigQuery Connection with Hybrid Approach\n",
            "================================================================================\n",
            "üìã Loading sample data from fact_trips table...\n",
            "üìä Table 'fact_trips' has complex schema, using BigQuery client method\n",
            "üîç Running query: SELECT * FROM `dtc-de-course-466501.dbt_production.fact_trips` LIMIT 10000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/08/04 19:59:16 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
            "25/08/04 19:59:19 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ BigQuery client successful: 10000 rows loaded\n",
            "‚úÖ Successfully loaded BigQuery table into Spark DataFrame!\n",
            "üìä Schema:\n",
            "root\n",
            " |-- tripid: string (nullable = true)\n",
            " |-- vendorid: long (nullable = true)\n",
            " |-- service_type: string (nullable = true)\n",
            " |-- ratecodeid: long (nullable = true)\n",
            " |-- pickup_locationid: long (nullable = true)\n",
            " |-- pickup_borough: string (nullable = true)\n",
            " |-- pickup_zone: string (nullable = true)\n",
            " |-- dropoff_locationid: long (nullable = true)\n",
            " |-- dropoff_borough: string (nullable = true)\n",
            " |-- dropoff_zone: string (nullable = true)\n",
            " |-- pickup_datetime: timestamp (nullable = true)\n",
            " |-- pickup_date: date (nullable = true)\n",
            " |-- dropoff_datetime: timestamp (nullable = true)\n",
            " |-- store_and_fwd_flag: string (nullable = true)\n",
            " |-- passenger_count: long (nullable = true)\n",
            " |-- trip_distance: decimal(38,18) (nullable = true)\n",
            " |-- fare_amount: decimal(38,18) (nullable = true)\n",
            " |-- extra: decimal(38,18) (nullable = true)\n",
            " |-- mta_tax: decimal(38,18) (nullable = true)\n",
            " |-- tip_amount: decimal(38,18) (nullable = true)\n",
            " |-- tolls_amount: decimal(38,18) (nullable = true)\n",
            " |-- improvement_surcharge: decimal(38,18) (nullable = true)\n",
            " |-- total_amount: decimal(38,18) (nullable = true)\n",
            " |-- payment_type: long (nullable = true)\n",
            " |-- payment_type_description: string (nullable = true)\n",
            " |-- climate_date: date (nullable = true)\n",
            " |-- mjd: long (nullable = true)\n",
            " |-- cloudCover: decimal(38,18) (nullable = true)\n",
            " |-- humidity: decimal(38,18) (nullable = true)\n",
            " |-- dewPoint: decimal(38,18) (nullable = true)\n",
            " |-- precipIntensity: decimal(38,18) (nullable = true)\n",
            " |-- highTemp: decimal(38,18) (nullable = true)\n",
            " |-- lowTemp: decimal(38,18) (nullable = true)\n",
            " |-- visibility: decimal(38,18) (nullable = true)\n",
            " |-- windSpeed: decimal(38,18) (nullable = true)\n",
            "\n",
            "üìã Sample data:\n",
            "+--------------------+--------+------------+----------+-----------------+--------------+--------------------+------------------+---------------+----------------+-------------------+-----------+-------------------+------------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+------------+------------------------+------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|              tripid|vendorid|service_type|ratecodeid|pickup_locationid|pickup_borough|         pickup_zone|dropoff_locationid|dropoff_borough|    dropoff_zone|    pickup_datetime|pickup_date|   dropoff_datetime|store_and_fwd_flag|passenger_count|       trip_distance|         fare_amount|               extra|             mta_tax|          tip_amount|        tolls_amount|improvement_surcharge|        total_amount|payment_type|payment_type_description|climate_date|  mjd|          cloudCover|            humidity|            dewPoint|     precipIntensity|            highTemp|             lowTemp|          visibility|           windSpeed|\n",
            "+--------------------+--------+------------+----------+-----------------+--------------+--------------------+------------------+---------------+----------------+-------------------+-----------+-------------------+------------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+------------+------------------------+------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|f05143d0185cae00d...|       1|      Yellow|         1|               50|     Manhattan|        Clinton West|               107|      Manhattan|        Gramercy|2014-12-31 17:08:22| 2015-01-01|2014-12-31 17:21:09|                 N|              5|2.900000000000000000|12.00000000000000...|0.500000000000000000|0.500000000000000000|0.000000000000000000|0.000000000000000000| 0.000000000000000000|13.30000000000000...|           2|                    Cash|  2015-01-01|57023|0.420000000000000000|0.500000000000000000|11.79000000000000...|0.000000000000000000|31.87000000000000...|24.32000000000000...|10.00000000000000...|2.790000000000000000|\n",
            "|416f2cd091245aaaa...|       2|      Yellow|         1|              239|     Manhattan|Upper West Side S...|               116|      Manhattan|Hamilton Heights|2014-12-31 17:13:29| 2015-01-01|2014-12-31 17:26:18|                 N|              1|4.730000000000000000|16.00000000000000...|0.500000000000000000|0.500000000000000000|0.000000000000000000|0.000000000000000000| 0.300000000000000000|17.30000000000000...|           2|                    Cash|  2015-01-01|57023|0.420000000000000000|0.500000000000000000|11.79000000000000...|0.000000000000000000|31.87000000000000...|24.32000000000000...|10.00000000000000...|2.790000000000000000|\n",
            "|0c6516826e674addd...|       1|      Yellow|         1|              263|     Manhattan|      Yorkville West|               141|      Manhattan| Lenox Hill West|2014-12-31 17:15:40| 2015-01-01|2014-12-31 17:17:35|                 N|              2|0.400000000000000000|3.500000000000000000|0.500000000000000000|0.500000000000000000|0.000000000000000000|0.000000000000000000| 0.000000000000000000|4.800000000000000000|           2|                    Cash|  2015-01-01|57023|0.420000000000000000|0.500000000000000000|11.79000000000000...|0.000000000000000000|31.87000000000000...|24.32000000000000...|10.00000000000000...|2.790000000000000000|\n",
            "|e88b1fe2d7f47558f...|       1|      Yellow|         1|              163|     Manhattan|       Midtown North|               170|      Manhattan|     Murray Hill|2014-12-31 17:17:38| 2015-01-01|2014-12-31 17:25:36|                 N|              3|1.200000000000000000|7.000000000000000000|0.500000000000000000|0.500000000000000000|0.000000000000000000|0.000000000000000000| 0.000000000000000000|8.300000000000000000|           2|                    Cash|  2015-01-01|57023|0.420000000000000000|0.500000000000000000|11.79000000000000...|0.000000000000000000|31.87000000000000...|24.32000000000000...|10.00000000000000...|2.790000000000000000|\n",
            "|daba8e2d30277a5d9...|       1|      Yellow|         1|              158|     Manhattan|Meatpacking/West ...|                50|      Manhattan|    Clinton West|2014-12-31 17:19:25| 2015-01-01|2014-12-31 17:39:53|                 N|              4|2.500000000000000000|14.00000000000000...|0.500000000000000000|0.500000000000000000|0.000000000000000000|0.000000000000000000| 0.000000000000000000|15.30000000000000...|           2|                    Cash|  2015-01-01|57023|0.420000000000000000|0.500000000000000000|11.79000000000000...|0.000000000000000000|31.87000000000000...|24.32000000000000...|10.00000000000000...|2.790000000000000000|\n",
            "+--------------------+--------+------------+----------+-----------------+--------------+--------------------+------------------+---------------+----------------+-------------------+-----------+-------------------+------------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+------------+------------------------+------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "üìà Basic statistics:\n",
            "   ‚Ä¢ Total rows: 10,000\n",
            "   ‚Ä¢ Total columns: 35\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# Create Local PySpark Session with BigQuery Integration\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# üÜï Environment Setup for Local PySpark\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "# Set up BigQuery credentials for local development\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Applications/saggydev/projects_learning/data_engineering_course/secrets/dtc-de-course-466501-e23cbf158abc.json\"\n",
        "print(f\"üîê Using credentials: {os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')}\")\n",
        "\n",
        "# Enhanced Spark configuration for local large-scale incremental processing\n",
        "# üîß LOCAL SPARK CONFIGURATION EXPLAINED:\n",
        "#\n",
        "# BIGQUERY INTEGRATION - Local PySpark to BigQuery connectivity\n",
        "# ‚îú‚îÄ \"spark.jars.packages\" - Automatically downloads BigQuery connector\n",
        "# ‚îÇ  ‚îî‚îÄ No manual JAR placement needed, handles all dependencies\n",
        "# ‚îÇ\n",
        "# ‚îú‚îÄ \"spark.sql.execution.arrow.pyspark.enabled\" = \"false\"\n",
        "# ‚îÇ  ‚îî‚îÄ Disabled to avoid compatibility issues with BigQuery connector\n",
        "# ‚îÇ\n",
        "# ‚îî‚îÄ Memory configs optimized for local machine processing\n",
        "#\n",
        "# ADAPTIVE QUERY EXECUTION (AQE) - Runtime optimization based on actual data\n",
        "# ‚îú‚îÄ \"spark.sql.adaptive.enabled\" = \"true\"\n",
        "# ‚îÇ  ‚îî‚îÄ Enables AQE for dynamic query optimization during execution\n",
        "# ‚îÇ\n",
        "# ‚îú‚îÄ \"spark.sql.adaptive.coalescePartitions.enabled\" = \"true\"\n",
        "# ‚îÇ  ‚îî‚îÄ Automatically reduces number of partitions after shuffle to optimize performance\n",
        "# ‚îÇ\n",
        "# ‚îú‚îÄ \"spark.sql.adaptive.localShuffleReader.enabled\" = \"true\"\n",
        "# ‚îÇ  ‚îî‚îÄ Reduces network I/O by reading shuffle data locally when possible\n",
        "# ‚îÇ\n",
        "# ‚îî‚îÄ \"spark.sql.adaptive.advisoryPartitionSizeInBytes\" = \"256MB\"\n",
        "#    ‚îî‚îÄ Target size for each partition (256MB optimal for most workloads)\n",
        "#\n",
        "# SKEW HANDLING - Deals with uneven data distribution\n",
        "# ‚îú‚îÄ \"spark.sql.adaptive.skewJoin.enabled\" = \"true\"\n",
        "# ‚îÇ  ‚îî‚îÄ Automatically detects and handles data skew in joins\n",
        "# ‚îÇ\n",
        "# ‚îú‚îÄ \"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\" = \"256MB\"\n",
        "# ‚îÇ  ‚îî‚îÄ Partition size threshold to identify skewed partitions\n",
        "# ‚îÇ\n",
        "# ‚îî‚îÄ \"spark.sql.adaptive.skewJoin.skewedPartitionFactor\" = \"5\"\n",
        "#    ‚îî‚îÄ Factor to determine skew (partition 5x larger than median = skewed)\n",
        "#\n",
        "# PERFORMANCE OPTIMIZATIONS\n",
        "# ‚îú‚îÄ \"spark.sql.shuffle.partitions\" = \"200\" (reduced for local)\n",
        "# ‚îÇ  ‚îî‚îÄ Number of partitions for shuffle operations (reduced for local processing)\n",
        "# ‚îÇ\n",
        "# ‚îî‚îÄ \"spark.serializer\" = \"org.apache.spark.serializer.KryoSerializer\"\n",
        "#    ‚îî‚îÄ Faster serialization compared to default Java serialization\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"NYC Taxi Duration Prediction - Local\")\n",
        "    # üÜï BigQuery connector - automatically downloads from Maven Central\n",
        "    .config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.32.0\")\n",
        "    # üÜï DISABLE Arrow to avoid local compatibility issues\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
        "    # üÜï Memory configs optimized for local machine\n",
        "    .config(\"spark.driver.memory\", \"4g\")\n",
        "    .config(\"spark.executor.memory\", \"4g\")\n",
        "    .config(\"spark.driver.maxResultSize\", \"2g\")\n",
        "    # Performance optimizations (adjusted for local)\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
        "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
        "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\n",
        "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"256MB\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\")  # Reduced for local processing\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
        "    .config(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"256MB\")\n",
        "    .config(\"spark.sql.adaptive.skewJoin.skewedPartitionFactor\", \"5\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# GCP Project and Dataset Config\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "PROJECT_ID = \"dtc-de-course-466501\"\n",
        "DATASET_ID = \"dbt_production\"\n",
        "\n",
        "INCREMENTAL_CONFIG = {\n",
        "    \"start_year\": 2015,\n",
        "    \"end_year\": 2016,\n",
        "    \"batch_size_months\": 6,\n",
        "    \"checkpoint_dir\": \"/tmp/spark-checkpoints\",\n",
        "    \"cache_level\": \"MEMORY_AND_DISK_SER\",\n",
        "    \"max_records_per_partition\": 1000000,\n",
        "    # üÜï Local processing limits\n",
        "    \"local_test_limit\": 10000,  # Limit for local testing\n",
        "    \"local_batch_limit\": 50000  # Limit for local batch processing\n",
        "}\n",
        "\n",
        "print(f\"‚úÖ Spark Version: {spark.version}\")\n",
        "print(f\"üìä Working with: {PROJECT_ID}.{DATASET_ID}\")\n",
        "print(f\"üîÑ Incremental Processing: {INCREMENTAL_CONFIG['start_year']}-{INCREMENTAL_CONFIG['end_year']}\")\n",
        "print(f\"üè† Local Processing Mode: Limits enabled for efficient local development\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Hybrid BigQuery Data Loading Functions\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def load_table_hybrid(spark, table_name, project_id=PROJECT_ID, dataset_id=DATASET_ID, limit=None):\n",
        "    \"\"\"\n",
        "    Hybrid loader that tries direct connector first, falls back to client method\n",
        "    \"\"\"\n",
        "    from google.cloud import bigquery\n",
        "    import logging\n",
        "    \n",
        "    # Tables known to have complex schemas that cause issues with direct connector\n",
        "    complex_tables = [\"fact_trips\"]\n",
        "    \n",
        "    # Skip direct connector for known complex tables\n",
        "    if table_name in complex_tables:\n",
        "        print(f\"üìä Table '{table_name}' has complex schema, using BigQuery client method\")\n",
        "        \n",
        "        # Create BigQuery client\n",
        "        client = bigquery.Client()\n",
        "        \n",
        "        # Build query with limit\n",
        "        limit_clause = f\"LIMIT {limit}\" if limit else \"\"\n",
        "        query = f\"SELECT * FROM `{project_id}.{dataset_id}.{table_name}` {limit_clause}\"\n",
        "        \n",
        "        print(f\"üîç Running query: {query}\")\n",
        "        \n",
        "        # Execute query and convert to pandas DataFrame\n",
        "        query_job = client.query(query)\n",
        "        df_pandas = query_job.to_dataframe()\n",
        "        \n",
        "        # Convert pandas DataFrame to Spark DataFrame\n",
        "        df_spark = spark.createDataFrame(df_pandas)\n",
        "        \n",
        "        print(f\"‚úÖ BigQuery client successful: {df_spark.count()} rows loaded\")\n",
        "        return df_spark\n",
        "    \n",
        "    else:\n",
        "        # Try direct connector for simple tables\n",
        "        print(f\"üìä Attempting direct connector for table '{table_name}'\")\n",
        "        try:\n",
        "            reader = (spark.read\n",
        "                    .format(\"bigquery\")\n",
        "                    .option(\"project\", project_id)\n",
        "                    .option(\"dataset\", dataset_id)\n",
        "                    .option(\"table\", table_name)\n",
        "                    .option(\"readDataFormat\", \"AVRO\")\n",
        "                    .option(\"maxParallelism\", \"1\")\n",
        "            )\n",
        "            \n",
        "            df = reader.load()\n",
        "            if limit:\n",
        "                df = df.limit(limit)\n",
        "            \n",
        "            # Test if it works\n",
        "            df.take(1)\n",
        "            print(f\"‚úÖ Direct connector successful for {table_name}\")\n",
        "            return df\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Direct connector failed for {table_name}: {str(e)}\")\n",
        "            print(f\"üîÑ Falling back to BigQuery client method\")\n",
        "            \n",
        "            # Fallback to client method\n",
        "            client = bigquery.Client()\n",
        "            limit_clause = f\"LIMIT {limit}\" if limit else \"\"\n",
        "            query = f\"SELECT * FROM `{project_id}.{dataset_id}.{table_name}` {limit_clause}\"\n",
        "            \n",
        "            query_job = client.query(query)\n",
        "            df_pandas = query_job.to_dataframe()\n",
        "            df_spark = spark.createDataFrame(df_pandas)\n",
        "            \n",
        "            print(f\"‚úÖ BigQuery client fallback successful: {df_spark.count()} rows loaded\")\n",
        "            return df_spark\n",
        "\n",
        "def run_bigquery_query(spark, query):\n",
        "    \"\"\"\n",
        "    Run a custom BigQuery query and load results into Spark DataFrame\n",
        "    \"\"\"\n",
        "    client = bigquery.Client()\n",
        "    \n",
        "    print(f\"üîç Running custom query...\")\n",
        "    query_job = client.query(query)\n",
        "    df_pandas = query_job.to_dataframe()\n",
        "    df_spark = spark.createDataFrame(df_pandas)\n",
        "    \n",
        "    print(f\"‚úÖ Query successful: {df_spark.count()} rows loaded\")\n",
        "    return df_spark\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Example: Read from BigQuery using Hybrid Approach\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üöÄ Testing BigQuery Connection with Hybrid Approach\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test with a small sample first\n",
        "print(\"üìã Loading sample data from fact_trips table...\")\n",
        "\n",
        "df = load_table_hybrid(\n",
        "    spark, \n",
        "    \"fact_trips\", \n",
        "    PROJECT_ID, \n",
        "    DATASET_ID, \n",
        "    limit=INCREMENTAL_CONFIG[\"local_test_limit\"]  # Use local test limit\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Successfully loaded BigQuery table into Spark DataFrame!\")\n",
        "print(f\"üìä Schema:\")\n",
        "df.printSchema()\n",
        "\n",
        "print(f\"üìã Sample data:\")\n",
        "df.show(5)\n",
        "\n",
        "print(f\"üìà Basic statistics:\")\n",
        "print(f\"   ‚Ä¢ Total rows: {df.count():,}\")\n",
        "print(f\"   ‚Ä¢ Total columns: {len(df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek4yk-oQcdMy"
      },
      "source": [
        "**Test if data batching works**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        },
        "executionInfo": {
          "elapsed": 47502,
          "status": "ok",
          "timestamp": 1754355983990,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "R_vlxQaHcYLX",
        "outputId": "5ad72f2a-3fc8-4595-934b-c1c30c235ede"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Spark Version: 3.5.4\n",
            "üìä Working with: dtc-de-course-466501.dbt_production\n",
            "üîÑ Incremental Processing for years: 2015-2016\n",
            "üîç Running custom query...\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "START_YEAR = 2015\n",
        "END_YEAR = 2016\n",
        "\n",
        "print(f\"‚úÖ Spark Version: {spark.version}\")\n",
        "print(f\"üìä Working with: {PROJECT_ID}.{DATASET_ID}\")\n",
        "print(f\"üîÑ Incremental Processing for years: {START_YEAR}-{END_YEAR}\")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Read data from BigQuery in yearly batches\n",
        "# ---------------------------------------------------------------\n",
        "table_name = f\"{PROJECT_ID}.{DATASET_ID}.fact_trips\"\n",
        "\n",
        "for year in range(START_YEAR, END_YEAR + 1):\n",
        "\n",
        "    year_filter = f\"WHERE EXTRACT(YEAR FROM pickup_date) = {year}\"\n",
        "    query = f\"\"\"\n",
        "    SELECT * FROM `{PROJECT_ID}.{DATASET_ID}.fact_trips`\n",
        "    {year_filter}\n",
        "    \"\"\"\n",
        "\n",
        "    # Load data for that year\n",
        "    df_year = run_bigquery_query(spark, query)  # ‚úÖ No Java errors!\n",
        "\n",
        "    # Add year_month for inspection\n",
        "    df_with_month = df_year.withColumn(\n",
        "        \"year_month\",\n",
        "        F.substring(\"pickup_date\", 1, 7)\n",
        "    )\n",
        "\n",
        "    # Show distinct months in this batch\n",
        "    months_df = (\n",
        "        df_with_month\n",
        "        .select(\"year_month\")\n",
        "        .distinct()\n",
        "        .orderBy(\"year_month\")\n",
        "    )\n",
        "\n",
        "    months_list = months_df.collect()\n",
        "    months_str_list = [row[\"year_month\"] for row in months_list]\n",
        "\n",
        "    print(\"‚úÖ Loaded batch for year:\", year)\n",
        "    months_df.show(truncate=False)\n",
        "\n",
        "    print(f\"üìÖ Number of months found: {len(months_str_list)}\")\n",
        "    if months_str_list:\n",
        "      print(f\"üóìÔ∏è First month in batch: {months_str_list[0]}\")\n",
        "      print(f\"üóìÔ∏è Last month in batch: {months_str_list[-1]}\")\n",
        "    else:\n",
        "      print(\"‚ö†Ô∏è No data found for this batch.\")\n",
        "\n",
        "    # ----\n",
        "    # YOUR LOGIC HERE for processing df_with_month\n",
        "    # ----\n",
        "\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqdEF5CDE-xp"
      },
      "source": [
        "**Potential Optimization for this query**\n",
        "- Push‚Äêdown your entire date‚Äêrange filter so you only hit BigQuery once, rather than once per year.\n",
        "\n",
        "- Select only the columns you actually need (e.g. pickup_date plus whatever other fields your logic needs) to reduce I/O.\n",
        "\n",
        "- Use Spark‚Äôs built-in date functions (date_format, year) instead of substring.\n",
        "\n",
        "- Compute your ‚Äúmonths seen‚Äù stats in one pass via DataFrame aggregations‚Äîno collect() of the full list required.\n",
        "\n",
        "- Cache the filtered DataFrame if you‚Äôre going to loop over it multiple times.\n",
        "\n",
        "- Iterate per month (rather than per year) so you can build true ‚Äúincremental‚Äù logic month-by-month if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# FIXED Yearly Batch Processing - LOCAL PYSPARK VERSION\n",
        "# ----------------------------------------------------------------------\n",
        "# ‚ö†Ô∏è Use this cell instead of the problematic one above!\n",
        "\n",
        "START_YEAR = 2015\n",
        "END_YEAR = 2016\n",
        "\n",
        "print(f\"‚úÖ Spark Version: {spark.version}\")\n",
        "print(f\"üìä Working with: {PROJECT_ID}.{DATASET_ID}\")\n",
        "print(f\"üîÑ Incremental Processing for years: {START_YEAR}-{END_YEAR}\")\n",
        "print(f\"üè† Local Mode: Using batch limits for efficient processing\")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Read data from BigQuery in yearly batches - HYBRID APPROACH\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "yearly_dataframes = {}  # Store DataFrames for each year\n",
        "\n",
        "for year in range(START_YEAR, END_YEAR + 1):\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üóìÔ∏è Processing Year: {year}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # üÜï Use custom BigQuery SQL query instead of direct connector\n",
        "    # This avoids the Java DirectByteBuffer errors\n",
        "    query = f\"\"\"\n",
        "    SELECT * FROM `{PROJECT_ID}.{DATASET_ID}.fact_trips`\n",
        "    WHERE EXTRACT(YEAR FROM pickup_date) = {year}\n",
        "    LIMIT {INCREMENTAL_CONFIG['local_batch_limit']}\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"üîç Loading up to {INCREMENTAL_CONFIG['local_batch_limit']:,} rows for year {year}\")\n",
        "    \n",
        "    # üÜï Use the hybrid BigQuery query function (no direct connector issues)\n",
        "    df_year = run_bigquery_query(spark, query)\n",
        "    \n",
        "    # Add year_month for inspection (same as before)\n",
        "    df_with_month = df_year.withColumn(\n",
        "        \"year_month\",\n",
        "        F.substring(\"pickup_date\", 1, 7)\n",
        "    )\n",
        "    \n",
        "    # Show distinct months in this batch\n",
        "    months_df = (\n",
        "        df_with_month\n",
        "        .select(\"year_month\")\n",
        "        .distinct()\n",
        "        .orderBy(\"year_month\")\n",
        "    )\n",
        "    \n",
        "    months_list = months_df.collect()\n",
        "    months_str_list = [row[\"year_month\"] for row in months_list]\n",
        "    \n",
        "    print(f\"‚úÖ Loaded batch for year: {year}\")\n",
        "    print(f\"üìä Actual rows loaded: {df_with_month.count():,}\")\n",
        "    \n",
        "    print(f\"üìÖ Number of months found: {len(months_str_list)}\")\n",
        "    if months_str_list:\n",
        "        print(f\"üóìÔ∏è First month in batch: {months_str_list[0]}\")\n",
        "        print(f\"üóìÔ∏è Last month in batch: {months_str_list[-1]}\")\n",
        "        \n",
        "        # Show month distribution\n",
        "        print(f\"üìä Months in this year:\")\n",
        "        months_df.show(truncate=False)\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No data found for this batch.\")\n",
        "    \n",
        "    # üÜï Store DataFrame for later use\n",
        "    yearly_dataframes[year] = df_with_month\n",
        "    \n",
        "    # üÜï Show sample of the data\n",
        "    print(f\"üìã Sample data for {year}:\")\n",
        "    if 'tripid' in df_with_month.columns:\n",
        "        df_with_month.select(\n",
        "            \"tripid\", \"pickup_zone\", \"dropoff_zone\", \n",
        "            \"trip_distance\", \"fare_amount\", \"year_month\"\n",
        "        ).show(3)\n",
        "    else:\n",
        "        # Show whatever columns are available\n",
        "        df_with_month.show(3)\n",
        "    \n",
        "    # ----\n",
        "    # YOUR PROCESSING LOGIC HERE for df_with_month\n",
        "    # ----\n",
        "    \n",
        "    print(f\"üíæ Year {year} processing complete\")\n",
        "\n",
        "print(f\"\\nüéâ All yearly batches processed!\")\n",
        "print(f\"üìä Years available: {list(yearly_dataframes.keys())}\")\n",
        "print(f\"üí° Access individual years using: yearly_dataframes[2015], yearly_dataframes[2016], etc.\")\n",
        "\n",
        "# üÜï Optional: Create combined DataFrame from all years\n",
        "print(f\"\\nüîó Creating combined DataFrame...\")\n",
        "all_years_df = None\n",
        "for year, df in yearly_dataframes.items():\n",
        "    if all_years_df is None:\n",
        "        all_years_df = df\n",
        "    else:\n",
        "        all_years_df = all_years_df.union(df)\n",
        "\n",
        "if all_years_df:\n",
        "    total_rows = all_years_df.count()\n",
        "    print(f\"‚úÖ Combined DataFrame created with {total_rows:,} total rows\")\n",
        "    print(f\"üìä Data spans {len(yearly_dataframes)} years\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No data loaded\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmMnyGDFkDxH"
      },
      "source": [
        "## Multiplot plotting functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 299,
          "status": "ok",
          "timestamp": 1754355992118,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "0cJiwf4PkFTI",
        "outputId": "d6bad8f7-6d25-4a80-aa20-d8f1b1a8349f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Multi-panel plotting functions created (R multiplot equivalent)\n",
            "‚úÖ PySpark-specific multiplot functions ready\n",
            "‚úÖ Quick comparison plotting utilities available\n",
            "\n",
            "==================================================\n",
            "MULTIPLOT USAGE EXAMPLES\n",
            "==================================================\n",
            "\n",
            "# Basic usage:\n",
            "fig1, ax1 = plt.subplots()\n",
            "ax1.plot([1,2,3], [1,4,2])\n",
            "\n",
            "fig2, ax2 = plt.subplots()\n",
            "ax2.bar([1,2,3], [3,1,4])\n",
            "\n",
            "combined = multiplot(fig1, fig2, cols=2, title=\"Side by Side\")\n",
            "\n",
            "# PySpark DataFrame plotting:\n",
            "plot_configs = [\n",
            "    (df_2020, {'type': 'hist', 'x': 'trip_distance', 'title': '2020 Trips'}),\n",
            "    (df_2021, {'type': 'hist', 'x': 'trip_distance', 'title': '2021 Trips'})\n",
            "]\n",
            "multiplot_spark(plot_configs, cols=2)\n",
            "\n",
            "# Quick year comparison:\n",
            "yearly_data = {2020: df_2020, 2021: df_2021, 2022: df_2022}\n",
            "quick_multiplot_comparison(yearly_data, 'hist', 'trip_distance')\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Multi-panel plotting function (R multiplot equivalent)\n",
        "# Courtesy of R Cookbooks adapted for Python/PySpark\n",
        "def multiplot(*plots, plotlist=None, cols=1, layout=None, figsize=(15, 10),\n",
        "              title=None, save_path=None, dpi=300):\n",
        "    \"\"\"\n",
        "    Create multi-panel plots from matplotlib/seaborn/plotly figures\n",
        "\n",
        "    Python equivalent of R's multiplot function from R Cookbooks\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    *plots : matplotlib figures or plot functions\n",
        "        Individual plots to be arranged\n",
        "    plotlist : list, optional\n",
        "        List of plots as alternative to *plots\n",
        "    cols : int, default=1\n",
        "        Number of columns in layout\n",
        "    layout : array-like, optional\n",
        "        Matrix specifying the layout. If present, 'cols' is ignored.\n",
        "        Example: [[1,2], [3,3]] means plot 1 top-left, 2 top-right, 3 bottom spanning both columns\n",
        "    figsize : tuple, default=(15, 10)\n",
        "        Figure size (width, height) in inches\n",
        "    title : str, optional\n",
        "        Overall title for the multi-panel plot\n",
        "    save_path : str, optional\n",
        "        Path to save the figure\n",
        "    dpi : int, default=300\n",
        "        Resolution for saved figure\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    matplotlib.figure.Figure\n",
        "        The combined figure with all subplots\n",
        "\n",
        "    Example:\n",
        "    --------\n",
        "    # Create individual plots\n",
        "    fig1, ax1 = plt.subplots()\n",
        "    ax1.plot([1,2,3], [1,4,2])\n",
        "\n",
        "    fig2, ax2 = plt.subplots()\n",
        "    ax2.bar([1,2,3], [3,1,4])\n",
        "\n",
        "    # Combine them\n",
        "    combined_fig = multiplot(fig1, fig2, cols=2, title=\"Combined Analysis\")\n",
        "    \"\"\"\n",
        "\n",
        "    # Combine plots from arguments and plotlist\n",
        "    all_plots = list(plots) if plots else []\n",
        "    if plotlist:\n",
        "        all_plots.extend(plotlist)\n",
        "\n",
        "    num_plots = len(all_plots)\n",
        "\n",
        "    if num_plots == 0:\n",
        "        print(\"‚ö†Ô∏è No plots provided\")\n",
        "        return None\n",
        "\n",
        "    # Handle single plot case\n",
        "    if num_plots == 1:\n",
        "        if hasattr(all_plots[0], 'show'):\n",
        "            all_plots[0].show()\n",
        "        else:\n",
        "            plt.figure(figsize=figsize)\n",
        "            if title:\n",
        "                plt.suptitle(title, fontsize=16, fontweight='bold')\n",
        "            plt.show()\n",
        "        return all_plots[0]\n",
        "\n",
        "    # Determine layout\n",
        "    if layout is None:\n",
        "        # Calculate rows and columns\n",
        "        nrows = math.ceil(num_plots / cols)\n",
        "        ncols = cols\n",
        "        layout_matrix = np.arange(1, cols * nrows + 1).reshape(nrows, ncols)\n",
        "    else:\n",
        "        layout_matrix = np.array(layout)\n",
        "        nrows, ncols = layout_matrix.shape\n",
        "\n",
        "    # Create the main figure\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "\n",
        "    if title:\n",
        "        fig.suptitle(title, fontsize=16, fontweight='bold', y=0.95)\n",
        "\n",
        "    # Create GridSpec for flexible subplot arrangement\n",
        "    gs = gridspec.GridSpec(nrows, ncols, figure=fig, hspace=0.3, wspace=0.3)\n",
        "\n",
        "    # Place each plot in the correct position\n",
        "    for i, plot in enumerate(all_plots, 1):\n",
        "        if i > num_plots:\n",
        "            break\n",
        "\n",
        "        # Find positions where this plot should go\n",
        "        positions = np.where(layout_matrix == i)\n",
        "\n",
        "        if len(positions[0]) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate subplot span\n",
        "        row_min, row_max = positions[0].min(), positions[0].max()\n",
        "        col_min, col_max = positions[1].min(), positions[1].max()\n",
        "\n",
        "        # Create subplot\n",
        "        ax = fig.add_subplot(gs[row_min:row_max+1, col_min:col_max+1])\n",
        "\n",
        "        # Handle different plot types\n",
        "        if hasattr(plot, 'figure'):\n",
        "            # It's a matplotlib figure\n",
        "            _copy_plot_to_axis(plot, ax)\n",
        "        elif callable(plot):\n",
        "            # It's a plotting function\n",
        "            plot(ax)\n",
        "        elif hasattr(plot, 'axes'):\n",
        "            # It's a figure with axes\n",
        "            _copy_plot_to_axis(plot, ax)\n",
        "        else:\n",
        "            # Try to handle as data for direct plotting\n",
        "            ax.text(0.5, 0.5, f'Plot {i}', ha='center', va='center',\n",
        "                   transform=ax.transAxes)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save if path provided\n",
        "    if save_path:\n",
        "        fig.savefig(save_path, dpi=dpi, bbox_inches='tight')\n",
        "        print(f\"üíæ Multi-panel plot saved to: {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def _copy_plot_to_axis(source_fig, target_ax):\n",
        "    \"\"\"Helper function to copy plot content from source figure to target axis\"\"\"\n",
        "    try:\n",
        "        if hasattr(source_fig, 'axes') and source_fig.axes:\n",
        "            source_ax = source_fig.axes[0]\n",
        "\n",
        "            # Copy lines\n",
        "            for line in source_ax.get_lines():\n",
        "                target_ax.plot(line.get_xdata(), line.get_ydata(),\n",
        "                             color=line.get_color(), linewidth=line.get_linewidth(),\n",
        "                             linestyle=line.get_linestyle(), marker=line.get_marker(),\n",
        "                             label=line.get_label())\n",
        "\n",
        "            # Copy patches (bars, etc.)\n",
        "            for patch in source_ax.patches:\n",
        "                target_ax.add_patch(patch)\n",
        "\n",
        "            # Copy collections (scatter plots, etc.)\n",
        "            for collection in source_ax.collections:\n",
        "                target_ax.add_collection(collection)\n",
        "\n",
        "            # Copy labels and title\n",
        "            target_ax.set_xlabel(source_ax.get_xlabel())\n",
        "            target_ax.set_ylabel(source_ax.get_ylabel())\n",
        "            target_ax.set_title(source_ax.get_title())\n",
        "\n",
        "            # Copy limits\n",
        "            target_ax.set_xlim(source_ax.get_xlim())\n",
        "            target_ax.set_ylim(source_ax.get_ylim())\n",
        "\n",
        "            # Copy legend if exists\n",
        "            if source_ax.get_legend():\n",
        "                target_ax.legend()\n",
        "\n",
        "    except Exception as e:\n",
        "        # Fallback: just add a text placeholder\n",
        "        target_ax.text(0.5, 0.5, 'Plot Content', ha='center', va='center',\n",
        "                      transform=target_ax.transAxes)\n",
        "        print(f\"‚ö†Ô∏è Could not copy plot content: {e}\")\n",
        "\n",
        "# Enhanced multiplot for PySpark DataFrames\n",
        "def multiplot_spark(dataframes_and_plots, cols=2, figsize=(15, 10), title=None):\n",
        "    \"\"\"\n",
        "    Create multi-panel plots specifically for PySpark DataFrame visualizations\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    dataframes_and_plots : list of tuples\n",
        "        Each tuple contains (spark_dataframe, plot_config)\n",
        "        plot_config is a dict with keys: 'type', 'x', 'y', 'title', etc.\n",
        "    cols : int\n",
        "        Number of columns\n",
        "    figsize : tuple\n",
        "        Figure size\n",
        "    title : str\n",
        "        Overall title\n",
        "\n",
        "    Example:\n",
        "    --------\n",
        "    plot_configs = [\n",
        "        (df_2020, {'type': 'hist', 'x': 'trip_distance', 'title': '2020 Trip Distance'}),\n",
        "        (df_2021, {'type': 'scatter', 'x': 'trip_distance', 'y': 'fare_amount', 'title': '2021 Distance vs Fare'}),\n",
        "        (df_2022, {'type': 'bar', 'x': 'pickup_hour', 'y': 'count', 'title': '2022 Hourly Trips'})\n",
        "    ]\n",
        "    multiplot_spark(plot_configs, cols=2, title=\"Yearly Comparison\")\n",
        "    \"\"\"\n",
        "\n",
        "    num_plots = len(dataframes_and_plots)\n",
        "    nrows = math.ceil(num_plots / cols)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows, cols, figsize=figsize)\n",
        "    if title:\n",
        "        fig.suptitle(title, fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Flatten axes array for easy indexing\n",
        "    if num_plots == 1:\n",
        "        axes = [axes]\n",
        "    elif nrows == 1:\n",
        "        axes = axes\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    for i, (spark_df, plot_config) in enumerate(dataframes_and_plots):\n",
        "        if i >= len(axes):\n",
        "            break\n",
        "\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Convert to Pandas for plotting\n",
        "        pandas_df = spark_df.toPandas()\n",
        "\n",
        "        plot_type = plot_config.get('type', 'scatter')\n",
        "        x_col = plot_config.get('x')\n",
        "        y_col = plot_config.get('y')\n",
        "        plot_title = plot_config.get('title', f'Plot {i+1}')\n",
        "\n",
        "        # Create the appropriate plot\n",
        "        if plot_type == 'hist':\n",
        "            ax.hist(pandas_df[x_col], bins=30, alpha=0.7)\n",
        "            ax.set_xlabel(x_col)\n",
        "            ax.set_ylabel('Frequency')\n",
        "        elif plot_type == 'scatter':\n",
        "            ax.scatter(pandas_df[x_col], pandas_df[y_col], alpha=0.6)\n",
        "            ax.set_xlabel(x_col)\n",
        "            ax.set_ylabel(y_col)\n",
        "        elif plot_type == 'bar':\n",
        "            if y_col:\n",
        "                ax.bar(pandas_df[x_col], pandas_df[y_col])\n",
        "            else:\n",
        "                value_counts = pandas_df[x_col].value_counts()\n",
        "                ax.bar(value_counts.index, value_counts.values)\n",
        "            ax.set_xlabel(x_col)\n",
        "            ax.set_ylabel(y_col or 'Count')\n",
        "        elif plot_type == 'line':\n",
        "            ax.plot(pandas_df[x_col], pandas_df[y_col])\n",
        "            ax.set_xlabel(x_col)\n",
        "            ax.set_ylabel(y_col)\n",
        "\n",
        "        ax.set_title(plot_title)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for i in range(num_plots, len(axes)):\n",
        "        axes[i].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "# Quick plotting helper for common PySpark visualizations\n",
        "def quick_multiplot_comparison(yearly_data_dict, plot_type='hist', column='trip_distance',\n",
        "                              cols=2, figsize=(15, 10)):\n",
        "    \"\"\"\n",
        "    Quick comparison plots across multiple years\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    yearly_data_dict : dict\n",
        "        Dictionary with year as key, PySpark DataFrame as value\n",
        "    plot_type : str\n",
        "        Type of plot ('hist', 'box', 'violin')\n",
        "    column : str\n",
        "        Column to plot\n",
        "    cols : int\n",
        "        Number of columns in layout\n",
        "    figsize : tuple\n",
        "        Figure size\n",
        "\n",
        "    Example:\n",
        "    --------\n",
        "    yearly_data = {\n",
        "        2020: df_2020,\n",
        "        2021: df_2021,\n",
        "        2022: df_2022\n",
        "    }\n",
        "    quick_multiplot_comparison(yearly_data, 'hist', 'trip_distance')\n",
        "    \"\"\"\n",
        "\n",
        "    plot_configs = []\n",
        "    for year, df in yearly_data_dict.items():\n",
        "        config = {\n",
        "            'type': plot_type,\n",
        "            'x': column,\n",
        "            'title': f'{year} - {column.replace(\"_\", \" \").title()}'\n",
        "        }\n",
        "        plot_configs.append((df, config))\n",
        "\n",
        "    return multiplot_spark(plot_configs, cols=cols, figsize=figsize,\n",
        "                          title=f\"Year-over-Year Comparison: {column.replace('_', ' ').title()}\")\n",
        "\n",
        "print(\"‚úÖ Multi-panel plotting functions created (R multiplot equivalent)\")\n",
        "print(\"‚úÖ PySpark-specific multiplot functions ready\")\n",
        "print(\"‚úÖ Quick comparison plotting utilities available\")\n",
        "\n",
        "# Example usage guide\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MULTIPLOT USAGE EXAMPLES\")\n",
        "print(\"=\"*50)\n",
        "print(\"\"\"\n",
        "# Basic usage:\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.plot([1,2,3], [1,4,2])\n",
        "\n",
        "fig2, ax2 = plt.subplots()\n",
        "ax2.bar([1,2,3], [3,1,4])\n",
        "\n",
        "combined = multiplot(fig1, fig2, cols=2, title=\"Side by Side\")\n",
        "\n",
        "# PySpark DataFrame plotting:\n",
        "plot_configs = [\n",
        "    (df_2020, {'type': 'hist', 'x': 'trip_distance', 'title': '2020 Trips'}),\n",
        "    (df_2021, {'type': 'hist', 'x': 'trip_distance', 'title': '2021 Trips'})\n",
        "]\n",
        "multiplot_spark(plot_configs, cols=2)\n",
        "\n",
        "# Quick year comparison:\n",
        "yearly_data = {2020: df_2020, 2021: df_2021, 2022: df_2022}\n",
        "quick_multiplot_comparison(yearly_data, 'hist', 'trip_distance')\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "executionInfo": {
          "elapsed": 6709,
          "status": "ok",
          "timestamp": 1754356006580,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "cHq7xG_LkKzi",
        "outputId": "bd8117c5-440f-4f3e-9b3b-7fb5ab097d26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded combined data across years\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "              <div>\n",
              "                  <p><a href=\"https://console.cloud.google.com/dataproc/interactive/us-central1/sc-20250805-005920-tj1ii9/sparkApplications/application;associatedSqlOperationId=4aee8357-a7ad-4f99-82fb-8850048462e0?project=dtc-de-course-466501\">Spark Query</a> (Operation: 4aee8357-a7ad-4f99-82fb-8850048462e0)</p>\n",
              "              </div>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------+------------+----------+-----------------+--------------+-----------------+------------------+---------------+--------------------+-------------------+-----------+-------------------+------------------+---------------+-------------+------------+-----------+-----------+-----------+------------+---------------------+------------+------------+------------------------+------------+-----+-----------+-----------+------------+---------------+------------+------------+------------+-----------+\n",
            "|              tripid|vendorid|service_type|ratecodeid|pickup_locationid|pickup_borough|      pickup_zone|dropoff_locationid|dropoff_borough|        dropoff_zone|    pickup_datetime|pickup_date|   dropoff_datetime|store_and_fwd_flag|passenger_count|trip_distance| fare_amount|      extra|    mta_tax| tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type|payment_type_description|climate_date|  mjd| cloudCover|   humidity|    dewPoint|precipIntensity|    highTemp|     lowTemp|  visibility|  windSpeed|\n",
            "+--------------------+--------+------------+----------+-----------------+--------------+-----------------+------------------+---------------+--------------------+-------------------+-----------+-------------------+------------------+---------------+-------------+------------+-----------+-----------+-----------+------------+---------------------+------------+------------+------------------------+------------+-----+-----------+-----------+------------+---------------+------------+------------+------------+-----------+\n",
            "|8a3b73f6ece7290ca...|       2|      Yellow|         1|               79|     Manhattan|     East Village|               223|         Queens|            Steinway|2015-01-01 00:00:42| 2015-01-01|2015-01-01 00:22:22|                 N|              1|  7.080000000|23.000000000|0.500000000|0.500000000|4.700000000| 0.000000000|          0.000000000|28.700000000|           1|             Credit card|  2015-01-01|57023|0.420000000|0.500000000|11.790000000|    0.000000000|31.870000000|24.320000000|10.000000000|2.790000000|\n",
            "|719a87b7eef227076...|       1|       Green|         1|              197|        Queens|    Richmond Hill|               238|      Manhattan|Upper West Side N...|2015-01-01 00:06:04| 2015-01-01|2015-01-01 00:39:02|                 N|              1| 16.600000000|47.000000000|0.500000000|0.500000000|2.220000000| 5.330000000|          0.300000000|55.850000000|           1|             Credit card|  2015-01-01|57023|0.420000000|0.500000000|11.790000000|    0.000000000|31.870000000|24.320000000|10.000000000|2.790000000|\n",
            "|357cb0487cc11568d...|       2|      Yellow|         1|               74|     Manhattan|East Harlem North|                75|      Manhattan|   East Harlem South|2015-01-01 00:10:40| 2015-01-01|2015-01-01 00:15:34|                 N|              1|  1.150000000| 6.000000000|0.500000000|0.500000000|0.000000000| 0.000000000|          0.300000000| 7.300000000|           1|             Credit card|  2015-01-01|57023|0.420000000|0.500000000|11.790000000|    0.000000000|31.870000000|24.320000000|10.000000000|2.790000000|\n",
            "|85828eaa6a004fefc...|       1|      Yellow|         1|              249|     Manhattan|     West Village|                80|       Brooklyn|   East Williamsburg|2015-01-01 00:21:14| 2015-01-01|2015-01-01 00:38:55|                 N|              1|  4.300000000|15.700000000|0.500000000|0.500000000|0.000000000| 0.000000000|          0.000000000|17.000000000|           1|             Credit card|  2015-01-01|57023|0.420000000|0.500000000|11.790000000|    0.000000000|31.870000000|24.320000000|10.000000000|2.790000000|\n",
            "|988e7cd362c65f834...|       1|      Yellow|         1|              164|     Manhattan|    Midtown South|               233|      Manhattan| UN/Turtle Bay South|2015-01-01 00:22:38| 2015-01-01|2015-01-01 00:36:44|                 N|              1|  1.600000000|10.000000000|0.500000000|0.500000000|2.500000000| 0.000000000|          0.000000000|13.800000000|           1|             Credit card|  2015-01-01|57023|0.420000000|0.500000000|11.790000000|    0.000000000|31.870000000|24.320000000|10.000000000|2.790000000|\n",
            "+--------------------+--------+------------+----------+-----------------+--------------+-----------------+------------------+---------------+--------------------+-------------------+-----------+-------------------+------------------+---------------+-------------+------------+-----------+-----------+-----------+------------+---------------------+------------+------------+------------------------+------------+-----+-----------+-----------+------------+---------------+------------+------------+------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "combined_df = None\n",
        "\n",
        "for year in range(2015, 2017):\n",
        "  df_year = (\n",
        "      spark.read\n",
        "           .format(\"bigquery\")\n",
        "           .option(\"table\", table_name)\n",
        "           .option(\"filter\", f\"EXTRACT(YEAR FROM pickup_date) = {year}\")\n",
        "           .load()\n",
        "  )\n",
        "\n",
        "  if combined_df is None:\n",
        "    combined_df = df_year\n",
        "  else:\n",
        "    combined_df = combined_df.unionByName(df_year)\n",
        "\n",
        "print(\"‚úÖ Loaded combined data across years\")\n",
        "combined_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NR4LJuUI1tP"
      },
      "source": [
        "**Potential Optimization for this query**\n",
        "- None as of now. It is super-fast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "executionInfo": {
          "elapsed": 6215,
          "status": "ok",
          "timestamp": 1754356078716,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "replN1TOkUyU",
        "outputId": "48b3728e-f920-43dc-9c84-2c56604e34f1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "              <div>\n",
              "                  <p><a href=\"https://console.cloud.google.com/dataproc/interactive/us-central1/sc-20250805-005920-tj1ii9/sparkApplications/application;associatedSqlOperationId=72ccc917-57ab-430c-a1ae-a8996e5340ac?project=dtc-de-course-466501\">Spark Query</a> (Operation: 72ccc917-57ab-430c-a1ae-a8996e5340ac)</p>\n",
              "              </div>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------+------------+----------+-----------------+--------------+-----------------+------------------+---------------+--------------------+-------------------+-----------+-------------------+------------------+---------------+-------------+------------+-----------+-----------+-----------+------------+---------------------+------------+------------+------------------------+------------+-----+-----------+-----------+------------+---------------+------------+------------+------------+-----------+-----------+------------+\n",
            "|              tripid|vendorid|service_type|ratecodeid|pickup_locationid|pickup_borough|      pickup_zone|dropoff_locationid|dropoff_borough|        dropoff_zone|    pickup_datetime|pickup_date|   dropoff_datetime|store_and_fwd_flag|passenger_count|trip_distance| fare_amount|      extra|    mta_tax| tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type|payment_type_description|climate_date|  mjd| cloudCover|   humidity|    dewPoint|precipIntensity|    highTemp|     lowTemp|  visibility|  windSpeed|pickup_year|pickup_month|\n",
            "+--------------------+--------+------------+----------+-----------------+--------------+-----------------+------------------+---------------+--------------------+-------------------+-----------+-------------------+------------------+---------------+-------------+------------+-----------+-----------+-----------+------------+---------------------+------------+------------+------------------------+------------+-----+-----------+-----------+------------+---------------+------------+------------+------------+-----------+-----------+------------+\n",
            "|8a3b73f6ece7290ca...|       2|      Yellow|         1|               79|     Manhattan|     East Village|               223|         Queens|            Steinway|2015-01-01 00:00:42| 2015-01-01|2015-01-01 00:22:22|                 N|              1|  7.080000000|23.000000000|0.500000000|0.500000000|4.700000000| 0.000000000|          0.000000000|28.700000000|           1|             Credit card|  2015-01-01|57023|0.420000000|0.500000000|11.790000000|    0.000000000|31.870000000|24.320000000|10.000000000|2.790000000|       2015|           1|\n",
            "|719a87b7eef227076...|       1|       Green|         1|              197|        Queens|    Richmond Hill|               238|      Manhattan|Upper West Side N...|2015-01-01 00:06:04| 2015-01-01|2015-01-01 00:39:02|                 N|              1| 16.600000000|47.000000000|0.500000000|0.500000000|2.220000000| 5.330000000|          0.300000000|55.850000000|           1|             Credit card|  2015-01-01|57023|0.420000000|0.500000000|11.790000000|    0.000000000|31.870000000|24.320000000|10.000000000|2.790000000|       2015|           1|\n",
            "|357cb0487cc11568d...|       2|      Yellow|         1|               74|     Manhattan|East Harlem North|                75|      Manhattan|   East Harlem South|2015-01-01 00:10:40| 2015-01-01|2015-01-01 00:15:34|                 N|              1|  1.150000000| 6.000000000|0.500000000|0.500000000|0.000000000| 0.000000000|          0.300000000| 7.300000000|           1|             Credit card|  2015-01-01|57023|0.420000000|0.500000000|11.790000000|    0.000000000|31.870000000|24.320000000|10.000000000|2.790000000|       2015|           1|\n",
            "|85828eaa6a004fefc...|       1|      Yellow|         1|              249|     Manhattan|     West Village|                80|       Brooklyn|   East Williamsburg|2015-01-01 00:21:14| 2015-01-01|2015-01-01 00:38:55|                 N|              1|  4.300000000|15.700000000|0.500000000|0.500000000|0.000000000| 0.000000000|          0.000000000|17.000000000|           1|             Credit card|  2015-01-01|57023|0.420000000|0.500000000|11.790000000|    0.000000000|31.870000000|24.320000000|10.000000000|2.790000000|       2015|           1|\n",
            "|988e7cd362c65f834...|       1|      Yellow|         1|              164|     Manhattan|    Midtown South|               233|      Manhattan| UN/Turtle Bay South|2015-01-01 00:22:38| 2015-01-01|2015-01-01 00:36:44|                 N|              1|  1.600000000|10.000000000|0.500000000|0.500000000|2.500000000| 0.000000000|          0.000000000|13.800000000|           1|             Credit card|  2015-01-01|57023|0.420000000|0.500000000|11.790000000|    0.000000000|31.870000000|24.320000000|10.000000000|2.790000000|       2015|           1|\n",
            "+--------------------+--------+------------+----------+-----------------+--------------+-----------------+------------------+---------------+--------------------+-------------------+-----------+-------------------+------------------+---------------+-------------+------------+-----------+-----------+-----------+------------+---------------------+------------+------------+------------------------+------------+-----+-----------+-----------+------------+---------------+------------+------------+------------+-----------+-----------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_with_time = (\n",
        "    combined_df\n",
        "    .withColumn(\"pickup_year\", F.year(\"pickup_date\"))\n",
        "    .withColumn(\"pickup_month\", F.month(\"pickup_date\"))\n",
        ")\n",
        "\n",
        "df_with_time.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exBim-CwqJRl"
      },
      "source": [
        "**Consistency Check**\n",
        "- A column called `trip_durations` needs to be created which calculates the intervals between `pickup_datetime` and `dropoff_datetime`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVrZu5YLJCWP"
      },
      "source": [
        "**Potential Optimization for this query**\n",
        "- None as of now. Works for the time being"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "executionInfo": {
          "elapsed": 5443,
          "status": "ok",
          "timestamp": 1754356172874,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "vCn-vvcTqaJr",
        "outputId": "048be926-6246-41eb-b0e6-d9f8c6ac4815"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "              <div>\n",
              "                  <p><a href=\"https://console.cloud.google.com/dataproc/interactive/us-central1/sc-20250805-005920-tj1ii9/sparkApplications/application;associatedSqlOperationId=8e797d27-a456-423c-adcb-fccda9ab9ed0?project=dtc-de-course-466501\">Spark Query</a> (Operation: 8e797d27-a456-423c-adcb-fccda9ab9ed0)</p>\n",
              "              </div>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+------------------+\n",
            "|pickup_datetime    |dropoff_datetime   |trip_duration_min |\n",
            "+-------------------+-------------------+------------------+\n",
            "|2015-01-01 00:00:42|2015-01-01 00:22:22|21.666666666666668|\n",
            "|2015-01-01 00:06:04|2015-01-01 00:39:02|32.96666666666667 |\n",
            "|2015-01-01 00:10:40|2015-01-01 00:15:34|4.9               |\n",
            "|2015-01-01 00:21:14|2015-01-01 00:38:55|17.683333333333334|\n",
            "|2015-01-01 00:22:38|2015-01-01 00:36:44|14.1              |\n",
            "+-------------------+-------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_with_duration = df_with_time.withColumn(\n",
        "    \"trip_duration_min\",\n",
        "    (\n",
        "        (F.col(\"dropoff_datetime\").cast(\"long\")\n",
        "         - F.col(\"pickup_datetime\").cast(\"long\"))\n",
        "        .cast(\"double\")\n",
        "        / F.lit(60.0)\n",
        "    )\n",
        ")\n",
        "\n",
        "df_with_duration.select(\n",
        "    \"pickup_datetime\", \"dropoff_datetime\", \"trip_duration_min\"\n",
        ").show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoTvdMMj0tf6"
      },
      "source": [
        "**Potential Optimization for this query**\n",
        "- None needed as of now. Fast enough IMO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3643,
          "status": "ok",
          "timestamp": 1754356176514,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "nPSZc8Sa0soY",
        "outputId": "52866aa5-58f7-4f9c-fb28-4d904671bb50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Parsed Logical Plan ==\n",
            "Project [tripid#3084, vendorid#3085L, service_type#3086, ratecodeid#3087L, pickup_locationid#3088L, pickup_borough#3089, pickup_zone#3090, dropoff_locationid#3091L, dropoff_borough#3092, dropoff_zone#3093, pickup_datetime#3094, pickup_date#3095, dropoff_datetime#3096, store_and_fwd_flag#3097, passenger_count#3098L, trip_distance#3099, fare_amount#3100, extra#3101, mta_tax#3102, tip_amount#3103, tolls_amount#3104, improvement_surcharge#3105, total_amount#3106, payment_type#3107L, ... 14 more fields]\n",
            "+- Project [tripid#3084, vendorid#3085L, service_type#3086, ratecodeid#3087L, pickup_locationid#3088L, pickup_borough#3089, pickup_zone#3090, dropoff_locationid#3091L, dropoff_borough#3092, dropoff_zone#3093, pickup_datetime#3094, pickup_date#3095, dropoff_datetime#3096, store_and_fwd_flag#3097, passenger_count#3098L, trip_distance#3099, fare_amount#3100, extra#3101, mta_tax#3102, tip_amount#3103, tolls_amount#3104, improvement_surcharge#3105, total_amount#3106, payment_type#3107L, ... 13 more fields]\n",
            "   +- Project [tripid#3084, vendorid#3085L, service_type#3086, ratecodeid#3087L, pickup_locationid#3088L, pickup_borough#3089, pickup_zone#3090, dropoff_locationid#3091L, dropoff_borough#3092, dropoff_zone#3093, pickup_datetime#3094, pickup_date#3095, dropoff_datetime#3096, store_and_fwd_flag#3097, passenger_count#3098L, trip_distance#3099, fare_amount#3100, extra#3101, mta_tax#3102, tip_amount#3103, tolls_amount#3104, improvement_surcharge#3105, total_amount#3106, payment_type#3107L, ... 12 more fields]\n",
            "      +- Union false, false\n",
            "         :- RelationV2[tripid#3084, vendorid#3085L, service_type#3086, ratecodeid#3087L, pickup_locationid#3088L, pickup_borough#3089, pickup_zone#3090, dropoff_locationid#3091L, dropoff_borough#3092, dropoff_zone#3093, pickup_datetime#3094, pickup_date#3095, dropoff_datetime#3096, store_and_fwd_flag#3097, passenger_count#3098L, trip_distance#3099, fare_amount#3100, extra#3101, mta_tax#3102, tip_amount#3103, tolls_amount#3104, improvement_surcharge#3105, total_amount#3106, payment_type#3107L, ... 11 more fields]  fact_trips\n",
            "         +- Project [tripid#3154, vendorid#3155L, service_type#3156, ratecodeid#3157L, pickup_locationid#3158L, pickup_borough#3159, pickup_zone#3160, dropoff_locationid#3161L, dropoff_borough#3162, dropoff_zone#3163, pickup_datetime#3164, pickup_date#3165, dropoff_datetime#3166, store_and_fwd_flag#3167, passenger_count#3168L, trip_distance#3169, fare_amount#3170, extra#3171, mta_tax#3172, tip_amount#3173, tolls_amount#3174, improvement_surcharge#3175, total_amount#3176, payment_type#3177L, ... 11 more fields]\n",
            "            +- RelationV2[tripid#3154, vendorid#3155L, service_type#3156, ratecodeid#3157L, pickup_locationid#3158L, pickup_borough#3159, pickup_zone#3160, dropoff_locationid#3161L, dropoff_borough#3162, dropoff_zone#3163, pickup_datetime#3164, pickup_date#3165, dropoff_datetime#3166, store_and_fwd_flag#3167, passenger_count#3168L, trip_distance#3169, fare_amount#3170, extra#3171, mta_tax#3172, tip_amount#3173, tolls_amount#3174, improvement_surcharge#3175, total_amount#3176, payment_type#3177L, ... 11 more fields]  fact_trips\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "tripid: string, vendorid: bigint, service_type: string, ratecodeid: bigint, pickup_locationid: bigint, pickup_borough: string, pickup_zone: string, dropoff_locationid: bigint, dropoff_borough: string, dropoff_zone: string, pickup_datetime: timestamp, pickup_date: date, dropoff_datetime: timestamp, store_and_fwd_flag: string, passenger_count: bigint, trip_distance: decimal(38,9), fare_amount: decimal(38,9), extra: decimal(38,9), mta_tax: decimal(38,9), tip_amount: decimal(38,9), tolls_amount: decimal(38,9), improvement_surcharge: decimal(38,9), total_amount: decimal(38,9), payment_type: bigint, ... 14 more fields\n",
            "Project [tripid#3084, vendorid#3085L, service_type#3086, ratecodeid#3087L, pickup_locationid#3088L, pickup_borough#3089, pickup_zone#3090, dropoff_locationid#3091L, dropoff_borough#3092, dropoff_zone#3093, pickup_datetime#3094, pickup_date#3095, dropoff_datetime#3096, store_and_fwd_flag#3097, passenger_count#3098L, trip_distance#3099, fare_amount#3100, extra#3101, mta_tax#3102, tip_amount#3103, tolls_amount#3104, improvement_surcharge#3105, total_amount#3106, payment_type#3107L, ... 14 more fields]\n",
            "+- Project [tripid#3084, vendorid#3085L, service_type#3086, ratecodeid#3087L, pickup_locationid#3088L, pickup_borough#3089, pickup_zone#3090, dropoff_locationid#3091L, dropoff_borough#3092, dropoff_zone#3093, pickup_datetime#3094, pickup_date#3095, dropoff_datetime#3096, store_and_fwd_flag#3097, passenger_count#3098L, trip_distance#3099, fare_amount#3100, extra#3101, mta_tax#3102, tip_amount#3103, tolls_amount#3104, improvement_surcharge#3105, total_amount#3106, payment_type#3107L, ... 13 more fields]\n",
            "   +- Project [tripid#3084, vendorid#3085L, service_type#3086, ratecodeid#3087L, pickup_locationid#3088L, pickup_borough#3089, pickup_zone#3090, dropoff_locationid#3091L, dropoff_borough#3092, dropoff_zone#3093, pickup_datetime#3094, pickup_date#3095, dropoff_datetime#3096, store_and_fwd_flag#3097, passenger_count#3098L, trip_distance#3099, fare_amount#3100, extra#3101, mta_tax#3102, tip_amount#3103, tolls_amount#3104, improvement_surcharge#3105, total_amount#3106, payment_type#3107L, ... 12 more fields]\n",
            "      +- Union false, false\n",
            "         :- RelationV2[tripid#3084, vendorid#3085L, service_type#3086, ratecodeid#3087L, pickup_locationid#3088L, pickup_borough#3089, pickup_zone#3090, dropoff_locationid#3091L, dropoff_borough#3092, dropoff_zone#3093, pickup_datetime#3094, pickup_date#3095, dropoff_datetime#3096, store_and_fwd_flag#3097, passenger_count#3098L, trip_distance#3099, fare_amount#3100, extra#3101, mta_tax#3102, tip_amount#3103, tolls_amount#3104, improvement_surcharge#3105, total_amount#3106, payment_type#3107L, ... 11 more fields]  fact_trips\n",
            "         +- Project [tripid#3154, vendorid#3155L, service_type#3156, ratecodeid#3157L, pickup_locationid#3158L, pickup_borough#3159, pickup_zone#3160, dropoff_locationid#3161L, dropoff_borough#3162, dropoff_zone#3163, pickup_datetime#3164, pickup_date#3165, dropoff_datetime#3166, store_and_fwd_flag#3167, passenger_count#3168L, trip_distance#3169, fare_amount#3170, extra#3171, mta_tax#3172, tip_amount#3173, tolls_amount#3174, improvement_surcharge#3175, total_amount#3176, payment_type#3177L, ... 11 more fields]\n",
            "            +- RelationV2[tripid#3154, vendorid#3155L, service_type#3156, ratecodeid#3157L, pickup_locationid#3158L, pickup_borough#3159, pickup_zone#3160, dropoff_locationid#3161L, dropoff_borough#3162, dropoff_zone#3163, pickup_datetime#3164, pickup_date#3165, dropoff_datetime#3166, store_and_fwd_flag#3167, passenger_count#3168L, trip_distance#3169, fare_amount#3170, extra#3171, mta_tax#3172, tip_amount#3173, tolls_amount#3174, improvement_surcharge#3175, total_amount#3176, payment_type#3177L, ... 11 more fields]  fact_trips\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Union false, false\n",
            ":- Project [tripid#3084, vendorid#3085L, service_type#3086, ratecodeid#3087L, pickup_locationid#3088L, pickup_borough#3089, pickup_zone#3090, dropoff_locationid#3091L, dropoff_borough#3092, dropoff_zone#3093, pickup_datetime#3094, pickup_date#3095, dropoff_datetime#3096, store_and_fwd_flag#3097, passenger_count#3098L, trip_distance#3099, fare_amount#3100, extra#3101, mta_tax#3102, tip_amount#3103, tolls_amount#3104, improvement_surcharge#3105, total_amount#3106, payment_type#3107L, ... 14 more fields]\n",
            ":  +- RelationV2[tripid#3084, vendorid#3085L, service_type#3086, ratecodeid#3087L, pickup_locationid#3088L, pickup_borough#3089, pickup_zone#3090, dropoff_locationid#3091L, dropoff_borough#3092, dropoff_zone#3093, pickup_datetime#3094, pickup_date#3095, dropoff_datetime#3096, store_and_fwd_flag#3097, passenger_count#3098L, trip_distance#3099, fare_amount#3100, extra#3101, mta_tax#3102, tip_amount#3103, tolls_amount#3104, improvement_surcharge#3105, total_amount#3106, payment_type#3107L, ... 11 more fields] fact_trips\n",
            "+- Project [tripid#3154, vendorid#3155L, service_type#3156, ratecodeid#3157L, pickup_locationid#3158L, pickup_borough#3159, pickup_zone#3160, dropoff_locationid#3161L, dropoff_borough#3162, dropoff_zone#3163, pickup_datetime#3164, pickup_date#3165, dropoff_datetime#3166, store_and_fwd_flag#3167, passenger_count#3168L, trip_distance#3169, fare_amount#3170, extra#3171, mta_tax#3172, tip_amount#3173, tolls_amount#3174, improvement_surcharge#3175, total_amount#3176, payment_type#3177L, ... 14 more fields]\n",
            "   +- RelationV2[tripid#3154, vendorid#3155L, service_type#3156, ratecodeid#3157L, pickup_locationid#3158L, pickup_borough#3159, pickup_zone#3160, dropoff_locationid#3161L, dropoff_borough#3162, dropoff_zone#3163, pickup_datetime#3164, pickup_date#3165, dropoff_datetime#3166, store_and_fwd_flag#3167, passenger_count#3168L, trip_distance#3169, fare_amount#3170, extra#3171, mta_tax#3172, tip_amount#3173, tolls_amount#3174, improvement_surcharge#3175, total_amount#3176, payment_type#3177L, ... 11 more fields] fact_trips\n",
            "\n",
            "== Physical Plan ==\n",
            "Union\n",
            ":- *(1) Project [tripid#3084, vendorid#3085L, service_type#3086, ratecodeid#3087L, pickup_locationid#3088L, pickup_borough#3089, pickup_zone#3090, dropoff_locationid#3091L, dropoff_borough#3092, dropoff_zone#3093, pickup_datetime#3094, pickup_date#3095, dropoff_datetime#3096, store_and_fwd_flag#3097, passenger_count#3098L, trip_distance#3099, fare_amount#3100, extra#3101, mta_tax#3102, tip_amount#3103, tolls_amount#3104, improvement_surcharge#3105, total_amount#3106, payment_type#3107L, ... 14 more fields]\n",
            ":  +- *(1) ColumnarToRow\n",
            ":     +- BatchScan fact_trips[tripid#3084, vendorid#3085L, service_type#3086, ratecodeid#3087L, pickup_locationid#3088L, pickup_borough#3089, pickup_zone#3090, dropoff_locationid#3091L, dropoff_borough#3092, dropoff_zone#3093, pickup_datetime#3094, pickup_date#3095, dropoff_datetime#3096, store_and_fwd_flag#3097, passenger_count#3098L, trip_distance#3099, fare_amount#3100, extra#3101, mta_tax#3102, tip_amount#3103, tolls_amount#3104, improvement_surcharge#3105, total_amount#3106, payment_type#3107L, ... 11 more fields] Reading table [dtc-de-course-466501.dbt_production.fact_trips], Read session Id : projects/dtc-de-course-466501/locations/us/sessions/CAISDGlSamk2VGZMeGluYhoCamYaAmpkGgJpZxoCbnkaAmljGgd5dWNiZmFi  RuntimeFilters: []\n",
            "+- *(2) Project [tripid#3154, vendorid#3155L, service_type#3156, ratecodeid#3157L, pickup_locationid#3158L, pickup_borough#3159, pickup_zone#3160, dropoff_locationid#3161L, dropoff_borough#3162, dropoff_zone#3163, pickup_datetime#3164, pickup_date#3165, dropoff_datetime#3166, store_and_fwd_flag#3167, passenger_count#3168L, trip_distance#3169, fare_amount#3170, extra#3171, mta_tax#3172, tip_amount#3173, tolls_amount#3174, improvement_surcharge#3175, total_amount#3176, payment_type#3177L, ... 14 more fields]\n",
            "   +- *(2) ColumnarToRow\n",
            "      +- BatchScan fact_trips[tripid#3154, vendorid#3155L, service_type#3156, ratecodeid#3157L, pickup_locationid#3158L, pickup_borough#3159, pickup_zone#3160, dropoff_locationid#3161L, dropoff_borough#3162, dropoff_zone#3163, pickup_datetime#3164, pickup_date#3165, dropoff_datetime#3166, store_and_fwd_flag#3167, passenger_count#3168L, trip_distance#3169, fare_amount#3170, extra#3171, mta_tax#3172, tip_amount#3173, tolls_amount#3174, improvement_surcharge#3175, total_amount#3176, payment_type#3177L, ... 11 more fields] Reading table [dtc-de-course-466501.dbt_production.fact_trips], Read session Id : projects/dtc-de-course-466501/locations/us/sessions/CAISDHF2NXdwV05hLXdSMBoCamYaAmpkGgJpZxoCbnkaAmljGgd5dWNiZmFi  RuntimeFilters: []\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_with_duration.explain(extended=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1754356176514,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "urz9zMrrKLlO"
      },
      "outputs": [],
      "source": [
        "def temporal_train_test_split(df, strategy=\"chronological\", test_ratio=0.2,\n",
        "                            random_seed=42, validation_split=True):\n",
        "    \"\"\"\n",
        "    Split time series data into train/test sets using various strategies\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : PySpark DataFrame\n",
        "        Input data with datetime columns\n",
        "    strategy : str\n",
        "        Splitting strategy: 'chronological', 'random', 'stratified_temporal', 'holdout_months'\n",
        "    test_ratio : float\n",
        "        Proportion of data for testing (default: 0.2 = 20%)\n",
        "    random_seed : int\n",
        "        Random seed for reproducibility\n",
        "    validation_split : bool\n",
        "        Whether to create a validation set (splits train further)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary with 'train', 'test', and optionally 'validation' DataFrames\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"üîÑ Splitting data using '{strategy}' strategy\")\n",
        "    print(f\"üìä Test ratio: {test_ratio*100}%\")\n",
        "\n",
        "    if strategy == \"chronological\":\n",
        "        # RECOMMENDED for time series: Use 2015 for training, 2016 for testing\n",
        "        train_df = df.filter(col(\"pickup_year\") == 2015)\n",
        "        test_df = df.filter(col(\"pickup_year\") == 2016)\n",
        "\n",
        "        result = {\"train\": train_df, \"test\": test_df}\n",
        "\n",
        "        if validation_split:\n",
        "            # Use last 2 months of 2015 for validation\n",
        "            train_final = train_df.filter(col(\"pickup_month\") <= 10)\n",
        "            validation_df = train_df.filter(col(\"pickup_month\") > 10)\n",
        "            result[\"train\"] = train_final\n",
        "            result[\"validation\"] = validation_df\n",
        "\n",
        "        print(\"‚úÖ Chronological split: 2015 (train) vs 2016 (test)\")\n",
        "\n",
        "    elif strategy == \"random\":\n",
        "        # Random split within each year to maintain temporal balance\n",
        "        df_with_split = df.withColumn(\"rand_val\", rand(seed=random_seed))\n",
        "\n",
        "        train_df = df_with_split.filter(col(\"rand_val\") > test_ratio)\n",
        "        test_df = df_with_split.filter(col(\"rand_val\") <= test_ratio)\n",
        "\n",
        "        result = {\"train\": train_df.drop(\"rand_val\"), \"test\": test_df.drop(\"rand_val\")}\n",
        "\n",
        "        if validation_split:\n",
        "            val_ratio = test_ratio / 2\n",
        "            train_final = train_df.filter(col(\"rand_val\") > (test_ratio + val_ratio))\n",
        "            validation_df = train_df.filter(\n",
        "                (col(\"rand_val\") > test_ratio) & (col(\"rand_val\") <= (test_ratio + val_ratio))\n",
        "            )\n",
        "            result[\"train\"] = train_final.drop(\"rand_val\")\n",
        "            result[\"validation\"] = validation_df.drop(\"rand_val\")\n",
        "\n",
        "        print(\"‚úÖ Random split across both years\")\n",
        "\n",
        "    elif strategy == \"stratified_temporal\":\n",
        "        # Stratified split maintaining temporal patterns (by month)\n",
        "        df_with_split = df.withColumn(\"rand_val\", rand(seed=random_seed))\n",
        "\n",
        "        # Create stratified split by month\n",
        "        train_df = df_with_split.filter(col(\"rand_val\") > test_ratio)\n",
        "        test_df = df_with_split.filter(col(\"rand_val\") <= test_ratio)\n",
        "\n",
        "        result = {\"train\": train_df.drop(\"rand_val\"), \"test\": test_df.drop(\"rand_val\")}\n",
        "        print(\"‚úÖ Stratified temporal split\")\n",
        "\n",
        "    elif strategy == \"holdout_months\":\n",
        "        # Hold out specific months for testing (e.g., Dec 2015 and Dec 2016)\n",
        "        train_df = df.filter(col(\"pickup_month\") != 12)\n",
        "        test_df = df.filter(col(\"pickup_month\") == 12)\n",
        "\n",
        "        result = {\"train\": train_df, \"test\": test_df}\n",
        "\n",
        "        if validation_split:\n",
        "            # Use November for validation\n",
        "            train_final = train_df.filter(col(\"pickup_month\") != 11)\n",
        "            validation_df = train_df.filter(col(\"pickup_month\") == 11)\n",
        "            result[\"train\"] = train_final\n",
        "            result[\"validation\"] = validation_df\n",
        "\n",
        "        print(\"‚úÖ Holdout months split: December for testing\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "\n",
        "    # Show split statistics\n",
        "    for split_name, split_df in result.items():\n",
        "        count = split_df.count()\n",
        "        print(f\"üìä {split_name.capitalize()}: {count:,} records\")\n",
        "\n",
        "        # Show year distribution\n",
        "        year_dist = split_df.groupBy(\"pickup_year\").count().collect()\n",
        "        year_info = \", \".join([f\"{row['pickup_year']}: {row['count']:,}\" for row in year_dist])\n",
        "        print(f\"   Year distribution: {year_info}\")\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "executionInfo": {
          "elapsed": 224,
          "status": "ok",
          "timestamp": 1754356178117,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "txqxId0eKbLD"
      },
      "outputs": [],
      "source": [
        "# RECOMMENDED APPROACH for your use case\n",
        "def create_production_splits(df, approach=\"chronological_with_validation\"):\n",
        "    \"\"\"\n",
        "    Create production-ready train/test splits optimized for taxi duration prediction\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : PySpark DataFrame\n",
        "        Full dataset\n",
        "    approach : str\n",
        "        'chronological_with_validation' or 'cross_temporal_validation'\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Split datasets ready for ML pipeline\n",
        "    \"\"\"\n",
        "\n",
        "    if approach == \"chronological_with_validation\":\n",
        "        # BEST for your case: 2015 for training, 2016 for testing\n",
        "        splits = temporal_train_test_split(df, strategy=\"chronological\",\n",
        "                                         validation_split=True)\n",
        "\n",
        "        print(\"\\nüéØ RECOMMENDED APPROACH:\")\n",
        "        print(\"   ‚Ä¢ Train: 2015 (Jan-Oct)\")\n",
        "        print(\"   ‚Ä¢ Validation: 2015 (Nov-Dec)\")\n",
        "        print(\"   ‚Ä¢ Test: 2016 (Full year)\")\n",
        "        print(\"   ‚Ä¢ Benefit: Tests model on completely unseen future data\")\n",
        "\n",
        "    elif approach == \"cross_temporal_validation\":\n",
        "        # Alternative: Mixed years with temporal awareness\n",
        "        splits = temporal_train_test_split(df, strategy=\"stratified_temporal\",\n",
        "                                         test_ratio=0.2, validation_split=True)\n",
        "\n",
        "        print(\"\\nüîÑ ALTERNATIVE APPROACH:\")\n",
        "        print(\"   ‚Ä¢ Mixed temporal cross-validation\")\n",
        "        print(\"   ‚Ä¢ Maintains seasonal patterns in all splits\")\n",
        "\n",
        "    return splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "executionInfo": {
          "elapsed": 54639,
          "status": "ok",
          "timestamp": 1754356235655,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "dBFivBjIKgmt",
        "outputId": "f16524ca-fa1a-4248-d3bf-090704f78cbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Splitting data using 'chronological' strategy\n",
            "üìä Test ratio: 20.0%\n",
            "‚úÖ Chronological split: 2015 (train) vs 2016 (test)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "              <div>\n",
              "                  <p><a href=\"https://console.cloud.google.com/dataproc/interactive/us-central1/sc-20250805-005920-tj1ii9/sparkApplications/application;associatedSqlOperationId=28685f67-80fa-4c60-b9f2-961b650feee9?project=dtc-de-course-466501\">Spark Query</a> (Operation: 28685f67-80fa-4c60-b9f2-961b650feee9)</p>\n",
              "              </div>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Train: 55,243,075 records\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "              <div>\n",
              "                  <p><a href=\"https://console.cloud.google.com/dataproc/interactive/us-central1/sc-20250805-005920-tj1ii9/sparkApplications/application;associatedSqlOperationId=eb6b8560-604d-47f9-9fba-e12e137ff09b?project=dtc-de-course-466501\">Spark Query</a> (Operation: eb6b8560-604d-47f9-9fba-e12e137ff09b)</p>\n",
              "              </div>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Year distribution: 2015: 55,243,075\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "              <div>\n",
              "                  <p><a href=\"https://console.cloud.google.com/dataproc/interactive/us-central1/sc-20250805-005920-tj1ii9/sparkApplications/application;associatedSqlOperationId=3e398ec5-23ea-4750-8e8e-408dbca32041?project=dtc-de-course-466501\">Spark Query</a> (Operation: 3e398ec5-23ea-4750-8e8e-408dbca32041)</p>\n",
              "              </div>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Test: 62,724,650 records\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "              <div>\n",
              "                  <p><a href=\"https://console.cloud.google.com/dataproc/interactive/us-central1/sc-20250805-005920-tj1ii9/sparkApplications/application;associatedSqlOperationId=5a82e2fd-71e3-4abc-b933-e5962e1c6c97?project=dtc-de-course-466501\">Spark Query</a> (Operation: 5a82e2fd-71e3-4abc-b933-e5962e1c6c97)</p>\n",
              "              </div>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Year distribution: 2016: 62,724,650\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "              <div>\n",
              "                  <p><a href=\"https://console.cloud.google.com/dataproc/interactive/us-central1/sc-20250805-005920-tj1ii9/sparkApplications/application;associatedSqlOperationId=3fc391a6-d009-431e-8abd-16b80e5ce192?project=dtc-de-course-466501\">Spark Query</a> (Operation: 3fc391a6-d009-431e-8abd-16b80e5ce192)</p>\n",
              "              </div>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Validation: 10,814,005 records\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "              <div>\n",
              "                  <p><a href=\"https://console.cloud.google.com/dataproc/interactive/us-central1/sc-20250805-005920-tj1ii9/sparkApplications/application;associatedSqlOperationId=fb0f2b80-d5b1-403a-a202-96d7cad3945b?project=dtc-de-course-466501\">Spark Query</a> (Operation: fb0f2b80-d5b1-403a-a202-96d7cad3945b)</p>\n",
              "              </div>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Year distribution: 2015: 10,814,005\n",
            "\n",
            "üéØ RECOMMENDED APPROACH:\n",
            "   ‚Ä¢ Train: 2015 (Jan-Oct)\n",
            "   ‚Ä¢ Validation: 2015 (Nov-Dec)\n",
            "   ‚Ä¢ Test: 2016 (Full year)\n",
            "   ‚Ä¢ Benefit: Tests model on completely unseen future data\n"
          ]
        }
      ],
      "source": [
        "splits = create_production_splits(df_with_duration, approach=\"chronological_with_validation\")\n",
        "\n",
        "train_data = splits[\"train\"]\n",
        "validation_data = splits[\"validation\"]\n",
        "test_data = splits[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "executionInfo": {
          "elapsed": 109831,
          "status": "ok",
          "timestamp": 1754356345484,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "Y6WieSs1PEfq",
        "outputId": "c5b3fc71-12b5-42a4-eb32-fe80b9901673"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "              <div>\n",
              "                  <p><a href=\"https://console.cloud.google.com/dataproc/interactive/us-central1/sc-20250805-005920-tj1ii9/sparkApplications/application;associatedSqlOperationId=0c46273a-dc71-43b8-a053-723e8e19e8c6?project=dtc-de-course-466501\">Spark Query</a> (Operation: 0c46273a-dc71-43b8-a053-723e8e19e8c6)</p>\n",
              "              </div>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------+--------+------------+----------+-----------------+--------------+-------------------------+------------------+---------------+--------------+-------------------+-----------+-------------------+------------------+---------------+-------------+------------+-----------+-----------+-----------+------------+---------------------+------------+------------+------------------------+------------+-----+-----------+-----------+------------+---------------+------------+------------+-----------+-----------+-----------+------------+------------------+\n",
            "|tripid                          |vendorid|service_type|ratecodeid|pickup_locationid|pickup_borough|pickup_zone              |dropoff_locationid|dropoff_borough|dropoff_zone  |pickup_datetime    |pickup_date|dropoff_datetime   |store_and_fwd_flag|passenger_count|trip_distance|fare_amount |extra      |mta_tax    |tip_amount |tolls_amount|improvement_surcharge|total_amount|payment_type|payment_type_description|climate_date|mjd  |cloudCover |humidity   |dewPoint    |precipIntensity|highTemp    |lowTemp     |visibility |windSpeed  |pickup_year|pickup_month|trip_duration_min |\n",
            "+--------------------------------+--------+------------+----------+-----------------+--------------+-------------------------+------------------+---------------+--------------+-------------------+-----------+-------------------+------------------+---------------+-------------+------------+-----------+-----------+-----------+------------+---------------------+------------+------------+------------------------+------------+-----+-----------+-----------+------------+---------------+------------+------------+-----------+-----------+-----------+------------+------------------+\n",
            "|fd19628a5ea8dfb36ac5a45e608ec1b1|2       |Green       |1         |135              |Queens        |Kew Gardens Hills        |134               |Queens         |Kew Gardens   |2016-12-31 23:59:59|2016-12-31 |2017-01-01 00:14:30|N                 |1              |3.410000000  |13.000000000|0.500000000|0.500000000|0.000000000|0.000000000 |0.300000000          |14.300000000|2           |Cash                    |2016-12-31  |57753|0.370000000|0.640000000|25.080000000|0.000000000    |39.970000000|30.440000000|9.920000000|6.400000000|2016       |12          |14.516666666666667|\n",
            "|a53c80fff907e963983bd5aa889ade4b|2       |Green       |1         |168              |Bronx         |Mott Haven/Port Morris   |161               |Manhattan      |Midtown Center|2016-12-31 23:59:58|2016-12-31 |2017-01-01 00:39:07|N                 |1              |8.830000000  |33.500000000|0.500000000|0.500000000|6.960000000|0.000000000 |0.300000000          |41.760000000|1           |Credit card             |2016-12-31  |57753|0.370000000|0.640000000|25.080000000|0.000000000    |39.970000000|30.440000000|9.920000000|6.400000000|2016       |12          |39.15             |\n",
            "|464f6ac2a14c1c6a7d603c8b80e0dd60|1       |Yellow      |1         |144              |Manhattan     |Little Italy/NoLiTa      |209               |Manhattan      |Seaport       |2016-12-31 23:59:58|2016-12-31 |2017-01-01 00:03:50|N                 |1              |0.700000000  |5.000000000 |0.500000000|0.500000000|0.000000000|0.000000000 |0.300000000          |6.300000000 |2           |Cash                    |2016-12-31  |57753|0.370000000|0.640000000|25.080000000|0.000000000    |39.970000000|30.440000000|9.920000000|6.400000000|2016       |12          |3.8666666666666667|\n",
            "|464f6ac2a14c1c6a7d603c8b80e0dd60|1       |Green       |1         |76               |Brooklyn      |East New York            |63                |Brooklyn       |Cypress Hills |2016-12-31 23:59:58|2016-12-31 |2017-01-01 00:15:29|N                 |2              |5.200000000  |16.500000000|0.000000000|0.500000000|0.000000000|0.000000000 |0.300000000          |17.300000000|2           |Cash                    |2016-12-31  |57753|0.370000000|0.640000000|25.080000000|0.000000000    |39.970000000|30.440000000|9.920000000|6.400000000|2016       |12          |15.516666666666667|\n",
            "|4b20fa37ac1c9cecd499300f86f5a5ec|2       |Green       |1         |255              |Brooklyn      |Williamsburg (North Side)|37                |Brooklyn       |Bushwick South|2016-12-31 23:59:57|2016-12-31 |2017-01-01 00:15:47|N                 |1              |3.190000000  |13.000000000|0.500000000|0.500000000|0.000000000|0.000000000 |0.300000000          |14.300000000|2           |Cash                    |2016-12-31  |57753|0.370000000|0.640000000|25.080000000|0.000000000    |39.970000000|30.440000000|9.920000000|6.400000000|2016       |12          |15.833333333333334|\n",
            "+--------------------------------+--------+------------+----------+-----------------+--------------+-------------------------+------------------+---------------+--------------+-------------------+-----------+-------------------+------------------+---------------+-------------+------------+-----------+-----------+-----------+------------+---------------------+------------+------------+------------------------+------------+-----+-----------+-----------+------------+---------------+------------+------------+-----------+-----------+-----------+------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# e.g. show the last 5 rows by descending pickup_datetime\n",
        "df_with_duration.orderBy(F.col(\"pickup_datetime\").desc()).show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "executionInfo": {
          "elapsed": 31242,
          "status": "ok",
          "timestamp": 1754356376723,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "fdJ1VH6BLin7",
        "outputId": "9b596deb-aa29-4e89-86b8-4f63d6ebceeb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "              <div>\n",
              "                  <p><a href=\"https://console.cloud.google.com/dataproc/interactive/us-central1/sc-20250805-005920-tj1ii9/sparkApplications/application;associatedSqlOperationId=6dc7b230-c38e-45ab-adc7-6c4dedccf29d?project=dtc-de-course-466501\">Spark Query</a> (Operation: 6dc7b230-c38e-45ab-adc7-6c4dedccf29d)</p>\n",
              "              </div>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------+------------+----------+-----------------+--------------+--------------------+------------------+---------------+--------------------+-------------------+-----------+-------------------+------------------+---------------+-------------+------------+-----------+-----------+-----------+------------+---------------------+------------+------------+------------------------+------------+-----+-----------+-----------+------------+---------------+------------+------------+-----------+-----------+-----------+------------+------------------+\n",
            "|              tripid|vendorid|service_type|ratecodeid|pickup_locationid|pickup_borough|         pickup_zone|dropoff_locationid|dropoff_borough|        dropoff_zone|    pickup_datetime|pickup_date|   dropoff_datetime|store_and_fwd_flag|passenger_count|trip_distance| fare_amount|      extra|    mta_tax| tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type|payment_type_description|climate_date|  mjd| cloudCover|   humidity|    dewPoint|precipIntensity|    highTemp|     lowTemp| visibility|  windSpeed|pickup_year|pickup_month| trip_duration_min|\n",
            "+--------------------+--------+------------+----------+-----------------+--------------+--------------------+------------------+---------------+--------------------+-------------------+-----------+-------------------+------------------+---------------+-------------+------------+-----------+-----------+-----------+------------+---------------------+------------+------------+------------------------+------------+-----+-----------+-----------+------------+---------------+------------+------------+-----------+-----------+-----------+------------+------------------+\n",
            "|c23344c625117d520...|       2|       Green|         1|               47|         Bronx|  Claremont/Bathgate|                69|          Bronx|East Concourse/Co...|2016-01-01 00:02:23| 2016-01-01|2016-01-01 00:09:20|                 N|              1|  1.550000000| 7.500000000|0.500000000|0.500000000|0.000000000| 0.000000000|          0.300000000| 8.800000000|           1|             Credit card|  2016-01-01|57388|0.920000000|0.790000000|38.870000000|    0.005800000|47.350000000|38.600000000|8.940000000|3.200000000|       2016|           1|              6.95|\n",
            "|4c12f3f30a5c5c9ee...|       1|      Yellow|         1|              137|     Manhattan|            Kips Bay|               162|      Manhattan|        Midtown East|2016-01-01 00:10:22| 2016-01-01|2016-01-01 00:19:05|                 N|              2|  1.600000000| 8.000000000|0.500000000|0.500000000|1.860000000| 0.000000000|          0.300000000|11.160000000|           1|             Credit card|  2016-01-01|57388|0.920000000|0.790000000|38.870000000|    0.005800000|47.350000000|38.600000000|8.940000000|3.200000000|       2016|           1| 8.716666666666667|\n",
            "|7b1754f37f9a2f62c...|       2|      Yellow|         1|              114|     Manhattan|Greenwich Village...|                13|      Manhattan|   Battery Park City|2016-01-01 00:14:09| 2016-01-01|2016-01-01 00:22:15|                 N|              5|  1.870000000| 8.500000000|0.500000000|0.500000000|2.000000000| 0.000000000|          0.300000000|11.800000000|           1|             Credit card|  2016-01-01|57388|0.920000000|0.790000000|38.870000000|    0.005800000|47.350000000|38.600000000|8.940000000|3.200000000|       2016|           1|               8.1|\n",
            "|976df69d8169affe4...|       2|      Yellow|         1|               79|     Manhattan|        East Village|               263|      Manhattan|      Yorkville West|2016-01-01 00:16:01| 2016-01-01|2016-01-01 00:34:03|                 N|              1|  3.850000000|15.000000000|0.500000000|0.500000000|3.260000000| 0.000000000|          0.300000000|19.560000000|           1|             Credit card|  2016-01-01|57388|0.920000000|0.790000000|38.870000000|    0.005800000|47.350000000|38.600000000|8.940000000|3.200000000|       2016|           1|18.033333333333335|\n",
            "|11eb24de5a4002940...|       2|      Yellow|         1|              148|     Manhattan|     Lower East Side|               234|      Manhattan|            Union Sq|2016-01-01 00:17:29| 2016-01-01|2016-01-01 00:28:30|                 N|              1|  1.870000000| 9.500000000|0.500000000|0.500000000|2.160000000| 0.000000000|          0.300000000|12.960000000|           1|             Credit card|  2016-01-01|57388|0.920000000|0.790000000|38.870000000|    0.005800000|47.350000000|38.600000000|8.940000000|3.200000000|       2016|           1|11.016666666666667|\n",
            "+--------------------+--------+------------+----------+-----------------+--------------+--------------------+------------------+---------------+--------------------+-------------------+-----------+-------------------+------------------+---------------+-------------+------------+-----------+-----------+-----------+------------+---------------------+------------+------------+------------------------+------------+-----+-----------+-----------+------------+---------------+------------+------------+-----------+-----------+-----------+------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_data.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A9-RoObT_w8"
      },
      "source": [
        "**Missing Values**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-y8re2CUC_A"
      },
      "source": [
        "Knowing about missing values is important because they indicate how much we don‚Äôt know about our data. Making inferences based on just a few cases is often unwise. In addition, many modelling procedures break down when missing values are involved and the corresponding rows will either have to be removed completely or the values need to be estimated somehow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwm4Re2LUF7G"
      },
      "source": [
        "Here, we are in the fortunate position that our data is complete and there are no missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "uwWe9QPVWPBD",
        "outputId": "2d8f1eee-31f6-4036-fc08-81e111c04ccd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "              <div>\n",
              "                  <p><a href=\"https://console.cloud.google.com/dataproc/interactive/us-central1/sc-20250805-005920-tj1ii9/sparkApplications/application;associatedSqlOperationId=348a87da-0565-4345-bd29-5b059482e48b?project=dtc-de-course-466501\">Spark Query</a> (Operation: 348a87da-0565-4345-bd29-5b059482e48b)</p>\n",
              "              </div>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%time\n",
        "# Compute null‚Äêcounts and total‚Äêrows in one aggregation\n",
        "agg_exprs = [\n",
        "    F.count(F.when(F.col(c).isNull(), c)).alias(f\"{c}_nulls\")\n",
        "    for c in df_with_time.columns\n",
        "] + [\n",
        "    F.count(\"*\").alias(\"total_rows\")\n",
        "]\n",
        "\n",
        "stats = df_with_time.agg(*agg_exprs).collect()[0].asDict()\n",
        "\n",
        "# Now compute percentages in Python\n",
        "null_percentages = {\n",
        "    c.replace(\"_nulls\",\"\"): (count / stats[\"total_rows\"]) * 100\n",
        "    for c, count in stats.items() if c.endswith(\"_nulls\")\n",
        "}\n",
        "\n",
        "print(f\"Total rows: {stats['total_rows']}\")\n",
        "for col, pct in null_percentages.items():\n",
        "    print(f\"{col:20s}: {pct:.2f}% null\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDHMs693VCp7"
      },
      "source": [
        "**Reformatting Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWXi66DPZaiW"
      },
      "source": [
        "For our following analysis, we will turn the data and time from characters into date objects. We also recode vendor_id as a factor. This makes it easier to visualise relationships that involve these features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEGDYC_TZcNU"
      },
      "outputs": [],
      "source": [
        "# Show full schema\n",
        "df_with_duration.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owEyr0vNZifE"
      },
      "outputs": [],
      "source": [
        "# Filter trips ‚â§ 1 minute\n",
        "short_trips = df_with_duration.filter(F.col(\"trip_duration_min\") <= F.lit(1.0))\n",
        "\n",
        "count_short = short_trips.count()\n",
        "count_all   = df_with_duration.count()\n",
        "\n",
        "print(f\"Short trips (‚â§1 min): {count_short:,} out of {count_all:,}\")\n",
        "print(f\"That‚Äôs {count_short/count_all:.2%} of all trips\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sU44CWJZ29h"
      },
      "source": [
        "**Observations**\n",
        "- Sub-1-minute rides usually come from GPS rounding, driver repositioning, or data glitches‚Äînot people really hailing a cab for 30 seconds.\n",
        "- They 're rare (<1% of trips).\n",
        "- They can either be kept for realism or be filtered as outliers, depending on modeling goals.\n",
        "\n",
        "**Analysis**\n",
        "- For the time being, I will be keeping them as they possibly won't affect our final predictions much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELL1Msk8Z52A"
      },
      "source": [
        "**Individual Feature Visualizations**\n",
        "- Visualisations of feature distributions and their relations are key to understanding a data set, and they often open up new lines of inquiry. I always recommend to examine the data from as many different perspectives as possible to notice even subtle trends and correlations.\n",
        "\n",
        "- In this section we will begin by having a look at the distributions of the individual data features.\n",
        "\n",
        "- We start with a map of NYC and overlay a managable number of pickup coordinates to get a general overview of the locations and distances in question. For this visualisation I have used the `leaflet` package, which includes a variety of cool tools for interactive maps. In this map you can zoom and pan through the pickup locations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmu836L9Z_L1"
      },
      "source": [
        "**Note**\n",
        "- Additional data to be added into the pipeline later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR74JYQMaCZI"
      },
      "source": [
        "Let‚Äôs start with plotting the target feature `trip_duration`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNVUfk6oaBcs"
      },
      "outputs": [],
      "source": [
        "train_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCdAEf_1aHo-"
      },
      "outputs": [],
      "source": [
        "pdf = (\n",
        "    train_data\n",
        "      .withColumn(\"rand\", F.rand(1234))\n",
        "      .orderBy(\"rand\")\n",
        "      .limit(8000)\n",
        "      .toPandas()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TP_WK3kHhEMy"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEgtqxfXa9Ht"
      },
      "outputs": [],
      "source": [
        "durations = pdf[\"trip_duration_min\"]\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(durations, bins=150, color=\"red\")\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Trip Duration (minutes)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Histogram of Trip Duration\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUlBcoPjdB6W"
      },
      "outputs": [],
      "source": [
        "# Compute key percentiles\n",
        "med = np.percentile(durations, 50)\n",
        "p75 = np.percentile(durations, 75)\n",
        "p95 = np.percentile(durations, 95)\n",
        "\n",
        "# Define log-spaced bins\n",
        "bins = np.logspace(np.log10(durations.min()+1e-3), np.log10(durations.max()), 50)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "n, bins_out, patches = plt.hist(\n",
        "    durations, bins=bins, color='red', alpha=0.6, edgecolor='black'\n",
        ")\n",
        "\n",
        "# Add vertical lines for percentiles\n",
        "for val, color, label in [(med, 'blue', 'Median'),\n",
        "                          (p75, 'green', '75th pct'),\n",
        "                          (p95, 'purple', '95th pct')]:\n",
        "    plt.axvline(val, color=color, linestyle='--', linewidth=2, label=f'{label}: {val:.2f} min')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.xlabel(\"Trip Duration (minutes)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Histogram of Trip Duration (Log‚Äêspaced bins)\")\n",
        "plt.legend()\n",
        "plt.grid(True, which='both', ls=':', linewidth=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcZBwpcSdLCC"
      },
      "source": [
        "Note the logarithmic x-axis and square-root y-axis.\n",
        "\n",
        "We find:\n",
        "\n",
        "- the majority of rides follow a rather smooth distribution that looks almost log-normal with a peak just short of 10 min.\n",
        "\n",
        "- There are some suspiciously short rides with less than a minute duration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukrxdVF2dZMw"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import desc\n",
        "\n",
        "# Use the .desc() method on the column expression\n",
        "ordered_df = train_data.orderBy(train_data.trip_duration_min.desc())\n",
        "\n",
        "# Reorder columns\n",
        "first_cols = [\"trip_duration_min\", \"pickup_datetime\", \"dropoff_datetime\"]\n",
        "rest_cols = [c for c in train_data.columns if c not in first_cols]\n",
        "reordered_df = ordered_df.select(*first_cols, *rest_cols)\n",
        "\n",
        "# Show the top 10 longest trips\n",
        "reordered_df.show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9DFimxqeJYz"
      },
      "source": [
        "- These trip durations are clearly unrealistic and indicative of serious data quality issues\n",
        "- This minor subset of records spans several months to years, when in fact its not practically feasible to have such long rides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WETZ2XyzeMmr"
      },
      "source": [
        "Over the year, the distributions of **pickup\\_datetime** and **dropoff\\_datetime** look like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMTmXT1-eOdB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8cpT7hBeQJo"
      },
      "outputs": [],
      "source": [
        "# Assuming you already have `pdf` as a Pandas DataFrame:\n",
        "# Convert datetime columns just in case\n",
        "pdf[\"pickup_datetime\"] = pd.to_datetime(pdf[\"pickup_datetime\"])\n",
        "pdf[\"dropoff_datetime\"] = pd.to_datetime(pdf[\"dropoff_datetime\"])\n",
        "\n",
        "# ‚úÖ Cut off after July 2015\n",
        "cutoff_date = pd.to_datetime(\"2015-08-01\")\n",
        "pdf_filtered = pdf[(pdf[\"pickup_datetime\"] < cutoff_date) & (pdf[\"dropoff_datetime\"] < cutoff_date)]\n",
        "\n",
        "# Plot histograms\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
        "\n",
        "# Pickup histogram\n",
        "ax1.hist(pdf_filtered[\"pickup_datetime\"], bins=120, color=\"red\")\n",
        "ax1.set_title(\"Pickup Dates\")\n",
        "ax1.set_ylabel(\"Frequency\")\n",
        "\n",
        "# Dropoff histogram\n",
        "ax2.hist(pdf_filtered[\"dropoff_datetime\"], bins=120, color=\"blue\")\n",
        "ax2.set_title(\"Dropoff Dates\")\n",
        "ax2.set_xlabel(\"Datetime\")\n",
        "ax2.set_ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgzBPcGpeYpZ"
      },
      "source": [
        "Fairly homogeneous, covering half a year between January and July 2015. There is an interesting drop around late January early February"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc3pYvIogK3_"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAK00GWUenza"
      },
      "outputs": [],
      "source": [
        "# Filter rows between Jan 20, 2016 and Feb 10, 2016\n",
        "filtered_data = train_data.filter(\n",
        "    (col(\"pickup_datetime\") > \"2015-01-20\") &\n",
        "    (col(\"pickup_datetime\") < \"2015-02-10\")\n",
        ")\n",
        "\n",
        "# Select pickup_datetime and collect to Pandas\n",
        "pickup_pdf = filtered_data.select(\"pickup_datetime\").toPandas()\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(pickup_pdf[\"pickup_datetime\"], bins=120, color=\"red\")\n",
        "plt.xlabel(\"Pickup Datetime\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Histogram of Pickup Datetimes (Jan 20‚ÄìFeb 10, 2015)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3DQjx8bgUkW"
      },
      "source": [
        "That‚Äôs winter in NYC, so maybe snow storms or other heavy weather? Events like this should be taken into account, maybe through some handy external data set?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeUoUL3vgWuu"
      },
      "source": [
        "In the plot above we can already see some daily and weekly modulations in the number of trips. Let‚Äôs investigate these variations together with the distributions of passenger_count and vendor_id by creating a multi-plot panel with different components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkhNFCCBgqiA"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmcl858XgX70"
      },
      "outputs": [],
      "source": [
        "# Parse pickup_datetime\n",
        "pdf[\"pickup_datetime\"] = pd.to_datetime(pdf[\"pickup_datetime\"])\n",
        "# ‚àö-scaled bar plot for passenger_count\n",
        "p1_data = pdf[\"passenger_count\"].value_counts().reset_index()\n",
        "p1_data.columns = [\"passenger_count\", \"n\"]\n",
        "p1_data[\"n_sqrt\"] = p1_data[\"n\"]**0.5\n",
        "\n",
        "plt.subplot(3, 2, 1)\n",
        "sns.barplot(data=p1_data, x=\"passenger_count\", y=\"n_sqrt\", hue=\"passenger_count\", palette=\"Reds\", dodge=False)\n",
        "plt.title(\"Passenger Count (‚àö scale)\")\n",
        "plt.xlabel(\"Passenger Count\")\n",
        "plt.ylabel(\"‚àöCount\")\n",
        "plt.legend().remove()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nh67HqHngyAi"
      },
      "outputs": [],
      "source": [
        "plt.subplot(3, 2, 2)\n",
        "sns.countplot(\n",
        "    data=pdf,\n",
        "    x=\"vendorid\",\n",
        "    hue=\"vendorid\",\n",
        "    palette=\"YlOrBr\",  # yellow-orange-brown sequential\n",
        "    dodge=False\n",
        ")\n",
        "plt.title(\"Vendor ID Count\")\n",
        "plt.xlabel(\"Vendor ID\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend().remove()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHxOJYPhhBVw"
      },
      "outputs": [],
      "source": [
        "plt.subplot(3, 2, 3)\n",
        "sns.countplot(data=pdf, x=\"store_and_fwd_flag\", color=\"gray\")\n",
        "plt.yscale(\"log\")  # Log scale on y-axis\n",
        "plt.title(\"Store and Forward Flag (Log Scale)\")\n",
        "plt.xlabel(\"store_and_fwd_flag\")\n",
        "plt.ylabel(\"Log Count\")\n",
        "plt.legend().remove()  # Remove legend like theme(legend.position = \"none\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR92onnKhRo0"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import date_format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeK-S8wDh0kS"
      },
      "outputs": [],
      "source": [
        "# 1) Ensure pickup_datetime is a pandas datetime\n",
        "pdf[\"pickup_datetime\"] = pd.to_datetime(pdf[\"pickup_datetime\"])\n",
        "\n",
        "# 2) Add abbreviated weekday (Mon, Tue, ‚Ä¶) with Monday as first day\n",
        "pdf[\"wday\"] = pdf[\"pickup_datetime\"].dt.strftime(\"%a\")\n",
        "\n",
        "# 3) Group by weekday & vendor_id, then count\n",
        "p4_pdf = (\n",
        "    pdf\n",
        "    .groupby([\"wday\", \"vendorid\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"count\")\n",
        ")\n",
        "\n",
        "# 4) Order the weekdays properly\n",
        "day_order = [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
        "p4_pdf[\"wday\"] = pd.Categorical(p4_pdf[\"wday\"], categories=day_order, ordered=True)\n",
        "p4_pdf = p4_pdf.sort_values(\"wday\")\n",
        "\n",
        "# 5) Plot exactly like your ggplot2 version\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.scatterplot(\n",
        "    data=p4_pdf,\n",
        "    x=\"wday\",\n",
        "    y=\"count\",\n",
        "    hue=\"vendorid\",\n",
        "    s=100,\n",
        "    legend=False\n",
        ")\n",
        "plt.xlabel(\"Day of the week\")\n",
        "plt.ylabel(\"Total number of pickups\")\n",
        "plt.title(\"Pickups by weekday & vendor\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ne_MwdsNh9Vs"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import hour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49FsQoIrh-Jn"
      },
      "outputs": [],
      "source": [
        "# Ensure pickup_datetime is a pandas datetime (if not already)\n",
        "pdf[\"pickup_datetime\"] = pd.to_datetime(pdf[\"pickup_datetime\"])\n",
        "\n",
        "# Extract hour of day\n",
        "pdf[\"hpick\"] = pdf[\"pickup_datetime\"].dt.hour\n",
        "\n",
        "# Group by hour & vendorid, then count trips\n",
        "p5_pdf = (\n",
        "    pdf\n",
        "    .groupby([\"hpick\", \"vendorid\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"count\")\n",
        ")\n",
        "\n",
        "# Order by hour for clarity\n",
        "p5_pdf = p5_pdf.sort_values(\"hpick\")\n",
        "\n",
        "# Plot just like ggplot2‚Äôs geom_point(size=4), legend off\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.scatterplot(\n",
        "    data=p5_pdf,\n",
        "    x=\"hpick\",\n",
        "    y=\"count\",\n",
        "    hue=\"vendorid\",\n",
        "    s=100,       # size ~4pt\n",
        "    legend=False\n",
        ")\n",
        "plt.xlabel(\"Hour of the day\")\n",
        "plt.ylabel(\"Total number of pickups\")\n",
        "plt.title(\"Pickups by hour & vendor\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hJ3y3nPiUL7"
      },
      "source": [
        "**Check for 0 or between 7-9 passengers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSS_QMuEiXQr"
      },
      "outputs": [],
      "source": [
        "# 1) Filter for passenger_count == 0 or between 7 and 9\n",
        "anomalous_df = train_data.filter(\n",
        "    (col(\"passenger_count\") == 0) |\n",
        "    (col(\"passenger_count\").between(7, 9))\n",
        ")\n",
        "\n",
        "# 2) See how many of each you have\n",
        "anomalous_df.groupBy(\"passenger_count\") \\\n",
        "    .count() \\\n",
        "    .orderBy(\"passenger_count\") \\\n",
        "    .show()\n",
        "\n",
        "# 3) (Optional) Peek at a few of the actual rows\n",
        "anomalous_df.show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGENIRMbi0cd"
      },
      "source": [
        "There are a few trips with zero, or seven to nine passengers but they are a rare exception.\n",
        "\n",
        "Towards larger passenger numbers we are seeing a smooth decline through 3 to 4, until the larger crowds (and larger cars) give us another peak at 5 to 6 passengers.\n",
        "\n",
        "Vendor 2 has significantly more trips in this data set than vendor 1 (note the logarithmic y-axis). This is true for every day of the week.\n",
        "\n",
        "We find an interesting pattern with Monday being the quietest day and Saturday very busy. This is the same for the two different vendors, with vendor_id == 2 showing significantly higher trip numbers.\n",
        "\n",
        "The `store_and_fwd_flag` values, indicating whether the trip data was sent immediately to the vendor (‚ÄúN‚Äù) or held in the memory of the taxi because there was no connection to the server (‚ÄúY‚Äù), show that there was almost no storing taking place (note again the logarithmic y-axis):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ohl-DBdRi1Lq"
      },
      "outputs": [],
      "source": [
        "# 1) Count occurrences in the pandas DataFrame\n",
        "counts_pdf = (\n",
        "    pdf[\"store_and_fwd_flag\"]\n",
        "    .value_counts(dropna=False)                  # include NaNs if you want\n",
        "    .reset_index(name=\"count\")\n",
        "    .rename(columns={\"index\": \"store_and_fwd_flag\"})\n",
        ")\n",
        "\n",
        "# 2) Plot the data\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(\n",
        "    data=counts_pdf,\n",
        "    x=\"store_and_fwd_flag\",\n",
        "    y=\"count\",\n",
        "    palette=\"pastel\"\n",
        ")\n",
        "plt.xlabel(\"Store and Forward Flag\")\n",
        "plt.ylabel(\"Number of Trips\")\n",
        "plt.title(\"Trips by Store-and-Forward Flag\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-cnLVo-jQ7K"
      },
      "source": [
        "These numbers are equivalent to about half a percent of trips not being transmitted immediately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fdr-_vz3jUBa"
      },
      "source": [
        "The trip volume per hour of the day depends somewhat on the month and strongly on the day of the week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR_xu32QjmVy"
      },
      "outputs": [],
      "source": [
        "# 1) Ensure pickup_datetime is datetime and extract hour & month abbreviation\n",
        "pdf[\"pickup_datetime\"] = pd.to_datetime(pdf[\"pickup_datetime\"])\n",
        "pdf[\"hpick\"] = pdf[\"pickup_datetime\"].dt.hour\n",
        "pdf[\"Month\"] = pdf[\"pickup_datetime\"].dt.strftime(\"%b\")\n",
        "\n",
        "# 2) Group by hour & Month, then count trips\n",
        "p1_pdf = (\n",
        "    pdf\n",
        "    .groupby([\"hpick\", \"Month\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"count\")\n",
        ")\n",
        "\n",
        "# 3) hpick is already int from dt.hour\n",
        "\n",
        "# 4) Order Month factor for a proper legend\n",
        "month_order = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
        "p1_pdf[\"Month\"] = pd.Categorical(p1_pdf[\"Month\"], categories=month_order, ordered=True)\n",
        "p1_pdf = p1_pdf.sort_values([\"Month\", \"hpick\"])\n",
        "\n",
        "# 5) Plot with Seaborn lineplot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(\n",
        "    data=p1_pdf,\n",
        "    x=\"hpick\",\n",
        "    y=\"count\",\n",
        "    hue=\"Month\",\n",
        "    linewidth=1.5\n",
        ")\n",
        "plt.xlabel(\"Hour of the day\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Pickups by Hour of the Day & Month\")\n",
        "plt.legend(title=\"Month\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mgpTYsEjtqz"
      },
      "source": [
        "**Not a good pattern. Let's convert the entire dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTE_wuHBj1KH"
      },
      "outputs": [],
      "source": [
        "# 1) Extract hour of day and month abbreviation from pickup_datetime\n",
        "df2 = (\n",
        "    train_data\n",
        "    .withColumn(\"hpick\", hour(\"pickup_datetime\"))\n",
        "    .withColumn(\"Month\", date_format(\"pickup_datetime\", \"MMM\"))\n",
        ")\n",
        "\n",
        "# 2) Group by hour & Month, then count trips\n",
        "p1_df = (\n",
        "    df2\n",
        "    .groupBy(\"hpick\", \"Month\")\n",
        "    .count()\n",
        "    .orderBy(\"hpick\", \"Month\")\n",
        ")\n",
        "\n",
        "# 3) Collect to Pandas for plotting\n",
        "p1_pdf = p1_df.toPandas()\n",
        "p1_pdf[\"hpick\"] = p1_pdf[\"hpick\"].astype(int)\n",
        "\n",
        "# 4) (Optional) Order Month factor for a proper legend\n",
        "month_order = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
        "p1_pdf[\"Month\"] = pd.Categorical(p1_pdf[\"Month\"], categories=month_order, ordered=True)\n",
        "p1_pdf = p1_pdf.sort_values([\"Month\",\"hpick\"])\n",
        "\n",
        "# 5) Plot with Seaborn lineplot (geom_line equivalent)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(\n",
        "    data=p1_pdf,\n",
        "    x=\"hpick\",\n",
        "    y=\"count\",\n",
        "    hue=\"Month\",\n",
        "    linewidth=1.5\n",
        ")\n",
        "plt.xlabel(\"Hour of the day\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Pickups by Hour of the Day & Month\")\n",
        "plt.legend(title=\"Month\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m9Pt5dA2UWv"
      },
      "source": [
        "- Deep pre-dawn trough around 4‚Äì5 AM on all days (counts fall below ~100 K), reflecting minimal overnight demand\n",
        "\n",
        "- Two clear rush-hour peaks‚Äîa morning spike and an evening spike (~5‚Äì7 PM, ~400‚Äì420 K rides)‚Äîdriven by commuter travel\n",
        "\n",
        "- Saturday demand is highest overall (evening peaks >430 K and a shallower early-morning dip)\n",
        "\n",
        "**Finally, we will look at a simple overview visualisation of the pickup/dropoff latitudes and longitudes**. We will look more into this when we have the latitudes and longitudes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOvklvyN6VXE"
      },
      "source": [
        "## **Feature Relations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ8Md62o6cK9"
      },
      "source": [
        "While the previous section looked primarily at the distributions of the individual features, here we will examine in more detail how those features are related to each other and to our target `trip_duration_min`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SisArO2z6iV3"
      },
      "source": [
        "### 3.1 Pickup date/time vs trip_duration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGFbxuZ46nQk"
      },
      "source": [
        "How does the variation in trip numbers throughout the day and the week affect the average trip duration? Do quieter days and hours lead to faster trips? Here we include the vendor_id as an additional feature. Furthermore, for the hours of the day we add a smoothing layer to indicate the extent of the variation and its uncertainties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSzeAdG96Z6p"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCY1zFrg6z1k"
      },
      "outputs": [],
      "source": [
        "from matplotlib.ticker import FuncFormatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkN2WRnU66SF"
      },
      "outputs": [],
      "source": [
        "# 1) Extract hour of day\n",
        "df_hpick = train_data.withColumn(\"hpick\", hour(\"pickup_datetime\"))\n",
        "\n",
        "# 2) Compute median trip_duration in minutes\n",
        "p2_df = (\n",
        "    df_hpick\n",
        "    .groupBy(\"hpick\", \"vendorid\")\n",
        "    .agg(\n",
        "        (expr(\"percentile_approx(trip_duration_min, 0.5) / 60\"))\n",
        "        .alias(\"median_min\")\n",
        "    )\n",
        "    .orderBy(\"hpick\", \"vendorid\")\n",
        ")\n",
        "\n",
        "# 3) Collect to Pandas\n",
        "p2_pdf = p2_df.toPandas()\n",
        "p2_pdf[\"hpick\"] = p2_pdf[\"hpick\"].astype(int)\n",
        "\n",
        "# 4) Plot with LOESS smoothing and points, legend on\n",
        "g = sns.lmplot(\n",
        "    data=p2_pdf,\n",
        "    x=\"hpick\",\n",
        "    y=\"median_min\",\n",
        "    hue=\"vendorid\",\n",
        "    lowess=True,\n",
        "    height=5,\n",
        "    aspect=2,\n",
        "    scatter_kws={\"s\": 100},\n",
        "    line_kws={\"linewidth\": 1.5},\n",
        "    legend=True          # enable legend\n",
        ")\n",
        "\n",
        "# 5) Format the y-axis to mm:ss\n",
        "ax = g.ax\n",
        "def mmss(x, pos):\n",
        "    m = int(x)\n",
        "    s = int(round((x - m) * 60))\n",
        "    return f\"{m}:{s:02d}\"\n",
        "ax.yaxis.set_major_formatter(FuncFormatter(mmss))\n",
        "\n",
        "# 6) Tidy up labels and legend title\n",
        "ax.set_xlabel(\"Hour of the day\")\n",
        "ax.set_ylabel(\"Median trip duration [min:sec]\")\n",
        "ax.set_title(\"Median Trip Duration by Hour & Vendor (LOESS smoothing)\")\n",
        "g._legend.set_title(\"Vendor ID\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBeDFFbL7gc5"
      },
      "source": [
        "**Pre-dawn dips to shortest trips**\n",
        "- Both vendors hit their minimum median durations (~7.5-9 min) around 5-6 AM, when traffic is lightest.\n",
        "\n",
        "**Steady climb into mid-afternoon peak**\n",
        "- From 6 AM onward, median trip times rise, reaching a high of ~11-11.5 minutes between 2-4 PM, reflecting heavier traffic.\n",
        "\n",
        "**Evening taper**\n",
        "- After 4 PM, durations gradually fall back toward 10 minutes by 10-11 PM as congestion eases.\n",
        "\n",
        "**Vendor parity with slight offsets**\n",
        "- Vendor 1 (blue) and Vendor 2 (orange) follow almost identical curves; Vendor 1's median is typically 5-15 seconds longer during the mid-day and evening peaks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZzol19y7l9M"
      },
      "source": [
        "### Passenger count and Vendor vs trip_duration\n",
        "The next question we are asking is whether different numbers of passengers and/or the different vendors are correlated with the duration of the trip. We choose to examine this issue using a series of boxplots for the `passenger_counts` together with a facet wrap which contrasts the two `vendor_ids`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSrkhu-m7hid"
      },
      "outputs": [],
      "source": [
        "# 1) Sample 10% of the data (or use .limit(N) if you want an exact number)\n",
        "df_bc = train_data \\\n",
        "    .select(\"passenger_count\", \"trip_duration_min\", \"vendorid\") \\\n",
        "    .sample(fraction=0.10, seed=42)\n",
        "\n",
        "# 2) Bring back to Pandas\n",
        "pdf_bc = df_bc.toPandas()\n",
        "\n",
        "# 3) Ensure dtypes\n",
        "pdf_bc[\"passenger_count\"] = pdf_bc[\"passenger_count\"].astype(int)\n",
        "pdf_bc[\"vendorid\"]          = pdf_bc[\"vendorid\"].astype(str)\n",
        "\n",
        "# 4) Plot with seaborn‚Äôs catplot (boxplots) and log‚Äêscale y‚Äìaxis\n",
        "g = sns.catplot(\n",
        "    data=pdf_bc,\n",
        "    x=\"passenger_count\",\n",
        "    y=\"trip_duration_min\",\n",
        "    col=\"vendorid\",\n",
        "    kind=\"box\",\n",
        "    sharey=False,\n",
        "    palette=\"tab10\",\n",
        "    height=4,\n",
        "    aspect=1\n",
        ")\n",
        "\n",
        "# 5) Apply log‚Äêscale to each facet, relabel axes\n",
        "for ax in g.axes.flatten():\n",
        "    ax.set_yscale(\"log\")\n",
        "    ax.set_xlabel(\"Number of passengers\")\n",
        "    ax.set_ylabel(\"Trip duration [min]\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7CscA-DDBQH"
      },
      "outputs": [],
      "source": [
        "pdf.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9o2AFlpCFSw"
      },
      "outputs": [],
      "source": [
        "# 1) Pull just the two columns, limit to e.g. 100 000 rows to avoid OOM\n",
        "pdf_den = (\n",
        "    train_data\n",
        "    .select(\"trip_duration_min\", \"vendorid\")\n",
        "    .limit(100000)\n",
        "    .toPandas()\n",
        ")\n",
        "\n",
        "# 2) Make sure vendorid is a string for hue\n",
        "pdf_den[\"vendorid\"] = pdf_den[\"vendorid\"].astype(str)\n",
        "\n",
        "# 3) Plot a stacked density on a log‚Äêx axis\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.kdeplot(\n",
        "    data=pdf_den,\n",
        "    x=\"trip_duration_min\",\n",
        "    hue=\"vendorid\",\n",
        "    fill=True,\n",
        "    common_norm=False,    # each density scaled to its own area\n",
        "    multiple=\"stack\",     # stack them up\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Trip duration (seconds)\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.title(\"Stacked density of trip duration by vendor\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfXY62H-HSKA"
      },
      "source": [
        "Comparing the densities of the trip_duration distribution for the two vendors we find that the medians are very similar, whereas the means are likely skewed by vendor 2 containing most of the long-duration outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U8Ltle7HaX3"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import mean\n",
        "\n",
        "# 1) Group by vendorid and compute mean & median of trip_duration (in seconds)\n",
        "summary_df = (\n",
        "    train_data\n",
        "    .groupBy(\"vendorid\")\n",
        "    .agg(\n",
        "        mean(\"trip_duration_min\").alias(\"mean_duration\"),\n",
        "        expr(\"percentile_approx(trip_duration_min, 0.5)\").alias(\"median_duration\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# 2) Show results\n",
        "summary_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox3pqqLlI3PA"
      },
      "source": [
        "### Store and Forward vs. Trip_Duration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-Fw11MmI582"
      },
      "outputs": [],
      "source": [
        "# Group by vendorid & store_and_forward_flag, then count trips\n",
        "counts_df = train_data.groupBy(\"vendorid\", \"store_and_fwd_flag\").count()\n",
        "\n",
        "# Display the result\n",
        "counts_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SyvvY0CKcuP"
      },
      "outputs": [],
      "source": [
        "# Filter for Vendor 1 and select relevant columns\n",
        "df_filtered = (\n",
        "            train_data\n",
        "            .filter(col(\"vendorid\") == 1)\n",
        "            .select(\"passenger_count\", \"trip_duration_min\", \"store_and_fwd_flag\")\n",
        ")\n",
        "\n",
        "# Sample a subset to avoid memory issues, then convert to Pandas\n",
        "pdf = df_filtered.sample(fraction=0.1, seed=42).toPandas()\n",
        "\n",
        "# Ensure categorical types for plotting\n",
        "pdf[\"passenger_count\"] = pdf[\"passenger_count\"].astype(str)\n",
        "pdf[\"store_and_fwd_flag\"] = pdf[\"store_and_fwd_flag\"].astype(str)\n",
        "\n",
        "# Create faceted boxplots with log-scaled y-axis\n",
        "g = sns.catplot(\n",
        "    data=pdf,\n",
        "    x=\"passenger_count\",\n",
        "    y=\"trip_duration_min\",\n",
        "    hue=\"passenger_count\",\n",
        "    col=\"store_and_fwd_flag\",\n",
        "    kind=\"box\",\n",
        "    sharey=False,\n",
        "    palette=\"tab10\",\n",
        "    height=4,\n",
        "    aspect=1,\n",
        ")\n",
        "\n",
        "# Add global title\n",
        "g.fig.suptitle(\"Store_and_fwd_flag impact\", y=1.03)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHaJZ7MGRQwD"
      },
      "source": [
        "We find that there is no overwhelming differences between the stored and non-stored trips. The stored ones might be slightly longer, though, and don‚Äôt include any of the suspiciously long trips."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxgwN_xyWR7Y"
      },
      "source": [
        "## **Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D_GhNXvWkqD"
      },
      "source": [
        "In this section we try to build new features from the existing ones, trying to find better predictors for the target variable.\n",
        "\n",
        "**To Do**\n",
        "- Define all of the new features and analyze them. Pick OR discard on the go\n",
        "\n",
        "\n",
        "The new temporal features (date, mos, wday, hr.) are derived from the `pickup_datetime`. We get the JFK and La Guardia airport co-ordinates from Wikipedia. The blizzard feature is based on the external weather data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJhbvyp2X0Am"
      },
      "source": [
        "### Direct distance of the trip\n",
        "\n",
        "From the coordinates of the trip and dropoff points we can calculate the direct distance (as the crow flies) between the two points, and compare it to our trip_duration(s). Since taxis aren‚Äôt crows (in most practical scenarios), these values correspond to the minimum possible travel distance.\n",
        "\n",
        "To compute these distances we are using the distCosine function of the `geosphere` package for spherical trigonometry. This method gives us the shortest distance between two points on the spherical Earth. For the purpose of this analysis, we choose to ignore ellipsoidal distortion of the Earth's shape. Here are the raw values of distance vs. duration (based on a downsized sample to speed up the kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgD3_dToWT8J"
      },
      "outputs": [],
      "source": [
        "train_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXwlZnfJvEH-"
      },
      "outputs": [],
      "source": [
        "# Load the public Taxi geometry table from Bigquery\n",
        "zone_centroids = (\n",
        "    spark.read.format(\"bigquery\")\n",
        "    .option(\"query\", \"\"\"\n",
        "        SELECT\n",
        "            zone_id AS locationid,\n",
        "            ST_X(ST_CENTROID(zone_geom)) AS lon,\n",
        "            ST_Y(ST_CENTROID(zone_geom)) AS lat\n",
        "        FROM `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom`\n",
        "    \"\"\")\n",
        "    .load()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKSBbv4MvW7R"
      },
      "outputs": [],
      "source": [
        "zone_centroids.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuxfDYRzvgP1"
      },
      "outputs": [],
      "source": [
        "train_data = (\n",
        "    train_data\n",
        "    .join(\n",
        "        zone_centroids\n",
        "            .withColumnRenamed(\"locationid\", \"pickup_locationid\")\n",
        "            .withColumnRenamed(\"lon\", \"pickup_lon\")\n",
        "            .withColumnRenamed(\"lat\", \"pickup_lat\"),\n",
        "        on=\"pickup_locationid\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "    .join(\n",
        "        zone_centroids\n",
        "            .withColumnRenamed(\"locationid\", \"dropoff_locationid\")\n",
        "            .withColumnRenamed(\"lon\", \"dropoff_lon\")\n",
        "            .withColumnRenamed(\"lat\", \"dropoff_lat\"),\n",
        "        on=\"dropoff_locationid\",\n",
        "        how=\"left\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLwoWKCtyXB_"
      },
      "outputs": [],
      "source": [
        "validation_data = (\n",
        "    validation_data\n",
        "    .join(\n",
        "        zone_centroids\n",
        "            .withColumnRenamed(\"locationid\", \"pickup_locationid\")\n",
        "            .withColumnRenamed(\"lon\", \"pickup_lon\")\n",
        "            .withColumnRenamed(\"lat\", \"pickup_lat\"),\n",
        "        on=\"pickup_locationid\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "    .join(\n",
        "        zone_centroids\n",
        "            .withColumnRenamed(\"locationid\", \"dropoff_locationid\")\n",
        "            .withColumnRenamed(\"lon\", \"dropoff_lon\")\n",
        "            .withColumnRenamed(\"lat\", \"dropoff_lat\"),\n",
        "        on=\"dropoff_locationid\",\n",
        "        how=\"left\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRIZl6-eyaOd"
      },
      "outputs": [],
      "source": [
        "test_data = (\n",
        "    test_data\n",
        "    .join(\n",
        "        zone_centroids\n",
        "            .withColumnRenamed(\"locationid\", \"pickup_locationid\")\n",
        "            .withColumnRenamed(\"lon\", \"pickup_lon\")\n",
        "            .withColumnRenamed(\"lat\", \"pickup_lat\"),\n",
        "        on=\"pickup_locationid\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "    .join(\n",
        "        zone_centroids\n",
        "            .withColumnRenamed(\"locationid\", \"dropoff_locationid\")\n",
        "            .withColumnRenamed(\"lon\", \"dropoff_lon\")\n",
        "            .withColumnRenamed(\"lat\", \"dropoff_lat\"),\n",
        "        on=\"dropoff_locationid\",\n",
        "        how=\"left\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y0fd_eZyhL9"
      },
      "outputs": [],
      "source": [
        "train_data.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Wgy1DigziDm"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import (\n",
        "    col, radians, sin, cos, sqrt, asin, expr, rand\n",
        ")\n",
        "\n",
        "# Earth radius in meters\n",
        "EARTH_RADIUS = 6_371_000.0\n",
        "\n",
        "df_dist = train_data.select(\n",
        "    \"*\", # keep all existing columns\n",
        "    # convert to radians\n",
        "    radians(col(\"pickup_lat\")).alias(\"plat\"),\n",
        "    radians(col(\"pickup_lon\")).alias(\"plon\"),\n",
        "    radians(col(\"dropoff_lat\")).alias(\"dlat\"),\n",
        "    radians(col(\"dropoff_lon\")).alias(\"dlon\"),\n",
        "    # compute deltas\n",
        "    (radians(col(\"dropoff_lat\")) - radians(col(\"pickup_lat\"))).alias(\"Œîlat\"),\n",
        "    (radians(col(\"dropoff_lon\")) - radians(col(\"pickup_lon\"))).alias(\"Œîlon\"),\n",
        "    # Haversine ‚Äúa‚Äù term\n",
        "    (\n",
        "        sin((radians(col(\"dropoff_lat\")) - radians(col(\"pickup_lat\"))) / 2)**2\n",
        "        + cos(radians(col(\"pickup_lat\"))) * cos(radians(col(\"dropoff_lat\")))\n",
        "          * sin((radians(col(\"dropoff_lon\")) - radians(col(\"pickup_lon\"))) / 2)**2\n",
        "    ).alias(\"a\"),\n",
        "    # angular distance c\n",
        "    (\n",
        "        2\n",
        "        * asin(\n",
        "            sqrt(\n",
        "                sin((radians(col(\"dropoff_lat\")) - radians(col(\"pickup_lat\"))) / 2)**2\n",
        "                + cos(radians(col(\"pickup_lat\"))) * cos(radians(col(\"dropoff_lat\")))\n",
        "                  * sin((radians(col(\"dropoff_lon\")) - radians(col(\"pickup_lon\"))) / 2)**2\n",
        "            )\n",
        "        )\n",
        "    ).alias(\"c\"),\n",
        "    # direct distance (m)\n",
        "    (col(\"c\") * EARTH_RADIUS).alias(\"direct_dist_m\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGAIlh1fGXE7"
      },
      "outputs": [],
      "source": [
        "# Sample 50K rows for plotting\n",
        "sampled = (\n",
        "    df_dist\n",
        "    .orderBy(rand(seed=4321))\n",
        "    .limit(50000)\n",
        "    .select(\"direct_dist_m\", \"trip_duration_min\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXe8pjLnGuA2"
      },
      "outputs": [],
      "source": [
        "# Convert trip_duration_min -> trip_duration_sec in the sampled Spark dataframe\n",
        "sampled = sampled.withColumn(\n",
        "        \"trip_duration_sec\",\n",
        "        col(\"trip_duration_min\")*60\n",
        ")\n",
        "\n",
        "# If you only want the distance and the new seconds column:\n",
        "sampled = sampled.select(\n",
        "    \"direct_dist_m\",\n",
        "    \"trip_duration_sec\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2F-vk6mG58x"
      },
      "outputs": [],
      "source": [
        "# 3) Plot in Pandas (log‚Äìlog)\n",
        "pdf = sampled.toPandas()\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(\n",
        "    x=\"direct_dist_m\",\n",
        "    y=\"trip_duration_sec\",\n",
        "    data=pdf,\n",
        "    alpha=0.3,\n",
        "    s=10\n",
        ")\n",
        "plt.xscale(\"log\")\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"Direct distance [m]\")\n",
        "plt.ylabel(\"Trip duration [sec]\")\n",
        "plt.title(\"Direct distance vs. Trip duration\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s34AvaUt_VGP"
      },
      "source": [
        "We find:\n",
        "- The distance generally increases with increasing trip_duration\n",
        "- The trip distances and the corresponding duration seem reasonable in this data and the associated plot\n",
        "- A significant number of trips are less than a km but their durations are significant. Seems to be erroneous recording"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLbv-ED0Hfaq"
      },
      "source": [
        "Let‚Äôs filter the data a little bit to remove the extreme (and the extremely suspicious) data points, and bin the data into a 2-d histogram. This plot shows that in log-log space the trip_duration is increasing slower than linear for larger distance values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ooShptxHzFt"
      },
      "outputs": [],
      "source": [
        "# Filter the dataframe\n",
        "filtered_df = df_dist.filter(\n",
        "    (col(\"trip_duration_min\")<3600) & (col(\"trip_duration_min\")>120) &\n",
        "    (col(\"direct_dist_m\")>100) &(col(\"direct_dist_m\")<100e3)\n",
        ")\n",
        "\n",
        "# Convert to Pandas for plotting\n",
        "pdf = filtered_df.select(\"direct_dist_m\", \"trip_duration_min\").toPandas()\n",
        "\n",
        "# Plot using seaborn or matplotlib\n",
        "plt.figure(figsize=(8,6))\n",
        "hb = plt.hexbin(\n",
        "    x=np.log10(pdf[\"direct_dist_m\"]),\n",
        "    y=np.log10(pdf[\"trip_duration_min\"]),\n",
        "    gridsize=500,\n",
        "    cmap=\"viridis\"\n",
        ")\n",
        "cb = plt.colorbar(hb)\n",
        "cb.set_label('Counts')\n",
        "\n",
        "plt.xlabel(\"Direct distance [log10(m)]\")\n",
        "plt.ylabel(\"Trip duration [log10(s)]\")\n",
        "plt.title(\"Trip Duration vs Distance (log-log)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA5z1WQGN0KA"
      },
      "source": [
        "## **Travel Speed**\n",
        "Distance over time is of course velocity, and by computing the average apparent velocity of our taxis we will have another diagnostic to remove bogus values. Of course, we won‚Äôt be able to use speed as a predictor for our model, since it requires knowing the travel time, but it can still be helpful in cleaning up our training data and finding other features with predictive power. This is the speed distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpxi79zvUQWW"
      },
      "outputs": [],
      "source": [
        "df_dist.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZphCjxlPjbP"
      },
      "outputs": [],
      "source": [
        "# Filter the values\n",
        "df_dist = df_dist.filter(col(\"trip_duration_min\")>0 )\n",
        "df_dist = df_dist.withColumn(\"speed\", col(\"direct_dist_m\")/(col(\"trip_duration_min\")*60))\n",
        "\n",
        "# Convert to pandas\n",
        "df_dist_plot = df_dist.toPandas()\n",
        "df_dist_plot = df_dist_plot[(df_dist_plot[\"speed\"]>2) & (df_dist_plot[\"speed\"]<1e2)]\n",
        "\n",
        "# Plot the figure\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(pdf[\"speed\"], bins=50, color=\"red\", edgecolor=\"black\")\n",
        "plt.xlabel(\"Average speed [km/h] (direct distance)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Histogram of Average Speed\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 23,
          "status": "aborted",
          "timestamp": 1754355791397,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "6OSZjgTdcbGl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "name": "nyc_taxi_duration_prediction",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
